{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcdxJ/Z3YQAETpwlI1hByj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasNazareth1993-coder/Virtual-cell/blob/main/Untitled89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r67mlL-OWjWy"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üî∫ 3D Attention Visualization (animated surface over time)\n",
        "# Paste this after training and after the model/env exist.\n",
        "# Requires: plotly\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q plotly\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import plotly.graph_objects as go\n",
        "from torch.nn import MultiheadAttention\n",
        "import time\n",
        "import os\n",
        "\n",
        "# ---------- Settings ----------\n",
        "NUM_FRAMES = 120    # how many time steps/frames to capture (or set to rollout length)\n",
        "OUTPUT_HTML = \"attention_3d_animation.html\"\n",
        "CAPTURE_STEPS = 120  # number of env steps to capture attention for\n",
        "\n",
        "# ---------- Utility: try to find a MultiheadAttention inside extractor ----------\n",
        "def find_mha_modules(module):\n",
        "    return [m for m in module.modules() if isinstance(m, MultiheadAttention)]\n",
        "\n",
        "extractor = model.policy.features_extractor  # use your trained model's extractor\n",
        "\n",
        "mha_modules = find_mha_modules(extractor)\n",
        "if len(mha_modules) == 0:\n",
        "    print(\"‚ö†Ô∏è No MultiheadAttention modules found in extractor; cannot extract attention weights.\")\n",
        "else:\n",
        "    print(f\"Found {len(mha_modules)} MultiheadAttention module(s). Using the first one for visualization.\")\n",
        "\n",
        "# ---------- Capture attention weights over a rollout ----------\n",
        "# We'll attempt to capture attn weights exposed as module.attn_output_weights after forward.\n",
        "attn_time_series = []  # list of (N_cells x N_cells) matrices (averaged over heads)\n",
        "\n",
        "# If your extractor's transformer has multiple layers/heads you can optionally average across them\n",
        "target_mha = mha_modules[0] if len(mha_modules)>0 else None\n",
        "\n",
        "# forward-hook fallback: store the most recent attn weights (if module exposes them)\n",
        "captured = {\"weights\": None}\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    # Some versions of MultiheadAttention expose `attn_output_weights` as an attribute after forward.\n",
        "    # Try to read it; fallback to None.\n",
        "    w = None\n",
        "    if hasattr(module, \"attn_output_weights\"):\n",
        "        w = module.attn_output_weights.detach().cpu().numpy()  # shape: (num_heads, target_len, source_len)\n",
        "    # Some builds may store it under different attr; attempt common alternatives\n",
        "    if w is None and hasattr(module, \"attn_weights\"):\n",
        "        w = module.attn_weights.detach().cpu().numpy()\n",
        "    captured[\"weights\"] = w\n",
        "\n",
        "hook_handle = None\n",
        "if target_mha is not None:\n",
        "    try:\n",
        "        hook_handle = target_mha.register_forward_hook(hook_fn)\n",
        "    except Exception as e:\n",
        "        print(\"Could not register hook on MultiheadAttention:\", e)\n",
        "        hook_handle = None\n",
        "\n",
        "# Run a rollout and run extractor on each observation to get per-step attention.\n",
        "obs, _ = env.reset()\n",
        "steps_to_capture = min(CAPTURE_STEPS, 3000)  # safety cap\n",
        "for step in range(steps_to_capture):\n",
        "    # call extractor to trigger attention computation\n",
        "    with torch.no_grad():\n",
        "        obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # (1, obs_dim)\n",
        "        _ = extractor(obs_t)  # this should run the transformer -> hook captures weights\n",
        "\n",
        "    # try to get captured weights; if None, fallback to computing attention manually (not trivial)\n",
        "    w = captured.get(\"weights\", None)\n",
        "    if w is None:\n",
        "        # fallback: try to run the MHA directly (construct queries/keys from extractor internals)\n",
        "        # But that's fragile; we'll record NaNs so user knows capture failed.\n",
        "        print(f\"Step {step}: warning ‚Äî no attention weights captured, recording NaN matrix.\")\n",
        "        Ncells = NUM_CELLS\n",
        "        attn_time_series.append(np.full((Ncells, Ncells), np.nan))\n",
        "    else:\n",
        "        # Depending on shape: (num_heads, tgt_len, src_len) or (tgt_len, src_len) if averaged.\n",
        "        if w.ndim == 3:\n",
        "            # average over heads\n",
        "            avg = w.mean(axis=0)  # (tgt_len, src_len)\n",
        "        elif w.ndim == 2:\n",
        "            avg = w\n",
        "        else:\n",
        "            raise ValueError(\"Unexpected attention weights shape: \" + str(w.shape))\n",
        "        # If transformer treated cells as sequence tokens, tgt_len == src_len == NUM_CELLS\n",
        "        attn_time_series.append(avg)\n",
        "    # step environment with deterministic policy to evolve state\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, r, done, _, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Remove hook\n",
        "if hook_handle is not None:\n",
        "    hook_handle.remove()\n",
        "\n",
        "if len(attn_time_series) == 0:\n",
        "    raise RuntimeError(\"No attention frames were collected; aborting visualization.\")\n",
        "\n",
        "# ---------- Prepare frames for Plotly ----------\n",
        "# Convert list to numpy array: (T, N, N)\n",
        "attn_arr = np.stack(attn_time_series, axis=0)  # shape (T, N, N)\n",
        "T, N, M = attn_arr.shape\n",
        "print(f\"Captured attention array: frames={T}, shape per frame={N}x{M}\")\n",
        "\n",
        "# Some frames may contain NaNs if capture failed for some steps; replace NaNs with nearest valid frame average\n",
        "if np.isnan(attn_arr).any():\n",
        "    # find first valid frame\n",
        "    valid_idx = np.where(~np.isnan(attn_arr).reshape(T, -1).any(axis=1))[0]\n",
        "    if len(valid_idx) == 0:\n",
        "        print(\"‚ö†Ô∏è All captured frames are NaN ‚Äî attention capture failed.\")\n",
        "    else:\n",
        "        fill = attn_arr[valid_idx[0]]\n",
        "        nan_mask = np.isnan(attn_arr)\n",
        "        attn_arr[nan_mask] = np.take(fill, np.where(nan_mask)[1], axis=0)  # crude fill\n",
        "\n",
        "# Normalize each frame for better surface scaling (optional)\n",
        "# We'll scale to [0,1] per frame\n",
        "attn_min = attn_arr.min(axis=(1,2), keepdims=True)\n",
        "attn_max = attn_arr.max(axis=(1,2), keepdims=True)\n",
        "attn_norm = (attn_arr - attn_min) / (np.maximum(attn_max - attn_min, 1e-9))\n",
        "\n",
        "# Create x,y grid for surface: use source (columns) along x, target (rows) along y\n",
        "x = np.arange(N)  # source cells\n",
        "y = np.arange(N)  # querying cells\n",
        "\n",
        "# ---------- Build plotly frames ----------\n",
        "frames = []\n",
        "for t in range(T):\n",
        "    z = attn_norm[t]\n",
        "    frame = go.Frame(\n",
        "        data=[go.Surface(z=z, x=x, y=y, cmin=0, cmax=1, showscale=False)],\n",
        "        name=str(t),\n",
        "        traces=[0],\n",
        "    )\n",
        "    frames.append(frame)\n",
        "\n",
        "# initial surface\n",
        "init_z = attn_norm[0]\n",
        "\n",
        "fig = go.Figure(\n",
        "    data=[go.Surface(z=init_z, x=x, y=y, cmin=0, cmax=1, colorscale=\"Viridis\", showscale=True)],\n",
        "    layout=go.Layout(\n",
        "        title=\"3D Attention Surface (per-cell interactions) ‚Äî animated\",\n",
        "        scene=dict(\n",
        "            xaxis=dict(title=\"Source Cell (attention to)\"),\n",
        "            yaxis=dict(title=\"Querying Cell (attention from)\"),\n",
        "            zaxis=dict(title=\"Normalized Attention\", range=[0,1]),\n",
        "        ),\n",
        "        updatemenus=[\n",
        "            dict(\n",
        "                type=\"buttons\",\n",
        "                buttons=[\n",
        "                    dict(label=\"Play\",\n",
        "                         method=\"animate\",\n",
        "                         args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True}, \"fromcurrent\": True}]),\n",
        "                    dict(label=\"Pause\",\n",
        "                         method=\"animate\",\n",
        "                         args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}])\n",
        "                ],\n",
        "                pad={\"r\": 10, \"t\": 10},\n",
        "                showactive=True,\n",
        "                x=0.1,\n",
        "                xanchor=\"right\",\n",
        "                y=0,\n",
        "                yanchor=\"top\"\n",
        "            )\n",
        "        ],\n",
        "        sliders=[{\n",
        "            \"pad\": {\"b\": 10, \"t\": 60},\n",
        "            \"len\": 0.9,\n",
        "            \"x\": 0.1,\n",
        "            \"y\": 0,\n",
        "            \"steps\": [\n",
        "                {\"args\": [[str(k)], {\"frame\": {\"duration\": 0, \"redraw\": True}, \"mode\": \"immediate\"}],\n",
        "                 \"label\": str(k), \"method\": \"animate\"}\n",
        "                for k in range(T)\n",
        "            ]\n",
        "        }]\n",
        "    ),\n",
        "    frames=frames\n",
        ")\n",
        "\n",
        "# Save interactive HTML\n",
        "fig.write_html(OUTPUT_HTML)\n",
        "print(f\"Saved interactive 3D attention animation to: {OUTPUT_HTML}\")\n",
        "# If in Colab, display inline\n",
        "try:\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML(fig.to_html(full_html=False, include_plotlyjs='cdn')))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------- Optional: export to static mp4/gif (requires extra tools) ----------\n",
        "# To save an MP4/GIF, you'd need to render frames to images and stitch them.\n",
        "# Example (Colab): pip install imageio kaleido && use fig.to_image for each frame, then imageio.mimsave\n",
        "# This can be slow and sometimes limited by server support for kaleido/ffmpeg."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from gymnasium import spaces\n",
        "\n",
        "class CustomExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: spaces.Box, features_dim: int = 64):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "        self.extractor = AttentionReportingTransformerExtractor(observation_space)\n",
        "        self._features_dim = features_dim\n",
        "\n",
        "    def forward(self, observations):\n",
        "        feats = self.extractor(observations)\n",
        "        return feats\n",
        "\n",
        "    def get_attention(self):\n",
        "        \"\"\"Retrieve latest attention weights (N x N)\"\"\"\n",
        "        return getattr(self.extractor, \"last_attention\", None)"
      ],
      "metadata": {
        "id": "SQlY1tlhWtxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomExtractor,\n",
        "    features_extractor_kwargs=dict(features_dim=64),\n",
        ")\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)"
      ],
      "metadata": {
        "id": "--21N6laWzsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, _ = env.reset()\n",
        "for step in range(100):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, done, _, info = env.step(action)\n",
        "\n",
        "    # Retrieve per-step attention weights (N_cells x N_cells)\n",
        "    attn = model.policy.features_extractor.get_attention()\n",
        "    if attn is not None:\n",
        "        print(f\"Step {step} attention shape: {attn.shape}\")"
      ],
      "metadata": {
        "id": "YiWj1OhuW5db"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}