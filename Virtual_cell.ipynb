{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAXX28R4k2F8+ETPZ8UKoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasNazareth1993-coder/Virtual-cell/blob/main/Virtual_cell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rzjD7RqyRn8",
        "outputId": "7776739a-ee68-4b89-e8cb-7df0ae833896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organ-on-Chip Telomere Simulation Snapshot (t=99)\n",
            "    t  cell_id   measured_L       true_L    action  mut_count       risk  \\\n",
            "0  99        0  5382.674459  5357.254875  0.107822        3.0  36.961110   \n",
            "1  99        1  5333.592560  5321.943773  0.111094        3.0  36.964120   \n",
            "2  99        2  5417.334106  5402.686217  0.105511        3.0  36.947836   \n",
            "3  99        3  5535.250110  5519.640509  0.097650        2.0  11.116666   \n",
            "4  99        4  5375.144311  5353.507147  0.108324        2.0  11.134162   \n",
            "\n",
            "   state  \n",
            "0      0  \n",
            "1      0  \n",
            "2      0  \n",
            "3      0  \n",
            "4      0  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/mnt/data/ooc_telomere_simulation.csv', (5000, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulation parameters\n",
        "N_cells = 50\n",
        "T = 100  # timesteps\n",
        "alpha = 0.8   # telomerase efficacy coefficient\n",
        "delta_rep_mean = 30.0  # mean loss per division (base pair units)\n",
        "delta_rep_sd = 5.0\n",
        "sigma_noise = 10.0  # measurement noise\n",
        "L_sen = 3000.0  # senescence threshold (bp)\n",
        "L_crit = 1000.0\n",
        "initial_L_mean = 8000.0\n",
        "initial_L_sd = 200.0\n",
        "\n",
        "# Controller thresholds\n",
        "monitor_low = 4000.0\n",
        "monitor_high = 7000.0\n",
        "max_action = 1.0\n",
        "min_action = 0.0\n",
        "\n",
        "# Risk model params (simple hazard)\n",
        "beta_action = 0.5\n",
        "beta_mut = 1.2\n",
        "\n",
        "# Initialize state arrays\n",
        "L = np.random.normal(initial_L_mean, initial_L_sd, size=N_cells)  # telomere lengths\n",
        "mut_count = np.zeros(N_cells)  # accumulated mutations proxy\n",
        "action_history = np.zeros((T, N_cells))\n",
        "observations = np.zeros((T, N_cells))\n",
        "risk_scores = np.zeros((T, N_cells))\n",
        "state_labels = np.zeros((T, N_cells), dtype=int)  # 0=healthy,1=senescent,2=critical\n",
        "\n",
        "# Simple rule-based controller function\n",
        "def controller_rule(obs_L, history_actions, mut, t):\n",
        "    \"\"\"\n",
        "    Rule:\n",
        "    - If observed L < monitor_low and mut small -> activate telomerase (action proportional)\n",
        "    - If observed L between monitor_low and monitor_high -> keep low action\n",
        "    - If observed L > monitor_high -> action=0 (avoid unnecessary activation)\n",
        "    - If mut_count high or risk predicted high -> block activation (action=0)\n",
        "    \"\"\"\n",
        "    # Estimate simple mutational risk threshold\n",
        "    mut_threshold = 5\n",
        "    if mut > mut_threshold:\n",
        "        return 0.0\n",
        "    if obs_L < monitor_low:\n",
        "        # proportional control to how far below threshold\n",
        "        return float(min(max_action, (monitor_low - obs_L) / monitor_low))\n",
        "    elif obs_L < monitor_high:\n",
        "        return 0.2 * (1 - (obs_L - monitor_low) / (monitor_high - monitor_low))\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "# Run simulation\n",
        "records = []\n",
        "for t in range(T):\n",
        "    # Simulate measurement noise and sensor sampling\n",
        "    measured_L = L + np.random.normal(0, sigma_noise, size=N_cells)\n",
        "    observations[t] = measured_L\n",
        "    for i in range(N_cells):\n",
        "        # Decide action based on controller rule and history\n",
        "        hist = action_history[max(0, t-10):t, i] if t>0 else np.array([])\n",
        "        act = controller_rule(measured_L[i], hist, mut_count[i], t)\n",
        "        action_history[t, i] = act\n",
        "\n",
        "        # Update telomere based on mechanistic eq: L_{t+1} = L_t - delta_rep + alpha*A + noise\n",
        "        delta_rep = max(0.0, np.random.normal(delta_rep_mean, delta_rep_sd))\n",
        "        L[i] = L[i] - delta_rep + alpha * act * 100.0 + np.random.normal(0, 2.0)  # action scaled\n",
        "        # Ensure lengths non-negative\n",
        "        L[i] = max(0.0, L[i])\n",
        "\n",
        "        # Update mutation count proxy: increases with action and low telomere length\n",
        "        mut_increment = np.random.poisson(0.02 + 0.0001 * max(0, (monitor_low - L[i])))\n",
        "        # action increases risk slightly\n",
        "        mut_increment += int(np.random.poisson(0.01 * act))\n",
        "        mut_count[i] += mut_increment\n",
        "\n",
        "        # Risk score (simple)\n",
        "        risk = np.exp(beta_action * np.sum(action_history[max(0,t-20):t+1, i]) / (t+1 + 1e-6) + beta_mut * mut_count[i])\n",
        "        risk_scores[t, i] = risk\n",
        "\n",
        "        # State labeling\n",
        "        if L[i] < L_crit:\n",
        "            state = 2  # critical\n",
        "        elif L[i] < L_sen:\n",
        "            state = 1  # senescent-prone\n",
        "        else:\n",
        "            state = 0  # healthy\n",
        "        state_labels[t, i] = state\n",
        "\n",
        "        # Record\n",
        "        records.append({\n",
        "            \"t\": t,\n",
        "            \"cell_id\": i,\n",
        "            \"measured_L\": measured_L[i],\n",
        "            \"true_L\": L[i],\n",
        "            \"action\": act,\n",
        "            \"mut_count\": mut_count[i],\n",
        "            \"risk\": risk,\n",
        "            \"state\": state\n",
        "        })\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame.from_records(records)\n",
        "out_path = \"/mnt/data/ooc_telomere_simulation.csv\"\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "df.to_csv(out_path, index=False)\n",
        "\n",
        "# Display summary dataframe (last timestep snapshot)\n",
        "snapshot = df[df['t']==(T-1)].sort_values('cell_id').reset_index(drop=True)\n",
        "\n",
        "# Replaced caas_jupyter_tools display with a direct print of the dataframe head\n",
        "print(\"Organ-on-Chip Telomere Simulation Snapshot (t={})\".format(T-1))\n",
        "print(snapshot.head())\n",
        "\n",
        "# Provide info about saved file\n",
        "out_path, df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"timestamp\": \"2025-11-18T16:00:00Z\",\n",
        "  \"chip_id\": \"OOC-01\",\n",
        "  \"cells\": [\n",
        "    {\"cell_id\": 0, \"measured_L\": 5120.4, \"biomarkers\": {\"p53\": 0.1, \"γH2AX\": 0.02}},\n",
        "    ...\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywYbXSr9ySs5",
        "outputId": "c25dd817-60b8-4d0e-9802-7f2028e9a9ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'timestamp': '2025-11-18T16:00:00Z',\n",
              " 'chip_id': 'OOC-01',\n",
              " 'cells': [{'cell_id': 0,\n",
              "   'measured_L': 5120.4,\n",
              "   'biomarkers': {'p53': 0.1, 'γH2AX': 0.02}},\n",
              "  Ellipsis]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_string = '''\n",
        "{\n",
        "  \"actions\": [{\"cell_id\":0,\"action\":0.2,\"explain\":\"L below target by 0.14x\",\"blocked\":false}],\n",
        "  \"global_alert\": false\n",
        "}\n",
        "'''\n",
        "\n",
        "# Parse the JSON string into a Python object\n",
        "data = json.loads(json_string)\n",
        "\n",
        "# You can now work with the 'data' dictionary\n",
        "# For example, print it to see the structure\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqIGDiEqyXdZ",
        "outputId": "98122018-e399-463b-d767-ecb458d9b33d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'actions': [{'cell_id': 0, 'action': 0.2, 'explain': 'L below target by 0.14x', 'blocked': False}], 'global_alert': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_string = '''\n",
        "{\n",
        "  \"actions\": [{\"cell_id\":0,\"action\":0.2,\"explain\":\"L below target by 0.14x\",\"blocked\":false}],\n",
        "  \"global_alert\": false\n",
        "}\n",
        "'''\n",
        "\n",
        "# Parse the JSON string into a Python object\n",
        "data = json.loads(json_string)\n",
        "\n",
        "# You can now work with the 'data' dictionary\n",
        "# For example, print it to see the structure\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT8P2mC6yZIg",
        "outputId": "ac831d78-f2cc-4315-ed8d-46d593fdff19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'actions': [{'cell_id': 0, 'action': 0.2, 'explain': 'L below target by 0.14x', 'blocked': False}], 'global_alert': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing an RL-ready training script (PPO) for the Organ-on-Chip telomere environment.\n",
        "# The script will be saved to /mnt/data/ppo_ooc.py. It defines a Gym-like env and a training harness.\n",
        "# The harness will try to use stable-baselines3 (recommended). If not installed, it will fallback to\n",
        "# a minimal random-rollout tester and print instructions for installing stable-baselines3 + PyTorch.\n",
        "#\n",
        "# This cell only writes the script file; it does not perform heavy training here.\n",
        "\n",
        "script = r'''\n",
        "\"\"\"\n",
        "ppo_ooc.py\n",
        "\n",
        "PPO training harness for the Organ-on-Chip telomere environment.\n",
        "\n",
        "Usage:\n",
        "    - Recommended: install PyTorch and stable-baselines3\n",
        "        pip install torch stable-baselines3[extra] gymnasium\n",
        "\n",
        "    - Then run:\n",
        "        python ppo_ooc.py --train --timesteps 200000 --model_path ./ppo_ooc_model.zip\n",
        "\n",
        "This script:\n",
        " - Defines a Gym-compatible environment `OOCTelomereEnv`\n",
        " - Provides a training harness using stable-baselines3.PPO if available\n",
        " - Implements evaluation and model saving\n",
        " - Includes configurable parameters for the mechanistic model\n",
        "\n",
        "Notes:\n",
        " - The environment exposes continuous action in [0,1] per cell, but for simplicity the action returned\n",
        "   by the agent is a single scalar applied to all cells (this can be extended to per-cell actions).\n",
        " - The script is intentionally self-contained and suitable for running on a workstation.\n",
        " - No wet-lab instructions or DNA sequences are included.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import json\n",
        "import random\n",
        "\n",
        "# ------------------------\n",
        "# Environment definition\n",
        "# ------------------------\n",
        "class OOCTelomereEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A Gym-like environment simulating multiple virtual cells in an organ-on-chip.\n",
        "    Observation: vector containing summary statistics and a sample of cell telomere lengths.\n",
        "    Action: continuous scalar in [0,1] representing global telomerase activation level.\n",
        "    Reward: composite reward encouraging maintenance of telomere lengths without increasing risk.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
        "\n",
        "    def __init__(self, n_cells=32, seed=None):\n",
        "        super().__init__()\n",
        "        self.n_cells = n_cells\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        # Mechanistic params (can be tuned)\n",
        "        self.alpha = 0.8\n",
        "        self.delta_rep_mean = 30.0\n",
        "        self.delta_rep_sd = 5.0\n",
        "        self.sigma_noise = 10.0\n",
        "        self.L_sen = 3000.0\n",
        "        self.L_crit = 1000.0\n",
        "        self.monitor_low = 4000.0\n",
        "        self.monitor_high = 7000.0\n",
        "\n",
        "        # State\n",
        "        self.L = None\n",
        "        self.mut_count = None\n",
        "        self.t = 0\n",
        "        self.max_steps = 200\n",
        "\n",
        "        # Observation space: we'll provide a vector of length 4 + n_sample\n",
        "        # [mean_L, median_L, min_L, max_L, sample_telomeres...]\n",
        "        self.n_sample = min(8, self.n_cells)\n",
        "        obs_dim = 4 + self.n_sample\n",
        "        self.observation_space = spaces.Box(low=0.0, high=1e5, shape=(obs_dim,), dtype=np.float32)\n",
        "\n",
        "        # Action space: single scalar in [0,1]\n",
        "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            self.rng.seed(seed)\n",
        "        # Initialize telomere lengths and mutation counts\n",
        "        initial_L_mean = 8000.0\n",
        "        initial_L_sd = 200.0\n",
        "        self.L = self.rng.normal(initial_L_mean, initial_L_sd, size=self.n_cells)\n",
        "        self.L = np.clip(self.L, 100.0, None)\n",
        "        self.mut_count = np.zeros(self.n_cells, dtype=np.float32)\n",
        "        self.t = 0\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # compute summary stats + sample telomeres\n",
        "        mean_L = float(np.mean(self.L))\n",
        "        median_L = float(np.median(self.L))\n",
        "        min_L = float(np.min(self.L))\n",
        "        max_L = float(np.max(self.L))\n",
        "        # sample first n_sample telomeres (or random sample for variety)\n",
        "        sample = self.L[:self.n_sample]\n",
        "        obs = np.concatenate([[mean_L, median_L, min_L, max_L], sample])\n",
        "        return obs.astype(np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        # action is a scalar in [0,1]; apply to all cells (global control)\n",
        "        a = float(np.clip(np.asarray(action).squeeze(), 0.0, 1.0))\n",
        "        # mechanistic update\n",
        "        for i in range(self.n_cells):\n",
        "            delta_rep = max(0.0, self.rng.normal(self.delta_rep_mean, self.delta_rep_sd))\n",
        "            self.L[i] = self.L[i] - delta_rep + self.alpha * a * 100.0 + self.rng.normal(0, 2.0)\n",
        "            self.L[i] = max(0.0, self.L[i])\n",
        "            # mutation increment proxy\n",
        "            mut_inc = self.rng.poisson(0.02 + 0.0001 * max(0, (self.monitor_low - self.L[i])))\n",
        "            mut_inc += int(self.rng.poisson(0.01 * a))\n",
        "            self.mut_count[i] += mut_inc\n",
        "\n",
        "        # compute risk proxy\n",
        "        beta_action = 0.5\n",
        "        beta_mut = 1.2\n",
        "        risk = np.exp(beta_action * a + beta_mut * np.mean(self.mut_count))\n",
        "\n",
        "        # reward design:\n",
        "        # - penalize cells below senescence threshold\n",
        "        # - penalize high risk\n",
        "        # - reward keeping mean_L near target (e.g., 6500)\n",
        "        mean_L = float(np.mean(self.L))\n",
        "        target = 6500.0\n",
        "        reward_homeostasis = -abs(mean_L - target) / target\n",
        "        penalty_senescent = -np.sum(self.L < self.L_sen) / float(self.n_cells)\n",
        "        penalty_risk = -0.1 * (risk / (1.0 + risk))\n",
        "        reward = 1.0 * reward_homeostasis + 2.0 * penalty_senescent + 1.0 * penalty_risk\n",
        "\n",
        "        self.t += 1\n",
        "        done = bool(self.t >= self.max_steps)\n",
        "        info = {\"mean_L\": mean_L, \"risk\": float(risk), \"mut_mean\": float(np.mean(self.mut_count))}\n",
        "        return self._get_obs(), float(reward), done, False, info\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        print(f\"t={self.t} mean_L={np.mean(self.L):.1f} mut_mean={np.mean(self.mut_count):.2f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Training harness\n",
        "# ------------------------\n",
        "def train_with_sb3(env_id=None, timesteps=100000, model_path=\"./ppo_ooc_model.zip\"):\n",
        "    try:\n",
        "        # import stable-baselines3 and train PPO\n",
        "        from stable_baselines3 import PPO\n",
        "        from stable_baselines3.common.env_util import make_vec_env\n",
        "        from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
        "\n",
        "        # Create vectorized env\n",
        "        def make_env():\n",
        "            return OOCTelomereEnv(n_cells=32)\n",
        "        venv = make_vec_env(lambda: OOCTelomereEnv(n_cells=32), n_envs=4)\n",
        "\n",
        "        model = PPO('MlpPolicy', venv, verbose=1, batch_size=256)\n",
        "        eval_env = OOCTelomereEnv(n_cells=32)\n",
        "        eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
        "                                     log_path='./logs/', eval_freq=5000, n_eval_episodes=5)\n",
        "        checkpoint_cb = CheckpointCallback(save_freq=5000, save_path='./logs/',\n",
        "                                           name_prefix='ppo_ooc_ckpt')\n",
        "        model.learn(total_timesteps=timesteps, callback=[eval_callback, checkpoint_cb])\n",
        "        model.save(model_path)\n",
        "        print(\"Training complete. Model saved to:\", model_path)\n",
        "    except Exception as e:\n",
        "        print(\"stable-baselines3 training failed or not available:\", str(e))\n",
        "        print(\"Please install stable-baselines3 and a supported PyTorch version to run PPO training.\")\n",
        "        print(\"Fallback: you can run random rollouts or implement a simple policy gradient.\")\n",
        "        return\n",
        "\n",
        "def evaluate_model(model_path=None, n_episodes=10):\n",
        "    try:\n",
        "        from stable_baselines3 import PPO\n",
        "        model = PPO.load(model_path)\n",
        "        env = OOCTelomereEnv(n_cells=32)\n",
        "        for ep in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0.0\n",
        "            while not done:\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                obs, reward, done, trunc, info = env.step(action)\n",
        "                total_reward += reward\n",
        "            print(f\"Episode {ep} reward: {total_reward:.3f} info={info}\")\n",
        "    except Exception as e:\n",
        "        print(\"Evaluation failed (stable-baselines3 may be missing):\", str(e))\n",
        "\n",
        "# ------------------------\n",
        "# CLI\n",
        "# ------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train\", action=\"store_true\", help=\"Train PPO model (requires stable-baselines3)\")\n",
        "    parser.add_argument(\"--timesteps\", type=int, default=200000, help=\"Total training timesteps\")\n",
        "    parser.add_argument(\"--model_path\", type=str, default=\"./ppo_ooc_model.zip\", help=\"Model save path\")\n",
        "    parser.add_argument(\"--eval\", action=\"store_true\", help=\"Run a short evaluation using saved model\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.train:\n",
        "        print(\"Starting PPO training (attempting to use stable-baselines3)...\")\n",
        "        train_with_sb3(timesteps=args.timesteps, model_path=args.model_path)\n",
        "    if args.eval:\n",
        "        print(\"Evaluating model:\", args.model_path)\n",
        "        evaluate_model(model_path=args.model_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "out_path = \"/mnt/data/ppo_ooc.py\"\n",
        "with open(out_path, \"w\") as f:\n",
        "    f.write(script)\n",
        "\n",
        "out_path\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pG-uK3l2yZLI",
        "outputId": "85539349-e1bd-40b4-8e43-cdca430afdc7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/ppo_ooc.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch stable-baselines3[extra] gymnasium\n",
        "\n",
        "!python /mnt/data/ppo_ooc.py --train --timesteps 200000 --model_path ./ppo_ooc_model.zip\n",
        "\n",
        "!python /mnt/data/ppo_ooc.py --eval --model_path ./ppo_ooc_model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPsWLtfXy0gp",
        "outputId": "9c9a3272-0b14-4a5e-a5ff-463d6005f0a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.12.0.88)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (0.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (11.3.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.7.0\n",
            "Starting PPO training (attempting to use stable-baselines3)...\n",
            "2025-11-18 21:25:28.714480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763501128.753091    1120 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763501128.765192    1120 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763501128.793351    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763501128.793433    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763501128.793443    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763501128.793451    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-18 21:25:28.801491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "Using cpu device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -51.8    |\n",
            "| time/              |          |\n",
            "|    fps             | 1799     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -46.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1502        |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005846488 |\n",
            "|    clip_fraction        | 0.0503      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.0106      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.697       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00391    |\n",
            "|    std                  | 0.987       |\n",
            "|    value_loss           | 2.4         |\n",
            "-----------------------------------------\n",
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "Eval num_timesteps=20000, episode_reward=-142.30 +/- 3.02\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -142         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 20000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033353923 |\n",
            "|    clip_fraction        | 0.014        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.36         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00124     |\n",
            "|    std                  | 0.985        |\n",
            "|    value_loss           | 3.07         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -42.7    |\n",
            "| time/              |          |\n",
            "|    fps             | 1360     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 18       |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -39.4        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1294         |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017355352 |\n",
            "|    clip_fraction        | 0.0166       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.00112      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.45         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00104     |\n",
            "|    std                  | 0.972        |\n",
            "|    value_loss           | 3.62         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-142.54 +/- 2.67\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -143         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 40000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004437538 |\n",
            "|    clip_fraction        | 0.000916     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.00151      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.09         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | 0.000219     |\n",
            "|    std                  | 0.965        |\n",
            "|    value_loss           | 3.17         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -38.5    |\n",
            "| time/              |          |\n",
            "|    fps             | 1277     |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -38.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1269         |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 38           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014835832 |\n",
            "|    clip_fraction        | 0.00741      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.45         |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.000129    |\n",
            "|    std                  | 0.957        |\n",
            "|    value_loss           | 4.07         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -38.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1289         |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031568431 |\n",
            "|    clip_fraction        | 0.0233       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | 0.000478     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.33         |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00113     |\n",
            "|    std                  | 0.955        |\n",
            "|    value_loss           | 5.04         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-141.94 +/- 2.17\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -142         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 60000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.332502e-05 |\n",
            "|    clip_fraction        | 0.00251      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | 0.00047      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.03         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | 9.38e-05     |\n",
            "|    std                  | 0.958        |\n",
            "|    value_loss           | 4.7          |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -38.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 1250     |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 52       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -38.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1265        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004428624 |\n",
            "|    clip_fraction        | 0.0239      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.81        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.000955   |\n",
            "|    std                  | 0.95        |\n",
            "|    value_loss           | 4.96        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-143.37 +/- 1.30\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -143         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 80000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041986573 |\n",
            "|    clip_fraction        | 0.0279       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.47         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.0016      |\n",
            "|    std                  | 0.939        |\n",
            "|    value_loss           | 6.24         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -39.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 1239     |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 66       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -38.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1249         |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 72           |\n",
            "|    total_timesteps      | 90112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.254894e-05 |\n",
            "|    clip_fraction        | 0.00072      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.85         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | 9.43e-07     |\n",
            "|    std                  | 0.939        |\n",
            "|    value_loss           | 5.44         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -39.2      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1251       |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 78         |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00366326 |\n",
            "|    clip_fraction        | 0.0223     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.35      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.85       |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.00123   |\n",
            "|    std                  | 0.932      |\n",
            "|    value_loss           | 5.53       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-142.43 +/- 3.47\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -142         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 100000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016914684 |\n",
            "|    clip_fraction        | 0.00226      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.35        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.24         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | 3.75e-05     |\n",
            "|    std                  | 0.936        |\n",
            "|    value_loss           | 5.04         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -39.6    |\n",
            "| time/              |          |\n",
            "|    fps             | 1237     |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 86       |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -40.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1242         |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048076017 |\n",
            "|    clip_fraction        | 0.0136       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.35        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.2          |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000565    |\n",
            "|    std                  | 0.932        |\n",
            "|    value_loss           | 5.9          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=-144.12 +/- 3.02\n",
            "Episode length: 200.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 200         |\n",
            "|    mean_reward          | -144        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 120000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002536293 |\n",
            "|    clip_fraction        | 0.00889     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.34       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.16        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.000463   |\n",
            "|    std                  | 0.914       |\n",
            "|    value_loss           | 5.34        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -40.4    |\n",
            "| time/              |          |\n",
            "|    fps             | 1228     |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 100      |\n",
            "|    total_timesteps | 122880   |\n",
            "---------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -39.9      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1236       |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 106        |\n",
            "|    total_timesteps      | 131072     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00544626 |\n",
            "|    clip_fraction        | 0.0216     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.32      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.52       |\n",
            "|    n_updates            | 150        |\n",
            "|    policy_gradient_loss | -0.000861  |\n",
            "|    std                  | 0.898      |\n",
            "|    value_loss           | 5.65       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -39.9        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1230         |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 113          |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010043125 |\n",
            "|    clip_fraction        | 0.0049       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.4          |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.000137    |\n",
            "|    std                  | 0.9          |\n",
            "|    value_loss           | 6            |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=-144.55 +/- 3.11\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -145         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 140000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051775025 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.6          |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.00122     |\n",
            "|    std                  | 0.89         |\n",
            "|    value_loss           | 6.24         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -39.8    |\n",
            "| time/              |          |\n",
            "|    fps             | 1225     |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 120      |\n",
            "|    total_timesteps | 147456   |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -39.9        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1219         |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 127          |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044557676 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.69         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.0013      |\n",
            "|    std                  | 0.889        |\n",
            "|    value_loss           | 6.21         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=-144.49 +/- 2.24\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -144         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 160000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034296783 |\n",
            "|    clip_fraction        | 0.0152       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.86         |\n",
            "|    n_updates            | 190          |\n",
            "|    policy_gradient_loss | -0.00057     |\n",
            "|    std                  | 0.877        |\n",
            "|    value_loss           | 5.83         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -40.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 1207     |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 135      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -40.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1209         |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 142          |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030786083 |\n",
            "|    clip_fraction        | 0.0162       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.54         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.000822    |\n",
            "|    std                  | 0.877        |\n",
            "|    value_loss           | 5.53         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=-141.51 +/- 3.24\n",
            "Episode length: 200.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 200          |\n",
            "|    mean_reward          | -142         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 180000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035100398 |\n",
            "|    clip_fraction        | 0.00916      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.79         |\n",
            "|    n_updates            | 210          |\n",
            "|    policy_gradient_loss | -0.000352    |\n",
            "|    std                  | 0.875        |\n",
            "|    value_loss           | 6.55         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -39.7    |\n",
            "| time/              |          |\n",
            "|    fps             | 1198     |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 150      |\n",
            "|    total_timesteps | 180224   |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -39.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1201         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 156          |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027606979 |\n",
            "|    clip_fraction        | 0.0058       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.94         |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.000199    |\n",
            "|    std                  | 0.87         |\n",
            "|    value_loss           | 6.39         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -38.7        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1197         |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 164          |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044960007 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.28         |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -0.00224     |\n",
            "|    std                  | 0.865        |\n",
            "|    value_loss           | 7.35         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=-142.32 +/- 2.57\n",
            "Episode length: 200.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 200           |\n",
            "|    mean_reward          | -142          |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 200000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038503678 |\n",
            "|    clip_fraction        | 0.000208      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.28         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.05          |\n",
            "|    n_updates            | 240           |\n",
            "|    policy_gradient_loss | 5.48e-05      |\n",
            "|    std                  | 0.877         |\n",
            "|    value_loss           | 6.07          |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -39.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 1196     |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 171      |\n",
            "|    total_timesteps | 204800   |\n",
            "---------------------------------\n",
            "Training complete. Model saved to: ./ppo_ooc_model.zip\n",
            "Evaluating model: ./ppo_ooc_model.zip\n",
            "2025-11-18 21:28:43.221685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763501323.245565    1917 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763501323.253423    1917 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763501323.271327    1917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763501323.271370    1917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763501323.271374    1917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763501323.271378    1917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-18 21:28:43.278296: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "Episode 0 reward: -143.311 info={'mean_L': 2011.694189702606, 'risk': 2084417.875, 'mut_mean': 12.125}\n",
            "Episode 1 reward: -146.284 info={'mean_L': 1962.4833959500052, 'risk': 385578.53125, 'mut_mean': 10.71875}\n",
            "Episode 2 reward: -143.329 info={'mean_L': 1993.507119452369, 'risk': 501320.5, 'mut_mean': 10.9375}\n",
            "Episode 3 reward: -143.648 info={'mean_L': 1982.8620855013278, 'risk': 1432598.0, 'mut_mean': 11.8125}\n",
            "Episode 4 reward: -141.060 info={'mean_L': 2023.6212731568876, 'risk': 275130.4375, 'mut_mean': 10.4375}\n",
            "Episode 5 reward: -148.770 info={'mean_L': 1916.9711993227475, 'risk': 371387.3125, 'mut_mean': 10.6875}\n",
            "Episode 6 reward: -145.451 info={'mean_L': 1969.2624504946912, 'risk': 103777.046875, 'mut_mean': 9.625}\n",
            "Episode 7 reward: -142.218 info={'mean_L': 2003.3558398470095, 'risk': 786226.9375, 'mut_mean': 11.3125}\n",
            "Episode 8 reward: -141.547 info={'mean_L': 2019.92934303779, 'risk': 816269.6875, 'mut_mean': 11.34375}\n",
            "Episode 9 reward: -139.010 info={'mean_L': 2051.8442683082358, 'risk': 482868.96875, 'mut_mean': 10.90625}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write updated environment script with per-cell action space\n",
        "script = r'''\n",
        "\"\"\"\n",
        "ooc_telomere_env_cell_actions.py\n",
        "\n",
        "Organ-on-chip telomere RL environment with per-cell telomerase control.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class OOCTelomereCellActionEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
        "\n",
        "    def __init__(self, n_cells=32, seed=None):\n",
        "        super().__init__()\n",
        "        self.n_cells = n_cells\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "\n",
        "        # parameters\n",
        "        self.delta_rep_mean = 30.0\n",
        "        self.delta_rep_sd = 5.0\n",
        "        self.alpha = 0.8\n",
        "        self.L_sen = 3000.0\n",
        "        self.target = 6500.0\n",
        "        self.max_steps = 200\n",
        "        self.t = 0\n",
        "\n",
        "        # state\n",
        "        self.L = None\n",
        "        self.mut = None\n",
        "\n",
        "        # actions: per-cell telomerase activation [0,1]^n_cells\n",
        "        self.action_space = spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(self.n_cells,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # observation: summary + all telomeres (can be reduced)\n",
        "        obs_dim = 4 + self.n_cells\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0.0, high=1e5, shape=(obs_dim,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            self.rng.seed(seed)\n",
        "        self.L = self.rng.normal(8000.0, 200.0, self.n_cells).clip(100, None)\n",
        "        self.mut = np.zeros(self.n_cells, dtype=np.float32)\n",
        "        self.t = 0\n",
        "        return self._obs(), {}\n",
        "\n",
        "    def _obs(self):\n",
        "        mean_L = float(self.L.mean())\n",
        "        median_L = float(np.median(self.L))\n",
        "        mn = float(self.L.min())\n",
        "        mx = float(self.L.max())\n",
        "        return np.concatenate([[mean_L, median_L, mn, mx], self.L]).astype(np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        a = np.clip(action, 0.0, 1.0)\n",
        "\n",
        "        # per-cell update\n",
        "        delta_rep = np.maximum(\n",
        "            0.0, self.rng.normal(self.delta_rep_mean, self.delta_rep_sd, self.n_cells)\n",
        "        )\n",
        "        self.L = (\n",
        "            self.L\n",
        "            - delta_rep\n",
        "            + self.alpha * a * 100.0\n",
        "            + self.rng.normal(0, 2.0, self.n_cells)\n",
        "        )\n",
        "        self.L = np.clip(self.L, 0.0, None)\n",
        "\n",
        "        # mutation proxy\n",
        "        self.mut += self.rng.poisson(0.02 + 0.0001 * np.maximum(0, 4000 - self.L)))\n",
        "        self.mut += self.rng.poisson(0.01 * a)\n",
        "\n",
        "        # rewards\n",
        "        r_homeo = -abs(self.L.mean() - self.target) / self.target\n",
        "        r_sen = -np.mean(self.L < self.L_sen)\n",
        "        r_mut = -0.1 * np.mean(self.mut)\n",
        "\n",
        "        reward = r_homeo + 2.0 * r_sen + r_mut\n",
        "\n",
        "        self.t += 1\n",
        "        done = self.t >= self.max_steps\n",
        "        info = dict(mean_L=float(self.L.mean()), mut=float(self.mut.mean()))\n",
        "        return self._obs(), float(reward), done, False, info\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        print(f\"t={self.t} mean_L={self.L.mean():.1f} mut={self.mut.mean():.2f}\")\n",
        "'''\n",
        "\n",
        "path = \"/mnt/data/ooc_telomere_env_cell_actions.py\"\n",
        "with open(path, \"w\") as f:\n",
        "    f.write(script)\n",
        "\n",
        "path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aeDve1dIy8Jh",
        "outputId": "096ee6e9-7e75-40b8-9e12-e7bc26e4295a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/ooc_telomere_env_cell_actions.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a multi-organ (multi-chip) environment script for Organ-on-Chip telomere control.\n",
        "# The script will be saved to /mnt/data/multi_ooc_network.py\n",
        "# It defines:\n",
        "#  - OOCTelomereCellActionEnv (imported or redefined inline)\n",
        "#  - MultiOOCTopologyEnv: composes multiple chips and models inter-chip signaling (simple diffusion)\n",
        "#  - Example usage: reset and step through the multi-chip env\n",
        "#\n",
        "# This is purely in-silico simulation code for RL/control research.\n",
        "\n",
        "script = r'''\n",
        "\"\"\"\n",
        "multi_ooc_network.py\n",
        "\n",
        "Multi-organ (multi-chip) Organ-on-Chip telomere environment.\n",
        "\n",
        "Defines:\n",
        " - OOCTelomereCellActionEnv: per-chip environment (redefined inline for standalone use)\n",
        " - MultiOOCTopologyEnv: composes multiple chips and models simple inter-chip signaling\n",
        " - Example usage at the bottom.\n",
        "\n",
        "Notes:\n",
        " - Actions: concatenated per-chip per-cell actions (shape = sum(n_cells_i))\n",
        " - Observations: concatenated summaries per chip\n",
        " - Inter-chip signaling: modeled as cytokine field values that diffuse between chips and affect mutation rates\n",
        " - Intended as simulation backbone for RL/control experiments across connected organ-chips\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# -----------------------\n",
        "# Simple per-chip env (standalone)\n",
        "# -----------------------\n",
        "class OOCTelomereCellActionEnv:\n",
        "    def __init__(self, n_cells=16, seed=None, chip_id=0):\n",
        "        self.n_cells = n_cells\n",
        "        self.chip_id = chip_id\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.delta_rep_mean = 30.0\n",
        "        self.delta_rep_sd = 5.0\n",
        "        self.alpha = 0.8\n",
        "        self.L_sen = 3000.0\n",
        "        self.target = 6500.0\n",
        "        self.max_steps = 200\n",
        "        self.t = 0\n",
        "        self.L = None\n",
        "        self.mut = None\n",
        "        self.cytokine = 0.0  # local cytokine level affecting stress/mutation\n",
        "\n",
        "    def reset(self):\n",
        "        self.L = self.rng.normal(8000.0, 200.0, self.n_cells).clip(100, None)\n",
        "        self.mut = np.zeros(self.n_cells, dtype=np.float32)\n",
        "        self.t = 0\n",
        "        self.cytokine = 0.0\n",
        "        return self._obs()\n",
        "\n",
        "    def _obs(self):\n",
        "        mean_L = float(self.L.mean())\n",
        "        median_L = float(np.median(self.L))\n",
        "        mn = float(self.L.min())\n",
        "        mx = float(self.L.max())\n",
        "        # include cytokine in observation\n",
        "        return np.concatenate([[mean_L, median_L, mn, mx, self.cytokine], self.L]).astype(np.float32)\n",
        "\n",
        "    def step(self, actions):\n",
        "        # actions: vector len n_cells [0,1]\n",
        "        a = np.clip(actions, 0.0, 1.0)\n",
        "        delta_rep = np.maximum(0.0, self.rng.normal(self.delta_rep_mean, self.delta_rep_sd, self.n_cells))\n",
        "        self.L = (\n",
        "            self.L\n",
        "            - delta_rep\n",
        "            + self.alpha * a * 100.0\n",
        "            + self.rng.normal(0, 2.0, self.n_cells)\n",
        "        )\n",
        "        self.L = np.clip(self.L, 0.0, None)\n",
        "\n",
        "        # mutation proxy increases with cytokine (stress) and low telomere\n",
        "        baseline = 0.02 + 0.0001 * np.maximum(0, 4000 - self.L)\n",
        "        cytokine_effect = 0.005 * self.cytokine  # cytokine increases mutation rate\n",
        "        self.mut += self.rng.poisson(baseline + cytokine_effect)\n",
        "        self.mut += self.rng.poisson(0.01 * a)\n",
        "\n",
        "        # rewards per chip\n",
        "        r_homeo = -abs(self.L.mean() - self.target) / self.target\n",
        "        r_sen = -np.mean(self.L < self.L_sen)\n",
        "        r_mut = -0.1 * np.mean(self.mut)\n",
        "        reward = r_homeo + 2.0 * r_sen + r_mut\n",
        "\n",
        "        self.t += 1\n",
        "        done = self.t >= self.max_steps\n",
        "        info = {\"mean_L\": float(self.L.mean()), \"mut_mean\": float(self.mut.mean()), \"cytokine\": float(self.cytokine)}\n",
        "        return self._obs(), float(reward), done, False, info\n",
        "\n",
        "# -----------------------\n",
        "# Multi-chip topology environment\n",
        "# -----------------------\n",
        "class MultiOOCTopologyEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Composes multiple OOCTelomereCellActionEnv instances into a connected topology.\n",
        "    Inter-chip signaling: each chip has a cytokine variable; after each step cytokine diffuses\n",
        "    between connected chips via adjacency matrix and decays over time.\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self, chip_config=[16,16], adjacency=None, seed=None):\n",
        "        \"\"\"\n",
        "        chip_config: list of ints, number of cells per chip\n",
        "        adjacency: square matrix NxN specifying diffusion weights (if None, fully connected small weight)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_chips = len(chip_config)\n",
        "        self.chips = [OOCTelomereCellActionEnv(n_cells=n, seed=(seed or 0)+i, chip_id=i) for i,n in enumerate(chip_config)]\n",
        "        self.chip_config = chip_config\n",
        "        self.total_cells = sum(chip_config)\n",
        "        self.seed = seed\n",
        "        # adjacency matrix\n",
        "        if adjacency is None:\n",
        "            # small all-to-all coupling\n",
        "            self.adjacency = np.full((self.n_chips, self.n_chips), 0.1)\n",
        "            np.fill_diagonal(self.adjacency, 0.0)\n",
        "        else:\n",
        "            self.adjacency = np.array(adjacency, dtype=float)\n",
        "        # cytokine decay\n",
        "        self.decay = 0.9\n",
        "\n",
        "        # gym spaces\n",
        "        # action: concatenated per-chip per-cell actions\n",
        "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(self.total_cells,), dtype=np.float32)\n",
        "        # observation: concatenated observations per chip (each chip obs length = 5 + n_cells)\n",
        "        obs_len_per_chip = [5 + n for n in chip_config]\n",
        "        self.obs_slices = []\n",
        "        total_obs = 0\n",
        "        for l in obs_len_per_chip:\n",
        "            self.obs_slices.append((total_obs, total_obs + l))\n",
        "            total_obs += l\n",
        "        self.observation_space = spaces.Box(low=0.0, high=1e6, shape=(total_obs,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        obs_parts = []\n",
        "        for chip in self.chips:\n",
        "            obs_parts.append(chip.reset())\n",
        "        self._assemble_obs(obs_parts)\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _assemble_obs(self, obs_parts):\n",
        "        # store individual chip obs in self.last_obs_parts\n",
        "        self.last_obs_parts = obs_parts\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.concatenate(self.last_obs_parts).astype(np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        # action: full concatenated vector length total_cells\n",
        "        assert len(action) == self.total_cells, \"Action length mismatch\"\n",
        "        rewards = []\n",
        "        infos = []\n",
        "        dones = []\n",
        "        obs_parts = []\n",
        "        idx = 0\n",
        "        # apply per-chip slices\n",
        "        for chip_i, chip in enumerate(self.chips):\n",
        "            n = chip.n_cells\n",
        "            act_slice = action[idx: idx + n]\n",
        "            obs, r, done, trunc, info = chip.step(act_slice)\n",
        "            rewards.append(r)\n",
        "            infos.append(info)\n",
        "            dones.append(done)\n",
        "            obs_parts.append(obs)\n",
        "            idx += n\n",
        "\n",
        "        # update cytokine diffusion between chips\n",
        "        cytokines = np.array([chip.cytokine for chip in self.chips])\n",
        "        # simple diffusion: new = decay*old + adjacency.dot(old) * small_factor\n",
        "        cytokines = cytokines * self.decay + self.adjacency.dot(cytokines) * 0.05\n",
        "        # write back\n",
        "        for i, chip in enumerate(self.chips):\n",
        "            chip.cytokine = float(cytokines[i])\n",
        "\n",
        "        self._assemble_obs(obs_parts)\n",
        "        total_reward = float(np.sum(rewards))\n",
        "        done = all(dones)\n",
        "        info = {\"per_chip\": infos}\n",
        "        return self._get_obs(), total_reward, done, False, info\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        for i, chip in enumerate(self.chips):\n",
        "            print(f\"Chip {i}: mean_L={chip.L.mean():.1f}, mut_mean={chip.mut.mean():.2f}, cytokine={chip.cytokine:.3f}\")\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # create a small network of 3 chips with heterogenous sizes\n",
        "    chip_config = [12, 20, 16]\n",
        "    # adjacency: chain topology (0<->1<->2)\n",
        "    adjacency = np.array([[0.0, 0.2, 0.0],\n",
        "                          [0.2, 0.0, 0.2],\n",
        "                          [0.0, 0.2, 0.0]])\n",
        "    env = MultiOOCTopologyEnv(chip_config=chip_config, adjacency=adjacency, seed=123)\n",
        "    obs, _ = env.reset()\n",
        "    print(\"Initial obs length:\", len(obs))\n",
        "    # random action example\n",
        "    action = np.random.rand(env.total_cells).astype(np.float32)\n",
        "    obs, reward, done, trunc, info = env.step(action)\n",
        "    print(\"Step reward:\", reward)\n",
        "    env.render()\n",
        "'''\n",
        "\n",
        "path = \"/mnt/data/multi_ooc_network.py\"\n",
        "with open(path, \"w\") as f:\n",
        "    f.write(script)\n",
        "\n",
        "path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qT5T3gz2y8Lt",
        "outputId": "95813551-182e-48dc-b67e-82926b07dfb9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/multi_ooc_network.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "multi_ooc_network.py\n",
        "\n",
        "Multi-organ (multi-chip) Organ-on-Chip telomere environment.\n",
        "\n",
        "Defines:\n",
        " - OOCTelomereCellActionEnv: per-chip environment (redefined inline for standalone use)\n",
        " - MultiOOCTopologyEnv: composes multiple chips and models simple inter-chip signaling\n",
        " - Example usage at the bottom.\n",
        "\n",
        "Notes:\n",
        " - Actions: concatenated per-chip per-cell actions (shape = sum(n_cells_i))\n",
        " - Observations: concatenated summaries per chip\n",
        " - Inter-chip signaling: modeled as cytokine field values that diffuse between chips and affect mutation rates\n",
        " - Intended as simulation backbone for RL/control experiments across connected organ-chips\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# -----------------------\n",
        "# Simple per-chip env (standalone)\n",
        "# -----------------------\n",
        "class OOCTelomereCellActionEnv:\n",
        "    def __init__(self, n_cells=16, seed=None, chip_id=0):\n",
        "        self.n_cells = n_cells\n",
        "        self.chip_id = chip_id\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.delta_rep_mean = 30.0\n",
        "        self.delta_rep_sd = 5.0\n",
        "        self.alpha = 0.8\n",
        "        self.L_sen = 3000.0\n",
        "        self.target = 6500.0\n",
        "        self.max_steps = 200\n",
        "        self.t = 0\n",
        "        self.L = None\n",
        "        self.mut = None\n",
        "        self.cytokine = 0.0  # local cytokine level affecting stress/mutation\n",
        "\n",
        "    def reset(self):\n",
        "        self.L = self.rng.normal(8000.0, 200.0, self.n_cells).clip(100, None)\n",
        "        self.mut = np.zeros(self.n_cells, dtype=np.float32)\n",
        "        self.t = 0\n",
        "        self.cytokine = 0.0\n",
        "        return self._obs()\n",
        "\n",
        "    def _obs(self):\n",
        "        mean_L = float(self.L.mean())\n",
        "        median_L = float(np.median(self.L))\n",
        "        mn = float(self.L.min())\n",
        "        mx = float(self.L.max())\n",
        "        # include cytokine in observation\n",
        "        return np.concatenate([[mean_L, median_L, mn, mx, self.cytokine], self.L]).astype(np.float32)\n",
        "\n",
        "    def step(self, actions):\n",
        "        # actions: vector len n_cells [0,1]\n",
        "        a = np.clip(actions, 0.0, 1.0)\n",
        "        delta_rep = np.maximum(0.0, self.rng.normal(self.delta_rep_mean, self.delta_rep_sd, self.n_cells))\n",
        "        self.L = (\n",
        "            self.L\n",
        "            - delta_rep\n",
        "            + self.alpha * a * 100.0\n",
        "            + self.rng.normal(0, 2.0, self.n_cells)\n",
        "        )\n",
        "        self.L = np.clip(self.L, 0.0, None)\n",
        "\n",
        "        # mutation proxy increases with cytokine (stress) and low telomere\n",
        "        baseline = 0.02 + 0.0001 * np.maximum(0, 4000 - self.L)\n",
        "        cytokine_effect = 0.005 * self.cytokine  # cytokine increases mutation rate\n",
        "        self.mut += self.rng.poisson(baseline + cytokine_effect)\n",
        "        self.mut += self.rng.poisson(0.01 * a)\n",
        "\n",
        "        # rewards per chip\n",
        "        r_homeo = -abs(self.L.mean() - self.target) / self.target\n",
        "        r_sen = -np.mean(self.L < self.L_sen)\n",
        "        r_mut = -0.1 * np.mean(self.mut)\n",
        "        reward = r_homeo + 2.0 * r_sen + r_mut\n",
        "\n",
        "        self.t += 1\n",
        "        done = self.t >= self.max_steps\n",
        "        info = {\"mean_L\": float(self.L.mean()), \"mut_mean\": float(self.mut.mean()), \"cytokine\": float(self.cytokine)}\n",
        "        return self._obs(), float(reward), done, False, info\n",
        "\n",
        "# -----------------------\n",
        "# Multi-chip topology environment\n",
        "# -----------------------\n",
        "class MultiOOCTopologyEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Composes multiple OOCTelomereCellActionEnv instances into a connected topology.\n",
        "    Inter-chip signaling: each chip has a cytokine variable; after each step cytokine diffuses\n",
        "    between connected chips via adjacency matrix and decays over time.\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self, chip_config=[16,16], adjacency=None, seed=None):\n",
        "        \"\"\"\n",
        "        chip_config: list of ints, number of cells per chip\n",
        "        adjacency: square matrix NxN specifying diffusion weights (if None, fully connected small weight)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_chips = len(chip_config)\n",
        "        self.chips = [OOCTelomereCellActionEnv(n_cells=n, seed=(seed or 0)+i, chip_id=i) for i,n in enumerate(chip_config)]\n",
        "        self.chip_config = chip_config\n",
        "        self.total_cells = sum(chip_config)\n",
        "        self.seed = seed\n",
        "        # adjacency matrix\n",
        "        if adjacency is None:\n",
        "            # small all-to-all coupling\n",
        "            self.adjacency = np.full((self.n_chips, self.n_chips), 0.1)\n",
        "            np.fill_diagonal(self.adjacency, 0.0)\n",
        "        else:\n",
        "            self.adjacency = np.array(adjacency, dtype=float)\n",
        "        # cytokine decay\n",
        "        self.decay = 0.9\n",
        "\n",
        "        # gym spaces\n",
        "        # action: concatenated per-chip per-cell actions\n",
        "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(self.total_cells,), dtype=np.float32)\n",
        "        # observation: concatenated observations per chip (each chip obs length = 5 + n_cells)\n",
        "        obs_len_per_chip = [5 + n for n in chip_config]\n",
        "        self.obs_slices = []\n",
        "        total_obs = 0\n",
        "        for l in obs_len_per_chip:\n",
        "            self.obs_slices.append((total_obs, total_obs + l))\n",
        "            total_obs += l\n",
        "        self.observation_space = spaces.Box(low=0.0, high=1e6, shape=(total_obs,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        obs_parts = []\n",
        "        for chip in self.chips:\n",
        "            obs_parts.append(chip.reset())\n",
        "        self._assemble_obs(obs_parts)\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _assemble_obs(self, obs_parts):\n",
        "        # store individual chip obs in self.last_obs_parts\n",
        "        self.last_obs_parts = obs_parts\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.concatenate(self.last_obs_parts).astype(np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        # action: full concatenated vector length total_cells\n",
        "        assert len(action) == self.total_cells, \"Action length mismatch\"\n",
        "        rewards = []\n",
        "        infos = []\n",
        "        dones = []\n",
        "        obs_parts = []\n",
        "        idx = 0\n",
        "        # apply per-chip slices\n",
        "        for chip_i, chip in enumerate(self.chips):\n",
        "            n = chip.n_cells\n",
        "            act_slice = action[idx: idx + n]\n",
        "            obs, r, done, trunc, info = chip.step(act_slice)\n",
        "            rewards.append(r)\n",
        "            infos.append(info)\n",
        "            dones.append(done)\n",
        "            obs_parts.append(obs)\n",
        "            idx += n\n",
        "\n",
        "        # update cytokine diffusion between chips\n",
        "        cytokines = np.array([chip.cytokine for chip in self.chips])\n",
        "        # simple diffusion: new = decay*old + adjacency.dot(old) * small_factor\n",
        "        cytokines = cytokines * self.decay + self.adjacency.dot(cytokines) * 0.05\n",
        "        # write back\n",
        "        for i, chip in enumerate(self.chips):\n",
        "            chip.cytokine = float(cytokines[i])\n",
        "\n",
        "        self._assemble_obs(obs_parts)\n",
        "        total_reward = float(np.sum(rewards))\n",
        "        done = all(dones)\n",
        "        info = {\"per_chip\": infos}\n",
        "        return self._get_obs(), total_reward, done, False, info\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        for i, chip in enumerate(self.chips):\n",
        "            print(f\"Chip {i}: mean_L={chip.L.mean():.1f}, mut_mean={chip.mut.mean():.2f}, cytokine={chip.cytokine:.3f}\")\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # create a small network of 3 chips with heterogenous sizes\n",
        "    chip_config = [12, 20, 16]\n",
        "    # adjacency: chain topology (0<->1<->2)\n",
        "    adjacency = np.array([[0.0, 0.2, 0.0],\n",
        "                          [0.2, 0.0, 0.2],\n",
        "                          [0.0, 0.2, 0.0]])\n",
        "    env = MultiOOCTopologyEnv(chip_config=chip_config, adjacency=adjacency, seed=123)\n",
        "    obs, _ = env.reset()\n",
        "    print(\"Initial obs length:\", len(obs))\n",
        "    # random action example\n",
        "    action = np.random.rand(env.total_cells).astype(np.float32)\n",
        "    obs, reward, done, trunc, info = env.step(action)\n",
        "    print(\"Step reward:\", reward)\n",
        "    env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hehhNfc1y0jB",
        "outputId": "d846bd5d-5744-4c37-8581-167dac42c0c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial obs length: 63\n",
            "Step reward: -0.6983940468148878\n",
            "Chip 0: mean_L=7950.2, mut_mean=0.08, cytokine=0.000\n",
            "Chip 1: mean_L=7985.0, mut_mean=0.00, cytokine=0.000\n",
            "Chip 2: mean_L=8009.6, mut_mean=0.06, cytokine=0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a decentralized multi-agent script for Organ-on-Chip environments.\n",
        "# The script will be saved to /mnt/data/decentralized_ooc_agents.py\n",
        "# It trains one agent per chip on standalone per-chip environments, then demonstrates deployment\n",
        "# by loading the trained agents and running them jointly in the MultiOOCTopologyEnv.\n",
        "#\n",
        "# This cell writes the script file but does not execute heavy training here.\n",
        "\n",
        "script = r'''\n",
        "\"\"\"\n",
        "decentralized_ooc_agents.py\n",
        "\n",
        "Decentralized multi-agent approach for Organ-on-Chip control.\n",
        "\n",
        "Workflow:\n",
        " 1) Train one agent per chip on a standalone per-chip environment (OOCTelomereCellActionEnv).\n",
        " 2) Save each agent's model to disk.\n",
        " 3) Deploy agents jointly in the MultiOOCTopologyEnv by having each agent control its chip\n",
        "    using only local observations (decentralized execution).\n",
        "\n",
        "Notes:\n",
        " - Training uses stable-baselines3.PPO if installed. If not available, the script will print instructions.\n",
        " - Training is independent per chip (decentralized) which simplifies credit assignment and allows\n",
        "   training on different hardware/resources.\n",
        " - Deployment composes saved agents into the multi-chip environment and runs episodes to evaluate joint behavior.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# ---- Per-chip environment (standalone) ----\n",
        "class OOCTelomereCellActionEnv:\n",
        "    def __init__(self, n_cells=16, seed=None, chip_id=0, max_steps=200):\n",
        "        self.n_cells = n_cells\n",
        "        self.chip_id = chip_id\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.delta_rep_mean = 30.0\n",
        "        self.delta_rep_sd = 5.0\n",
        "        self.alpha = 0.8\n",
        "        self.L_sen = 3000.0\n",
        "        self.target = 6500.0\n",
        "        self.max_steps = max_steps\n",
        "        self.t = 0\n",
        "        self.L = None\n",
        "        self.mut = None\n",
        "        self.cytokine = 0.0  # for standalone training can be 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.L = self.rng.normal(8000.0, 200.0, self.n_cells).clip(100, None)\n",
        "        self.mut = np.zeros(self.n_cells, dtype=np.float32)\n",
        "        self.t = 0\n",
        "        self.cytokine = 0.0\n",
        "        return self._obs()\n",
        "\n",
        "    def _obs(self):\n",
        "        mean_L = float(self.L.mean())\n",
        "        median_L = float(np.median(self.L))\n",
        "        mn = float(self.L.min())\n",
        "        mx = float(self.L.max())\n",
        "        return np.concatenate([[mean_L, median_L, mn, mx, self.cytokine], self.L]).astype(np.float32)\n",
        "\n",
        "    def step(self, actions):\n",
        "        a = np.clip(actions, 0.0, 1.0)\n",
        "        delta_rep = np.maximum(0.0, self.rng.normal(self.delta_rep_mean, self.delta_rep_sd, self.n_cells))\n",
        "        self.L = (\n",
        "            self.L\n",
        "            - delta_rep\n",
        "            + self.alpha * a * 100.0\n",
        "            + self.rng.normal(0, 2.0, self.n_cells)\n",
        "        )\n",
        "        self.L = np.clip(self.L, 0.0, None)\n",
        "        baseline = 0.02 + 0.0001 * np.maximum(0, 4000 - self.L)\n",
        "        self.mut += self.rng.poisson(baseline)\n",
        "        self.mut += self.rng.poisson(0.01 * a)\n",
        "        r_homeo = -abs(self.L.mean() - self.target) / self.target\n",
        "        r_sen = -np.mean(self.L < self.L_sen)\n",
        "        r_mut = -0.1 * np.mean(self.mut)\n",
        "        reward = r_homeo + 2.0 * r_sen + r_mut\n",
        "        self.t += 1\n",
        "        done = self.t >= self.max_steps\n",
        "        info = {\"mean_L\": float(self.L.mean()), \"mut_mean\": float(self.mut.mean())}\n",
        "        return self._obs(), float(reward), done, False, info\n",
        "\n",
        "# ---- Multi-chip topology environment (imported from multi_ooc_network if available) ----\n",
        "try:\n",
        "    from multi_ooc_network import MultiOOCTopologyEnv\n",
        "except Exception:\n",
        "    # Provide a minimal fallback implementation (subset of functionality)\n",
        "    import gymnasium as gym\n",
        "    from gymnasium import spaces\n",
        "\n",
        "    class MultiOOCTopologyEnv(gym.Env):\n",
        "        def __init__(self, chip_config=[16,16], adjacency=None, seed=None):\n",
        "            super().__init__()\n",
        "            self.chip_config = list(chip_config)\n",
        "            self.n_chips = len(self.chip_config)\n",
        "            self.chips = [OOCTelomereCellActionEnv(n_cells=n, seed=(seed or 0)+i, chip_id=i) for i,n in enumerate(self.chip_config)]\n",
        "            self.total_cells = sum(self.chip_config)\n",
        "            self.action_space = spaces.Box(low=0.0, high=1.0, shape=(self.total_cells,), dtype=np.float32)\n",
        "            obs_len_per_chip = [5 + n for n in self.chip_config]\n",
        "            total_obs = sum(obs_len_per_chip)\n",
        "            self.observation_space = spaces.Box(low=0.0, high=1e6, shape=(total_obs,), dtype=np.float32)\n",
        "            # simple adjacency\n",
        "            if adjacency is None:\n",
        "                self.adjacency = np.full((self.n_chips, self.n_chips), 0.1)\n",
        "                np.fill_diagonal(self.adjacency, 0.0)\n",
        "            else:\n",
        "                self.adjacency = np.array(adjacency, dtype=float)\n",
        "            self.decay = 0.9\n",
        "\n",
        "        def reset(self, seed=None, options=None):\n",
        "            obs_parts = []\n",
        "            for chip in self.chips:\n",
        "                obs_parts.append(chip.reset())\n",
        "            self.last_obs_parts = obs_parts\n",
        "            return self._get_obs(), {}\n",
        "\n",
        "        def _get_obs(self):\n",
        "            return np.concatenate(self.last_obs_parts).astype(np.float32)\n",
        "\n",
        "        def step(self, action):\n",
        "            rewards = []\n",
        "            obs_parts = []\n",
        "            idx = 0\n",
        "            for chip in self.chips:\n",
        "                n = chip.n_cells\n",
        "                act_slice = action[idx: idx + n]\n",
        "                obs, r, done, trunc, info = chip.step(act_slice)\n",
        "                rewards.append(r)\n",
        "                obs_parts.append(obs)\n",
        "                idx += n\n",
        "            # simple cytokine diffusion (not used during decentralized training)\n",
        "            cytokines = np.array([chip.cytokine for chip in self.chips])\n",
        "            cytokines = cytokines * self.decay + self.adjacency.dot(cytokines) * 0.05\n",
        "            for i, chip in enumerate(self.chips):\n",
        "                chip.cytokine = float(cytokines[i])\n",
        "            self.last_obs_parts = obs_parts\n",
        "            return self._get_obs(), float(np.sum(rewards)), all([chip.t >= chip.max_steps for chip in self.chips]), False, {\"per_chip\": [{\"mean_L\": c.L.mean(), \"mut_mean\": c.mut.mean()} for c in self.chips]}\n",
        "\n",
        "# ---- Training harness per-chip (decentralized training) ----\n",
        "def train_per_chip_agents(chip_config=[16,16], timesteps_per_agent=100000, save_dir=\"./agents\"):\n",
        "    \"\"\"\n",
        "    Train one PPO agent per chip on standalone OOCTelomereCellActionEnv instances.\n",
        "    Each agent controls only its chip (per-cell actions).\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    try:\n",
        "        from stable_baselines3 import PPO\n",
        "        from stable_baselines3.common.env_checker import check_env\n",
        "        from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "        import gymnasium as gym\n",
        "        print(\"stable-baselines3 detected. Training agents...\")\n",
        "\n",
        "        for i, n_cells in enumerate(chip_config):\n",
        "            print(f\"Preparing training for agent_{i} (n_cells={n_cells})...\")\n",
        "            # wrap standalone per-chip env for SB3\n",
        "            def make_env(seed=0, n=n_cells, chip_id=i):\n",
        "                def _thunk():\n",
        "                    env = PerChipGymWrapper(n, seed=seed, chip_id=chip_id)\n",
        "                    return env\n",
        "                return _thunk\n",
        "\n",
        "            vec_env = DummyVecEnv([make_env(seed=42+i) for _ in range(4)])  # 4 parallel envs\n",
        "            model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
        "            model.learn(total_timesteps=timesteps_per_agent)\n",
        "            model_path = os.path.join(save_dir, f\"agent_chip_{i}.zip\")\n",
        "            model.save(model_path)\n",
        "            print(f\"Saved agent {i} to {model_path}\")\n",
        "        print(\"All agents trained and saved.\")\n",
        "    except Exception as e:\n",
        "        print(\"stable-baselines3 not available or training failed:\", str(e))\n",
        "        print(\"Fallback: create random policy placeholders (no training).\")\n",
        "        # create placeholder random policies (numpy files with seed)\n",
        "        for i, n_cells in enumerate(chip_config):\n",
        "            placeholder = {\"n_cells\": n_cells, \"seed\": int(42+i)}\n",
        "            np.save(os.path.join(save_dir, f\"agent_chip_{i}_placeholder.npy\"), placeholder)\n",
        "        print(\"Saved placeholder policies to\", save_dir)\n",
        "\n",
        "# ---- Minimal Gym wrapper for standalone per-chip env for SB3 compatibility ----\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "class PerChipGymWrapper(gym.Env):\n",
        "    def __init__(self, n_cells=16, seed=None, chip_id=0):\n",
        "        super().__init__()\n",
        "        self.inner = OOCTelomereCellActionEnv(n_cells=n_cells, seed=seed, chip_id=chip_id)\n",
        "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(n_cells,), dtype=np.float32)\n",
        "        obs_len = 5 + n_cells\n",
        "        self.observation_space = spaces.Box(low=0.0, high=1e6, shape=(obs_len,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        obs = self.inner.reset()\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, trunc, info = self.inner.step(action)\n",
        "        return obs, reward, done, trunc, info\n",
        "\n",
        "# ---- Deployment: load per-chip agents and run in MultiOOCTopologyEnv ----\n",
        "def deploy_agents_in_multi_env(chip_config=[16,16], agents_dir=\"./agents\", episodes=5):\n",
        "    \"\"\"\n",
        "    Load saved agents (SB3 models or placeholders) and run episodes in the multi-chip env.\n",
        "    Each agent receives local observation (its chip's subvector) and outputs per-cell actions.\n",
        "    \"\"\"\n",
        "    # create multi env\n",
        "    env = MultiOOCTopologyEnv(chip_config=chip_config, seed=123)\n",
        "    total_cells = env.total_cells\n",
        "\n",
        "    # load agents\n",
        "    agents = []\n",
        "    for i in range(len(chip_config)):\n",
        "        model_path = os.path.join(agents_dir, f\"agent_chip_{i}.zip\")\n",
        "        if os.path.exists(model_path):\n",
        "            try:\n",
        "                from stable_baselines3 import PPO\n",
        "                model = PPO.load(model_path)\n",
        "                agents.append((\"sb3\", model))\n",
        "                print(f\"Loaded SB3 agent for chip {i}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(\"Failed to load SB3 model:\", e)\n",
        "        # fallback: try placeholder\n",
        "        placeholder_path = os.path.join(agents_dir, f\"agent_chip_{i}_placeholder.npy\")\n",
        "        if os.path.exists(placeholder_path):\n",
        "            placeholder = np.load(placeholder_path, allow_pickle=True).item()\n",
        "            agents.append((\"placeholder\", placeholder))\n",
        "            print(f\"Loaded placeholder for chip {i}\")\n",
        "        else:\n",
        "            agents.append((\"random\", {\"n_cells\": chip_config[i]}))\n",
        "            print(f\"No agent found for chip {i}; using random policy\")\n",
        "\n",
        "    # run episodes\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "        while not done:\n",
        "            # split observations per chip\n",
        "            actions = []\n",
        "            idx = 0\n",
        "            for i, n in enumerate(chip_config):\n",
        "                obs_start = sum([5 + x for x in chip_config[:i]])\n",
        "                obs_end = obs_start + 5 + n\n",
        "                local_obs = obs[obs_start:obs_end]\n",
        "                agent_type, agent_obj = agents[i]\n",
        "                if agent_type == \"sb3\":\n",
        "                    # SB3 models expect flattened observations; we pass local_obs\n",
        "                    action, _ = agent_obj.predict(local_obs, deterministic=True)\n",
        "                elif agent_type == \"placeholder\":\n",
        "                    seed = int(agent_obj[\"seed\"])\n",
        "                    rng = np.random.RandomState(seed)\n",
        "                    action = rng.rand(n).astype(np.float32) * 0.1  # low random activation as placeholder\n",
        "                else:\n",
        "                    action = np.random.rand(n).astype(np.float32) * 0.1\n",
        "                actions.append(action)\n",
        "            # concatenate and step\n",
        "            full_action = np.concatenate(actions)\n",
        "            obs, reward, done, trunc, info = env.step(full_action)\n",
        "            ep_reward += reward\n",
        "        print(f\"Episode {ep} total reward: {ep_reward:.3f}\")\n",
        "\n",
        "# ---- CLI ----\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train\", action=\"store_true\", help=\"Train per-chip agents (requires stable-baselines3)\")\n",
        "    parser.add_argument(\"--timesteps\", type=int, default=100000, help=\"Timesteps per agent\")\n",
        "    parser.add_argument(\"--deploy\", action=\"store_true\", help=\"Deploy agents in multi-chip env\")\n",
        "    parser.add_argument(\"--agents_dir\", type=str, default=\"./agents\", help=\"Directory to save/load agents\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    chip_config = [12, 20, 16]  # example heterogenous chips\n",
        "    if args.train:\n",
        "        train_per_chip_agents(chip_config=chip_config, timesteps_per_agent=args.timesteps, save_dir=args.agents_dir)\n",
        "    if args.deploy:\n",
        "        deploy_agents_in_multi_env(chip_config=chip_config, agents_dir=args.agents_dir, episodes=3)\n",
        "'''\n",
        "\n",
        "path = \"/mnt/data/decentralized_ooc_agents.py\"\n",
        "with open(path, \"w\") as f:\n",
        "    f.write(script)\n",
        "\n",
        "path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EOPMtZj81goR",
        "outputId": "d0807f13-cff2-417f-da4f-39ca793e8c30"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/decentralized_ooc_agents.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qINilyn02Pwq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}