{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfFGCrpMy89NQgPqgheXl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasNazareth1993-coder/Virtual-cell/blob/main/Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N2E7T1_e7n3N"
      },
      "outputs": [],
      "source": [
        "# # Pseudocode (conceptual) — safe to run in silico\n",
        "# initialize_simulator()\n",
        "# initialize_sequence_population()\n",
        "# initialize_policy_agent()\n",
        "\n",
        "# for iteration in range(N_iterations):\n",
        "#     # 1) Propose telomere variants\n",
        "#     candidates = propose_sequences(sequence_population)\n",
        "\n",
        "#     # 2) For each candidate run organ-on-chip simulation with policy agent\n",
        "#     results = []\n",
        "#     for seq in candidates:\n",
        "#         simulator.load_telomere_model(seq)\n",
        "#         # policy_agent may be pre-trained or co-trained\n",
        "#         sim_outcome = run_simulation(simulator, policy_agent, steps=T)\n",
        "#         results.append((seq, sim_outcome.metrics))\n",
        "\n",
        "#     # 3) Evaluate and update sequence optimizer\n",
        "#     sequence_population = update_sequence_population(results)\n",
        "\n",
        "#     # 4) Train/finetune policy agent on collected trajectories\n",
        "#     policy_agent.update(from_trajectories(results))\n",
        "\n",
        "#     # 5) Log metrics; enforce safety checks\n",
        "#     if detect_unacceptable_risk(results):\n",
        "#         apply_safety_mitigation()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.5\n",
        "beta = 0.2\n",
        "gamma = 0.3\n",
        "delta = 0.1\n",
        "\n",
        "# Placeholder values for demonstration; please replace with actual simulation outputs/metrics\n",
        "organ_function_t = 0.8\n",
        "organ_function_tminus1 = 0.75\n",
        "oncogenic_risk_proxy = 0.1\n",
        "change_in_senescent_fraction = 0.05\n",
        "telomerase_activation_cost = 0.02\n",
        "\n",
        "reward = (alpha * (organ_function_t - organ_function_tminus1)\n",
        "         - beta * oncogenic_risk_proxy\n",
        "         - gamma * change_in_senescent_fraction\n",
        "         - delta * telomerase_activation_cost)"
      ],
      "metadata": {
        "id": "ksfC8u037p_5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"StabilityScore\":                0–1,\n",
        "  \"TelomeraseAffinityScore\":       0–1,\n",
        "  \"FragilityRiskScore\":            0–1 (higher = worse),\n",
        "  \"SecondaryStructureScore\":       0–1,\n",
        "  \"ImmuneTriggerScore\":            0–1 (higher = worse)\n",
        "}\n"
      ],
      "metadata": {
        "id": "GnwkhmJm7r3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: telomere sequence (e.g., “TTAGGGTTAGGC…”)\n",
        "# → one-hot encode (A,T,G,C)\n",
        "\n",
        "\n",
        "# fused = concat(sequence_embedding, cell_state_embedding)\n",
        "\n",
        "\n",
        "# {\n",
        "#   \"sequence\": \"...\",\n",
        "#   \"environment\": { debris_load, phag_activity, cell_stress },\n",
        "#   \"true_simulated_outcomes\": { stability, affinity, fragility, etc. }\n",
        "# }\n",
        "\n",
        "\n",
        "# if StabilityScore > 0.7 and TelomeraseAffinityScore > 0.6:\n",
        "#       allow_low_activation()\n",
        "# elif FragilityRiskScore > 0.5:\n",
        "#       block_activation()\n",
        "# else:\n",
        "#       cautious_mode()"
      ],
      "metadata": {
        "id": "nq7P2CRp7r52"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 1) TEL-ML MODEL (SEQUENCE + CONTEXT PREDICTOR)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "class TelomerePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN for local motif detection\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(4, 32, kernel_size=6, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=6, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Transformer encoder for long-range interactions\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=64, nhead=4, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        # MLP for contextual/environment features\n",
        "        self.context_mlp = nn.Sequential(\n",
        "            nn.Linear(10, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32)\n",
        "        )\n",
        "\n",
        "        # Fusion + prediction head\n",
        "        self.pred_head = nn.Sequential(\n",
        "            nn.Linear(64 + 32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 5),   # 5 prediction outputs\n",
        "            nn.Sigmoid()        # normalize to 0–1\n",
        "        )\n",
        "\n",
        "    def forward(self, seq_onehot, context_vec):\n",
        "        # seq_onehot shape: [B, L, 4]\n",
        "        x = seq_onehot.permute(0, 2, 1)  # → [B, 4, L]\n",
        "        x = self.cnn(x)                  # → [B, 64, L]\n",
        "        x = x.permute(0, 2, 1)           # → [B, L, 64]\n",
        "        x = self.transformer(x)          # → [B, L, 64]\n",
        "        seq_embed = x.mean(dim=1)        # pooled representation\n",
        "\n",
        "        context_embed = self.context_mlp(context_vec)\n",
        "\n",
        "        fused = torch.cat([seq_embed, context_embed], dim=1)\n",
        "        return self.pred_head(fused)"
      ],
      "metadata": {
        "id": "YJ9FAi8j7r8B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# MULTI-TASK + MULTI-HEAD ATTENTION VARIANT (SAFE, CONCEPTUAL)\n",
        "# =========================================================\n",
        "\n",
        "class MultiTaskTelomereModel(nn.Module):\n",
        "    def __init__(self, num_context_features=12,\n",
        "                 d_model=96, num_heads=6, depth=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # -----------------------\n",
        "        # 1) CNN for sequence motifs\n",
        "        # -----------------------\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(4, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, d_model, kernel_size=5, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # -----------------------\n",
        "        # 2) Transformer for multi-head attention\n",
        "        # -----------------------\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=4*d_model,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=depth\n",
        "        )\n",
        "\n",
        "        # -----------------------\n",
        "        # 3) Context MLP (Organ-on-chip + cell-state)\n",
        "        # -----------------------\n",
        "        self.context_mlp = nn.Sequential(\n",
        "            nn.Linear(num_context_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, d_model)\n",
        "        )\n",
        "\n",
        "        # -----------------------\n",
        "        # 4) Shared fusion layer\n",
        "        # -----------------------\n",
        "        self.shared_fusion = nn.Sequential(\n",
        "            nn.Linear(2*d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # -----------------------\n",
        "        # 5) Task-Specific Heads\n",
        "        # -----------------------\n",
        "        self.stability_head  = self._make_task_head()\n",
        "        self.affinity_head   = self._make_task_head()\n",
        "        self.fragility_head  = self._make_task_head()\n",
        "        self.structure_head  = self._make_task_head()\n",
        "        self.immune_head     = self._make_task_head()\n",
        "\n",
        "    # small task-specific prediction module\n",
        "    def _make_task_head(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(96, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, seq_onehot, context_vec):\n",
        "        # ----- CNN -----\n",
        "        x = seq_onehot.permute(0, 2, 1)  # [B, 4, L]\n",
        "        x = self.cnn(x)                  # [B, d_model, L]\n",
        "        x = x.permute(0, 2, 1)           # [B, L, d_model]\n",
        "\n",
        "        # ----- Transformer -----\n",
        "        x = self.transformer(x)\n",
        "        seq_embed = x.mean(dim=1)\n",
        "\n",
        "        # ----- Context -----\n",
        "        cont_embed = self.context_mlp(context_vec)\n",
        "\n",
        "        # ----- Shared fusion -----\n",
        "        fused = torch.cat([seq_embed, cont_embed], dim=1)\n",
        "        fused = self.shared_fusion(fused)\n",
        "\n",
        "        # ----- Multi-task outputs -----\n",
        "        return {\n",
        "            \"StabilityScore\":        self.stability_head(fused),\n",
        "            \"TelomeraseAffinity\":    self.affinity_head(fused),\n",
        "            \"FragilityRisk\":         self.fragility_head(fused),\n",
        "            \"SecondaryStructure\":    self.structure_head(fused),\n",
        "            \"ImmuneTrigger\":         self.immune_head(fused)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "O0gP6Cn18Bdp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = (w1*MSE(Stability)\n",
        "#       + w2*MSE(Affinity)\n",
        "#       + w3*MSE(Fragility)\n",
        "#       + w4*MSE(Structure)\n",
        "#       + w5*MSE(Immune))\n",
        "# # SharedRepresentation → Multi-Head Cross-Attention → Task Heads"
      ],
      "metadata": {
        "id": "LZiw_H4C8UYq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a full training pipeline + synthetic dataset generator for the\n",
        "# multi-task, multi-head telomere predictor (simulation-only, safe).\n",
        "#\n",
        "# This notebook:\n",
        "# 1) Generates a synthetic dataset of telomere-like sequences + context features\n",
        "# 2) Builds a multi-task model (CNN + Transformer + task heads)\n",
        "# 3) Trains the model on synthetic targets (stability, affinity, fragility, structure, immune)\n",
        "# 4) Evaluates and saves model + dataset\n",
        "# 5) Displays sample data and training curves\n",
        "#\n",
        "# Outputs saved to /mnt/data:\n",
        "# - /mnt/data/synthetic_telomere_dataset.csv\n",
        "# - /mnt/data/telomere_multitask_model.pth\n",
        "# - /mnt/data/training_history.npy\n",
        "#\n",
        "# NOTE: This code is purely computational and does NOT produce any wet-lab or\n",
        "# actionable biological instructions. It's safe to run and intended for in-silico simulation.\n",
        "\n",
        "# Execute the training pipeline\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For interactive dataframe display in the notebook UI\n",
        "try:\n",
        "    from caas_jupyter_tools import display_dataframe_to_user\n",
        "except Exception:\n",
        "    display_dataframe_to_user = None\n",
        "\n",
        "# reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "OUTDIR = Path(\"/mnt/data\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Synthetic dataset generator\n",
        "# -------------------------\n",
        "BASE_REPEAT = \"TTAGGG\"  # reference repeat (human-like) used only as inspiration\n",
        "NUCLEOTIDES = ['A', 'T', 'G', 'C']\n",
        "\n",
        "def mutate_repeat_unit(base, mutation_rate=0.1):\n",
        "    \"\"\"Mutate a base repeat unit by randomly substituting bases with probability mutation_rate.\"\"\"\n",
        "    out = []\n",
        "    for ch in base:\n",
        "        if random.random() < mutation_rate:\n",
        "            out.append(random.choice(NUCLEOTIDES))\n",
        "        else:\n",
        "            out.append(ch)\n",
        "    return ''.join(out)\n",
        "\n",
        "def build_sequence(repeat_unit, repeats):\n",
        "    return repeat_unit * repeats\n",
        "\n",
        "def sequence_entropy(seq):\n",
        "    counts = {n: seq.count(n) for n in NUCLEOTIDES}\n",
        "    probs = np.array(list(counts.values())) / len(seq)\n",
        "    probs = probs[probs > 0]\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def g_rich_runs_score(seq):\n",
        "    # proxy for G-quadruplex-like propensity: count occurrences of \"GGG\" and longer runs\n",
        "    score = 0\n",
        "    run = 0\n",
        "    for ch in seq:\n",
        "        if ch == 'G':\n",
        "            run += 1\n",
        "        else:\n",
        "            if run >= 3:\n",
        "                score += run\n",
        "            run = 0\n",
        "    if run >= 3:\n",
        "        score += run\n",
        "    return score / max(1, len(seq)/6)  # normalize by length/6\n",
        "\n",
        "def palindromic_score(seq, k=6):\n",
        "    # count small palindromic windows as proxy for hairpins\n",
        "    score = 0\n",
        "    for i in range(len(seq)-k+1):\n",
        "        window = seq[i:i+k]\n",
        "        # simple reverse complement check\n",
        "        rc = window[::-1].translate(str.maketrans(\"ATGC\",\"TACG\"))\n",
        "        if rc == window:\n",
        "            score += 1\n",
        "    return score / max(1, len(seq)/k)\n",
        "\n",
        "def motif_purity_score(seq, repeat_unit):\n",
        "    # fraction of sequence that matches the repeated repeat_unit perfectly when tiled\n",
        "    L = len(repeat_unit)\n",
        "    perfect = 0\n",
        "    for i in range(0, len(seq), L):\n",
        "        block = seq[i:i+L]\n",
        "        if block == repeat_unit:\n",
        "            perfect += 1\n",
        "    return perfect * L / len(seq)\n",
        "\n",
        "def synth_targets_from_sequence(seq, repeat_unit, context_vec):\n",
        "    \"\"\"\n",
        "    Heuristic synthetic target generator. Produces five values in [0,1]:\n",
        "    StabilityScore, TelomeraseAffinityScore, FragilityRiskScore,\n",
        "    SecondaryStructureScore, ImmuneTriggerScore\n",
        "    \"\"\"\n",
        "    length = len(seq)\n",
        "    repeats = length / max(1, len(repeat_unit))\n",
        "    purity = motif_purity_score(seq, repeat_unit)  # 0..1\n",
        "    entropy = sequence_entropy(seq) / 2.0  # normalize roughly\n",
        "    gscore = g_rich_runs_score(seq)  # normalized proxy\n",
        "    palin = palindromic_score(seq)\n",
        "    variability = 1.0 - purity\n",
        "\n",
        "    # Context modifiers (safe, synthetic)\n",
        "    debris, phag_activity, inflammation, replication_stress = context_vec[:4]\n",
        "\n",
        "    # Stability: higher with purity, length; lowered by gscore/palin/replication stress\n",
        "    stability_raw = 0.5 * purity + 0.4 * (math.tanh((repeats-8)/8)+1)/2 - 0.3 * gscore - 0.2 * palin - 0.2 * replication_stress\n",
        "    stability = float(1/(1+math.exp(-stability_raw)))  # sigmoid-ish to 0..1\n",
        "\n",
        "    # Telomerase affinity: higher for periodicity & moderate G-richness; penalize high entropy\n",
        "    affinity_raw = 0.6 * purity + 0.2 * (math.tanh(gscore)+1)/2 - 0.3 * entropy + 0.2 * (1-replication_stress)\n",
        "    affinity = float(1/(1+math.exp(-affinity_raw)))\n",
        "\n",
        "    # Fragility risk: increased by variability, palindromes, and high replication stress\n",
        "    frag_raw = 0.5 * variability + 0.4 * palin + 0.3 * replication_stress + 0.2 * entropy\n",
        "    frag = float(1/(1+math.exp(- (frag_raw - 0.5) )))\n",
        "\n",
        "    # Secondary structure score: proxy from gscore + palin\n",
        "    sec_raw = 0.6 * gscore + 0.4 * palin\n",
        "    sec = float(1/(1+math.exp(- (sec_raw - 0.3) )))\n",
        "\n",
        "    # Immune trigger: higher entropy and exposed non-native motifs, increased by inflammation\n",
        "    imm_raw = 0.5 * entropy + 0.3 * variability + 0.4 * inflammation\n",
        "    imm = float(1/(1+math.exp(- (imm_raw - 0.2) )))\n",
        "\n",
        "    # Add small noise\n",
        "    stability = min(max(stability + np.random.normal(0, 0.02), 0.0), 1.0)\n",
        "    affinity = min(max(affinity + np.random.normal(0, 0.03), 0.0), 1.0)\n",
        "    frag = min(max(frag + np.random.normal(0, 0.03), 0.0), 1.0)\n",
        "    sec = min(max(sec + np.random.normal(0, 0.02), 0.0), 1.0)\n",
        "    imm = min(max(imm + np.random.normal(0, 0.03), 0.0), 1.0)\n",
        "\n",
        "    return stability, affinity, frag, sec, imm\n",
        "\n",
        "# Generate dataset parameters\n",
        "N_SAMPLES = 4000\n",
        "MIN_REPEATS = 5\n",
        "MAX_REPEATS = 60\n",
        "MAX_SEQ_LEN = MAX_REPEATS * len(BASE_REPEAT)  # for padding\n",
        "\n",
        "records = []\n",
        "sequences = []\n",
        "\n",
        "for i in range(N_SAMPLES):\n",
        "    # choose a repeat unit by mutating base with variable mutation rate\n",
        "    mut_rate = random.choice([0.0, 0.05, 0.1, 0.2])\n",
        "    ru = mutate_repeat_unit(BASE_REPEAT, mutation_rate=mut_rate)\n",
        "    repeats = random.randint(MIN_REPEATS, MAX_REPEATS)\n",
        "    seq = build_sequence(ru, repeats)\n",
        "\n",
        "    # randomly insert occasional random short motifs to increase diversity\n",
        "    if random.random() < 0.15:\n",
        "        pos = random.randint(0, max(0, len(seq)-10))\n",
        "        insert = ''.join(random.choices(NUCLEOTIDES, k=6))\n",
        "        seq = seq[:pos] + insert + seq[pos:]\n",
        "\n",
        "    # context features (synthetic)\n",
        "    debris = random.random()  # 0-1\n",
        "    phag_activity = random.random()\n",
        "    inflammation = random.random()\n",
        "    replication_stress = random.random()\n",
        "    # additional context features (stem cell fraction, local nutrients, oxygen proxy, time)\n",
        "    stem_frac = random.random()\n",
        "    nutrients = random.random()\n",
        "    oxygen = random.random()\n",
        "    time_of_day = random.random()\n",
        "\n",
        "    context_vec = [debris, phag_activity, inflammation, replication_stress,\n",
        "                   stem_frac, nutrients, oxygen, time_of_day, random.random(), random.random(), 0.0, 0.0][:12]\n",
        "\n",
        "    stability, affinity, frag, sec, imm = synth_targets_from_sequence(seq, ru, context_vec)\n",
        "\n",
        "    records.append({\n",
        "        \"sequence\": seq,\n",
        "        \"repeat_unit\": ru,\n",
        "        \"repeats\": repeats,\n",
        "        \"length\": len(seq),\n",
        "        \"purity\": motif_purity_score(seq, ru),\n",
        "        \"entropy\": sequence_entropy(seq),\n",
        "        \"gscore\": g_rich_runs_score(seq),\n",
        "        \"palin\": palindromic_score(seq),\n",
        "        \"debris\": debris,\n",
        "        \"phag_activity\": phag_activity,\n",
        "        \"inflammation\": inflammation,\n",
        "        \"replication_stress\": replication_stress,\n",
        "        \"stem_frac\": stem_frac,\n",
        "        \"nutrients\": nutrients,\n",
        "        \"oxygen\": oxygen,\n",
        "        \"time_of_day\": time_of_day,\n",
        "        \"StabilityScore\": stability,\n",
        "        \"TelomeraseAffinityScore\": affinity,\n",
        "        \"FragilityRiskScore\": frag,\n",
        "        \"SecondaryStructureScore\": sec,\n",
        "        \"ImmuneTriggerScore\": imm\n",
        "    })\n",
        "    sequences.append(seq)\n",
        "\n",
        "df = pd.DataFrame.from_records(records)\n",
        "# Save CSV for external inspection\n",
        "csv_path = OUTDIR / \"synthetic_telomere_dataset.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset and dataloader\n",
        "# -------------------------\n",
        "# One-hot encode sequences and pad to MAX_SEQ_LEN\n",
        "def one_hot_encode_seq(seq, max_len=MAX_SEQ_LEN):\n",
        "    mapping = {'A':0,'T':1,'G':2,'C':3}\n",
        "    arr = np.zeros((max_len, 4), dtype=np.float32)\n",
        "    for i, ch in enumerate(seq[:max_len]):\n",
        "        if ch in mapping:\n",
        "            arr[i, mapping[ch]] = 1.0\n",
        "    return arr\n",
        "\n",
        "# Build tensors\n",
        "seq_tensors = np.stack([one_hot_encode_seq(s) for s in df['sequence'].values])\n",
        "context_features = df[[\"debris\",\"phag_activity\",\"inflammation\",\"replication_stress\",\n",
        "                       \"stem_frac\",\"nutrients\",\"oxygen\",\"time_of_day\",\"purity\",\"entropy\",\n",
        "                       \"gscore\",\"palin\"]].values.astype(np.float32)\n",
        "targets = df[[\"StabilityScore\",\"TelomeraseAffinityScore\",\"FragilityRiskScore\",\n",
        "              \"SecondaryStructureScore\",\"ImmuneTriggerScore\"]].values.astype(np.float32)\n",
        "\n",
        "# Simple Dataset\n",
        "class TelomereDataset(Dataset):\n",
        "    def __init__(self, seq_tensor, context, targets):\n",
        "        self.seq = torch.tensor(seq_tensor)  # [N, L, 4]\n",
        "        self.context = torch.tensor(context)\n",
        "        self.targets = torch.tensor(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seq.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.seq[idx], self.context[idx], self.targets[idx]\n",
        "\n",
        "dataset = TelomereDataset(seq_tensors, context_features, targets)\n",
        "\n",
        "# Train/test split\n",
        "n_train = int(len(dataset)*0.8)\n",
        "n_val = len(dataset) - n_train\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------\n",
        "# Model definition (multi-task + multi-head)\n",
        "# -------------------------\n",
        "class MultiTaskTelomereModel(nn.Module):\n",
        "    def __init__(self, seq_len=MAX_SEQ_LEN, d_model=96, num_heads=6, depth=2, num_context=12):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # CNN\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(4, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, d_model, kernel_size=5, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Transformer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=4*d_model, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "        # Context MLP\n",
        "        self.context_mlp = nn.Sequential(\n",
        "            nn.Linear(num_context, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Shared fusion\n",
        "        self.shared_fusion = nn.Sequential(\n",
        "            nn.Linear(2*d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        # Task heads\n",
        "        def task_head():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(d_model, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        self.stability_head = task_head()\n",
        "        self.affinity_head = task_head()\n",
        "        self.fragility_head = task_head()\n",
        "        self.structure_head = task_head()\n",
        "        self.immune_head = task_head()\n",
        "\n",
        "    def forward(self, seq_onehot, context_vec):\n",
        "        # seq_onehot: [B, L, 4]\n",
        "        x = seq_onehot.permute(0,2,1)  # [B, 4, L]\n",
        "        x = self.cnn(x)               # [B, d_model, L]\n",
        "        x = x.permute(0,2,1)          # [B, L, d_model]\n",
        "        x = self.transformer(x)       # [B, L, d_model]\n",
        "        seq_embed = x.mean(dim=1)     # [B, d_model]\n",
        "\n",
        "        cont = self.context_mlp(context_vec)  # [B, d_model]\n",
        "        fused = torch.cat([seq_embed, cont], dim=1)  # [B, 2*d_model]\n",
        "        fused = self.shared_fusion(fused)            # [B, d_model]\n",
        "\n",
        "        out1 = self.stability_head(fused).squeeze(-1)\n",
        "        out2 = self.affinity_head(fused).squeeze(-1)\n",
        "        out3 = self.fragility_head(fused).squeeze(-1)\n",
        "        out4 = self.structure_head(fused).squeeze(-1)\n",
        "        out5 = self.immune_head(fused).squeeze(-1)\n",
        "        out = torch.stack([out1, out2, out3, out4, out5], dim=1)\n",
        "        return out\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiTaskTelomereModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# Weighted MSE on the 5 tasks (we may prioritize fragility/immune slightly)\n",
        "task_weights = torch.tensor([1.0, 1.0, 1.2, 0.8, 1.2], device=device)\n",
        "\n",
        "def compute_loss(preds, targets):\n",
        "    mse = (preds - targets).pow(2)\n",
        "    weighted = mse * task_weights\n",
        "    return weighted.mean()\n",
        "\n",
        "EPOCHS = 12\n",
        "train_history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for seq_batch, ctx_batch, tgt_batch in train_loader:\n",
        "        seq_batch = seq_batch.to(device)\n",
        "        ctx_batch = ctx_batch.to(device)\n",
        "        tgt_batch = tgt_batch.to(device)\n",
        "        preds = model(seq_batch, ctx_batch)\n",
        "        loss = compute_loss(preds, tgt_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * seq_batch.size(0)\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    model.eval()\n",
        "    val_running = 0.0\n",
        "    with torch.no_grad():\n",
        "        for seq_batch, ctx_batch, tgt_batch in val_loader:\n",
        "            seq_batch = seq_batch.to(device)\n",
        "            ctx_batch = ctx_batch.to(device)\n",
        "            tgt_batch = tgt_batch.to(device)\n",
        "            preds = model(seq_batch, ctx_batch)\n",
        "            loss = compute_loss(preds, tgt_batch)\n",
        "            val_running += loss.item() * seq_batch.size(0)\n",
        "    val_loss = val_running / len(val_loader.dataset)\n",
        "    train_history[\"train_loss\"].append(train_loss)\n",
        "    train_history[\"val_loss\"].append(val_loss)\n",
        "    print(f\"Epoch {epoch:02d}  Train Loss: {train_loss:.5f}  Val Loss: {val_loss:.5f}\")\n",
        "\n",
        "# Save model and training history\n",
        "model_path = OUTDIR / \"telomere_multitask_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "np.save(OUTDIR / \"training_history.npy\", np.array([train_history[\"train_loss\"], train_history[\"val_loss\"]]))\n",
        "\n",
        "# Quick evaluation: compute MAE per task on validation set\n",
        "model.eval()\n",
        "mae = np.zeros(5)\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for seq_batch, ctx_batch, tgt_batch in val_loader:\n",
        "        seq_batch = seq_batch.to(device)\n",
        "        ctx_batch = ctx_batch.to(device)\n",
        "        tgt_batch = tgt_batch.to(device)\n",
        "        preds = model(seq_batch, ctx_batch).cpu().numpy()\n",
        "        mae += np.abs(preds - tgt_batch.numpy()).sum(axis=0)\n",
        "        count += preds.shape[0]\n",
        "mae = mae / count\n",
        "\n",
        "metrics = {\n",
        "    \"MAE_Stability\": float(mae[0]),\n",
        "    \"MAE_Affinity\": float(mae[1]),\n",
        "    \"MAE_Fragility\": float(mae[2]),\n",
        "    \"MAE_SecondaryStructure\": float(mae[3]),\n",
        "    \"MAE_Immune\": float(mae[4])\n",
        "}\n",
        "print(\"Validation MAE per task:\", metrics)\n",
        "\n",
        "# Save a small sample of the dataset for display\n",
        "sample_df = df.sample(n=12, random_state=SEED).reset_index(drop=True)\n",
        "sample_path = OUTDIR / \"synthetic_dataset_sample.csv\"\n",
        "sample_df.to_csv(sample_path, index=False)\n",
        "\n",
        "# Display sample dataframe if UI tool available\n",
        "if display_dataframe_to_user is not None:\n",
        "    display_dataframe_to_user(\"Synthetic Telomere Dataset Sample\", sample_df)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_history[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(train_history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.tight_layout()\n",
        "plot_path = OUTDIR / \"training_loss.png\"\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "\n",
        "# Save full dataset\n",
        "# (already saved as synthetic_telomere_dataset.csv)\n",
        "print(\"\\nSaved files:\")\n",
        "print(\" - Dataset CSV:\", str(csv_path))\n",
        "print(\" - Model:\", str(model_path))\n",
        "print(\" - Training history:\", str(OUTDIR / 'training_history.npy'))\n",
        "print(\" - Training loss plot:\", str(plot_path))\n",
        "\n",
        "# Provide download links (UI will show files under /mnt/data)\n",
        "print(\"\\nDownload links (use in the chat UI):\")\n",
        "print(f\"[Download synthetic dataset] (sandbox:{csv_path})\")\n",
        "print(f\"[Download trained model] (sandbox:{model_path})\")\n",
        "print(f\"[Download training loss plot] (sandbox:{plot_path})\")\n",
        "\n",
        "# End of pipeline code."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_IWjcSd8UbC",
        "outputId": "ead9e112-ee53-4c4c-caf5-a7ce42801d3d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01  Train Loss: 0.00341  Val Loss: 0.00123\n",
            "Epoch 02  Train Loss: 0.00127  Val Loss: 0.00110\n",
            "Epoch 03  Train Loss: 0.00105  Val Loss: 0.00093\n",
            "Epoch 04  Train Loss: 0.00096  Val Loss: 0.00089\n",
            "Epoch 05  Train Loss: 0.00091  Val Loss: 0.00090\n",
            "Epoch 06  Train Loss: 0.00090  Val Loss: 0.00087\n",
            "Epoch 07  Train Loss: 0.00089  Val Loss: 0.00086\n",
            "Epoch 08  Train Loss: 0.00088  Val Loss: 0.00084\n",
            "Epoch 09  Train Loss: 0.00087  Val Loss: 0.00088\n",
            "Epoch 10  Train Loss: 0.00086  Val Loss: 0.00092\n",
            "Epoch 11  Train Loss: 0.00085  Val Loss: 0.00086\n",
            "Epoch 12  Train Loss: 0.00085  Val Loss: 0.00087\n",
            "Validation MAE per task: {'MAE_Stability': 0.01740162990987301, 'MAE_Affinity': 0.02459146432578564, 'MAE_Fragility': 0.02554728798568249, 'MAE_SecondaryStructure': 0.01979451548308134, 'MAE_Immune': 0.02517802432179451}\n",
            "\n",
            "Saved files:\n",
            " - Dataset CSV: /mnt/data/synthetic_telomere_dataset.csv\n",
            " - Model: /mnt/data/telomere_multitask_model.pth\n",
            " - Training history: /mnt/data/training_history.npy\n",
            " - Training loss plot: /mnt/data/training_loss.png\n",
            "\n",
            "Download links (use in the chat UI):\n",
            "[Download synthetic dataset] (sandbox:/mnt/data/synthetic_telomere_dataset.csv)\n",
            "[Download trained model] (sandbox:/mnt/data/telomere_multitask_model.pth)\n",
            "[Download training loss plot] (sandbox:/mnt/data/training_loss.png)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a full training pipeline + synthetic dataset generator for the\n",
        "# multi-task, multi-head telomere predictor (simulation-only, safe).\n",
        "#\n",
        "# This notebook:\n",
        "# 1) Generates a synthetic dataset of telomere-like sequences + context features\n",
        "# 2) Builds a multi-task model (CNN + Transformer + task heads)\n",
        "# 3) Trains the model on synthetic targets (stability, affinity, fragility, structure, immune)\n",
        "# 4) Evaluates and saves model + dataset\n",
        "# 5) Displays sample data and training curves\n",
        "#\n",
        "# Outputs saved to /mnt/data:\n",
        "# - /mnt/data/synthetic_telomere_dataset.csv\n",
        "# - /mnt/data/telomere_multitask_model.pth\n",
        "# - /mnt/data/training_history.npy\n",
        "#\n",
        "# NOTE: This code is purely computational and does NOT produce any wet-lab or\n",
        "# actionable biological instructions. It's safe to run and intended for in-silico simulation.\n",
        "\n",
        "# Execute the training pipeline\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For interactive dataframe display in the notebook UI\n",
        "try:\n",
        "    from caas_jupyter_tools import display_dataframe_to_user\n",
        "except Exception:\n",
        "    display_dataframe_to_user = None\n",
        "\n",
        "# reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "OUTDIR = Path(\"/mnt/data\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Synthetic dataset generator\n",
        "# -------------------------\n",
        "BASE_REPEAT = \"TTAGGG\"  # reference repeat (human-like) used only as inspiration\n",
        "NUCLEOTIDES = ['A', 'T', 'G', 'C']\n",
        "\n",
        "def mutate_repeat_unit(base, mutation_rate=0.1):\n",
        "    \"\"\"Mutate a base repeat unit by randomly substituting bases with probability mutation_rate.\"\"\"\n",
        "    out = []\n",
        "    for ch in base:\n",
        "        if random.random() < mutation_rate:\n",
        "            out.append(random.choice(NUCLEOTIDES))\n",
        "        else:\n",
        "            out.append(ch)\n",
        "    return ''.join(out)\n",
        "\n",
        "def build_sequence(repeat_unit, repeats):\n",
        "    return repeat_unit * repeats\n",
        "\n",
        "def sequence_entropy(seq):\n",
        "    counts = {n: seq.count(n) for n in NUCLEOTIDES}\n",
        "    probs = np.array(list(counts.values())) / len(seq)\n",
        "    probs = probs[probs > 0]\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def g_rich_runs_score(seq):\n",
        "    # proxy for G-quadruplex-like propensity: count occurrences of \"GGG\" and longer runs\n",
        "    score = 0\n",
        "    run = 0\n",
        "    for ch in seq:\n",
        "        if ch == 'G':\n",
        "            run += 1\n",
        "        else:\n",
        "            if run >= 3:\n",
        "                score += run\n",
        "            run = 0\n",
        "    if run >= 3:\n",
        "        score += run\n",
        "    return score / max(1, len(seq)/6)  # normalize by length/6\n",
        "\n",
        "def palindromic_score(seq, k=6):\n",
        "    # count small palindromic windows as proxy for hairpins\n",
        "    score = 0\n",
        "    for i in range(len(seq)-k+1):\n",
        "        window = seq[i:i+k]\n",
        "        # simple reverse complement check\n",
        "        rc = window[::-1].translate(str.maketrans(\"ATGC\",\"TACG\"))\n",
        "        if rc == window:\n",
        "            score += 1\n",
        "    return score / max(1, len(seq)/k)\n",
        "\n",
        "def motif_purity_score(seq, repeat_unit):\n",
        "    # fraction of sequence that matches the repeated repeat_unit perfectly when tiled\n",
        "    L = len(repeat_unit)\n",
        "    perfect = 0\n",
        "    for i in range(0, len(seq), L):\n",
        "        block = seq[i:i+L]\n",
        "        if block == repeat_unit:\n",
        "            perfect += 1\n",
        "    return perfect * L / len(seq)\n",
        "\n",
        "def synth_targets_from_sequence(seq, repeat_unit, context_vec):\n",
        "    \"\"\"\n",
        "    Heuristic synthetic target generator. Produces five values in [0,1]:\n",
        "    StabilityScore, TelomeraseAffinityScore, FragilityRiskScore,\n",
        "    SecondaryStructureScore, ImmuneTriggerScore\n",
        "    \"\"\"\n",
        "    length = len(seq)\n",
        "    repeats = length / max(1, len(repeat_unit))\n",
        "    purity = motif_purity_score(seq, repeat_unit)  # 0..1\n",
        "    entropy = sequence_entropy(seq) / 2.0  # normalize roughly\n",
        "    gscore = g_rich_runs_score(seq)  # normalized proxy\n",
        "    palin = palindromic_score(seq)\n",
        "    variability = 1.0 - purity\n",
        "\n",
        "    # Context modifiers (safe, synthetic)\n",
        "    debris, phag_activity, inflammation, replication_stress = context_vec[:4]\n",
        "\n",
        "    # Stability: higher with purity, length; lowered by gscore/palin/replication stress\n",
        "    stability_raw = 0.5 * purity + 0.4 * (math.tanh((repeats-8)/8)+1)/2 - 0.3 * gscore - 0.2 * palin - 0.2 * replication_stress\n",
        "    stability = float(1/(1+math.exp(-stability_raw)))  # sigmoid-ish to 0..1\n",
        "\n",
        "    # Telomerase affinity: higher for periodicity & moderate G-richness; penalize high entropy\n",
        "    affinity_raw = 0.6 * purity + 0.2 * (math.tanh(gscore)+1)/2 - 0.3 * entropy + 0.2 * (1-replication_stress)\n",
        "    affinity = float(1/(1+math.exp(-affinity_raw)))\n",
        "\n",
        "    # Fragility risk: increased by variability, palindromes, and high replication stress\n",
        "    frag_raw = 0.5 * variability + 0.4 * palin + 0.3 * replication_stress + 0.2 * entropy\n",
        "    frag = float(1/(1+math.exp(- (frag_raw - 0.5) )))\n",
        "\n",
        "    # Secondary structure score: proxy from gscore + palin\n",
        "    sec_raw = 0.6 * gscore + 0.4 * palin\n",
        "    sec = float(1/(1+math.exp(- (sec_raw - 0.3) )))\n",
        "\n",
        "    # Immune trigger: higher entropy and exposed non-native motifs, increased by inflammation\n",
        "    imm_raw = 0.5 * entropy + 0.3 * variability + 0.4 * inflammation\n",
        "    imm = float(1/(1+math.exp(- (imm_raw - 0.2) )))\n",
        "\n",
        "    # Add small noise\n",
        "    stability = min(max(stability + np.random.normal(0, 0.02), 0.0), 1.0)\n",
        "    affinity = min(max(affinity + np.random.normal(0, 0.03), 0.0), 1.0)\n",
        "    frag = min(max(frag + np.random.normal(0, 0.03), 0.0), 1.0)\n",
        "    sec = min(max(sec + np.random.normal(0, 0.02), 0.0), 1.0)\n",
        "    imm = min(max(imm + np.random.normal(0, 0.03), 0.0), 1.0)\n",
        "\n",
        "    return stability, affinity, frag, sec, imm\n",
        "\n",
        "# Generate dataset parameters\n",
        "N_SAMPLES = 4000\n",
        "MIN_REPEATS = 5\n",
        "MAX_REPEATS = 60\n",
        "MAX_SEQ_LEN = MAX_REPEATS * len(BASE_REPEAT)  # for padding\n",
        "\n",
        "records = []\n",
        "sequences = []\n",
        "\n",
        "for i in range(N_SAMPLES):\n",
        "    # choose a repeat unit by mutating base with variable mutation rate\n",
        "    mut_rate = random.choice([0.0, 0.05, 0.1, 0.2])\n",
        "    ru = mutate_repeat_unit(BASE_REPEAT, mutation_rate=mut_rate)\n",
        "    repeats = random.randint(MIN_REPEATS, MAX_REPEATS)\n",
        "    seq = build_sequence(ru, repeats)\n",
        "\n",
        "    # randomly insert occasional random short motifs to increase diversity\n",
        "    if random.random() < 0.15:\n",
        "        pos = random.randint(0, max(0, len(seq)-10))\n",
        "        insert = ''.join(random.choices(NUCLEOTIDES, k=6))\n",
        "        seq = seq[:pos] + insert + seq[pos:]\n",
        "\n",
        "    # context features (synthetic)\n",
        "    debris = random.random()  # 0-1\n",
        "    phag_activity = random.random()\n",
        "    inflammation = random.random()\n",
        "    replication_stress = random.random()\n",
        "    # additional context features (stem cell fraction, local nutrients, oxygen proxy, time)\n",
        "    stem_frac = random.random()\n",
        "    nutrients = random.random()\n",
        "    oxygen = random.random()\n",
        "    time_of_day = random.random()\n",
        "\n",
        "    context_vec = [debris, phag_activity, inflammation, replication_stress,\n",
        "                   stem_frac, nutrients, oxygen, time_of_day, random.random(), random.random(), 0.0, 0.0][:12]\n",
        "\n",
        "    stability, affinity, frag, sec, imm = synth_targets_from_sequence(seq, ru, context_vec)\n",
        "\n",
        "    records.append({\n",
        "        \"sequence\": seq,\n",
        "        \"repeat_unit\": ru,\n",
        "        \"repeats\": repeats,\n",
        "        \"length\": len(seq),\n",
        "        \"purity\": motif_purity_score(seq, ru),\n",
        "        \"entropy\": sequence_entropy(seq),\n",
        "        \"gscore\": g_rich_runs_score(seq),\n",
        "        \"palin\": palindromic_score(seq),\n",
        "        \"debris\": debris,\n",
        "        \"phag_activity\": phag_activity,\n",
        "        \"inflammation\": inflammation,\n",
        "        \"replication_stress\": replication_stress,\n",
        "        \"stem_frac\": stem_frac,\n",
        "        \"nutrients\": nutrients,\n",
        "        \"oxygen\": oxygen,\n",
        "        \"time_of_day\": time_of_day,\n",
        "        \"StabilityScore\": stability,\n",
        "        \"TelomeraseAffinityScore\": affinity,\n",
        "        \"FragilityRiskScore\": frag,\n",
        "        \"SecondaryStructureScore\": sec,\n",
        "        \"ImmuneTriggerScore\": imm\n",
        "    })\n",
        "    sequences.append(seq)\n",
        "\n",
        "df = pd.DataFrame.from_records(records)\n",
        "# Save CSV for external inspection\n",
        "csv_path = OUTDIR / \"synthetic_telomere_dataset.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset and dataloader\n",
        "# -------------------------\n",
        "# One-hot encode sequences and pad to MAX_SEQ_LEN\n",
        "def one_hot_encode_seq(seq, max_len=MAX_SEQ_LEN):\n",
        "    mapping = {'A':0,'T':1,'G':2,'C':3}\n",
        "    arr = np.zeros((max_len, 4), dtype=np.float32)\n",
        "    for i, ch in enumerate(seq[:max_len]):\n",
        "        if ch in mapping:\n",
        "            arr[i, mapping[ch]] = 1.0\n",
        "    return arr\n",
        "\n",
        "# Build tensors\n",
        "seq_tensors = np.stack([one_hot_encode_seq(s) for s in df['sequence'].values])\n",
        "context_features = df[[\"debris\",\"phag_activity\",\"inflammation\",\"replication_stress\",\n",
        "                       \"stem_frac\",\"nutrients\",\"oxygen\",\"time_of_day\",\"purity\",\"entropy\",\n",
        "                       \"gscore\",\"palin\"]].values.astype(np.float32)\n",
        "targets = df[[\"StabilityScore\",\"TelomeraseAffinityScore\",\"FragilityRiskScore\",\n",
        "              \"SecondaryStructureScore\",\"ImmuneTriggerScore\"]].values.astype(np.float32)\n",
        "\n",
        "# Simple Dataset\n",
        "class TelomereDataset(Dataset):\n",
        "    def __init__(self, seq_tensor, context, targets):\n",
        "        self.seq = torch.tensor(seq_tensor)  # [N, L, 4]\n",
        "        self.context = torch.tensor(context)\n",
        "        self.targets = torch.tensor(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seq.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.seq[idx], self.context[idx], self.targets[idx]\n",
        "\n",
        "dataset = TelomereDataset(seq_tensors, context_features, targets)\n",
        "\n",
        "# Train/test split\n",
        "n_train = int(len(dataset)*0.8)\n",
        "n_val = len(dataset) - n_train\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------\n",
        "# Model definition (multi-task + multi-head)\n",
        "# -------------------------\n",
        "class MultiTaskTelomereModel(nn.Module):\n",
        "    def __init__(self, seq_len=MAX_SEQ_LEN, d_model=96, num_heads=6, depth=2, num_context=12):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # CNN\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(4, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, d_model, kernel_size=5, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Transformer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=4*d_model, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "        # Context MLP\n",
        "        self.context_mlp = nn.Sequential(\n",
        "            nn.Linear(num_context, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Shared fusion\n",
        "        self.shared_fusion = nn.Sequential(\n",
        "            nn.Linear(2*d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        # Task heads\n",
        "        def task_head():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(d_model, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        self.stability_head = task_head()\n",
        "        self.affinity_head = task_head()\n",
        "        self.fragility_head = task_head()\n",
        "        self.structure_head = task_head()\n",
        "        self.immune_head = task_head()\n",
        "\n",
        "    def forward(self, seq_onehot, context_vec):\n",
        "        # seq_onehot: [B, L, 4]\n",
        "        x = seq_onehot.permute(0,2,1)  # [B, 4, L]\n",
        "        x = self.cnn(x)               # [B, d_model, L]\n",
        "        x = x.permute(0,2,1)          # [B, L, d_model]\n",
        "        x = self.transformer(x)       # [B, L, d_model]\n",
        "        seq_embed = x.mean(dim=1)     # [B, d_model]\n",
        "\n",
        "        cont = self.context_mlp(context_vec)  # [B, d_model]\n",
        "        fused = torch.cat([seq_embed, cont], dim=1)  # [B, 2*d_model]\n",
        "        fused = self.shared_fusion(fused)            # [B, d_model]\n",
        "\n",
        "        out1 = self.stability_head(fused).squeeze(-1)\n",
        "        out2 = self.affinity_head(fused).squeeze(-1)\n",
        "        out3 = self.fragility_head(fused).squeeze(-1)\n",
        "        out4 = self.structure_head(fused).squeeze(-1)\n",
        "        out5 = self.immune_head(fused).squeeze(-1)\n",
        "        out = torch.stack([out1, out2, out3, out4, out5], dim=1)\n",
        "        return out\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiTaskTelomereModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# Weighted MSE on the 5 tasks (we may prioritize fragility/immune slightly)\n",
        "task_weights = torch.tensor([1.0, 1.0, 1.2, 0.8, 1.2], device=device)\n",
        "\n",
        "def compute_loss(preds, targets):\n",
        "    mse = (preds - targets).pow(2)\n",
        "    weighted = mse * task_weights\n",
        "    return weighted.mean()\n",
        "\n",
        "EPOCHS = 12\n",
        "train_history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for seq_batch, ctx_batch, tgt_batch in train_loader:\n",
        "        seq_batch = seq_batch.to(device)\n",
        "        ctx_batch = ctx_batch.to(device)\n",
        "        tgt_batch = tgt_batch.to(device)\n",
        "        preds = model(seq_batch, ctx_batch)\n",
        "        loss = compute_loss(preds, tgt_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * seq_batch.size(0)\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    model.eval()\n",
        "    val_running = 0.0\n",
        "    with torch.no_grad():\n",
        "        for seq_batch, ctx_batch, tgt_batch in val_loader:\n",
        "            seq_batch = seq_batch.to(device)\n",
        "            ctx_batch = ctx_batch.to(device)\n",
        "            tgt_batch = tgt_batch.to(device)\n",
        "            preds = model(seq_batch, ctx_batch)\n",
        "            loss = compute_loss(preds, tgt_batch)\n",
        "            val_running += loss.item() * seq_batch.size(0)\n",
        "    val_loss = val_running / len(val_loader.dataset)\n",
        "    train_history[\"train_loss\"].append(train_loss)\n",
        "    train_history[\"val_loss\"].append(val_loss)\n",
        "    print(f\"Epoch {epoch:02d}  Train Loss: {train_loss:.5f}  Val Loss: {val_loss:.5f}\")\n",
        "\n",
        "# Save model and training history\n",
        "model_path = OUTDIR / \"telomere_multitask_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "np.save(OUTDIR / \"training_history.npy\", np.array([train_history[\"train_loss\"], train_history[\"val_loss\"]]))\n",
        "\n",
        "# Quick evaluation: compute MAE per task on validation set\n",
        "model.eval()\n",
        "mae = np.zeros(5)\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for seq_batch, ctx_batch, tgt_batch in val_loader:\n",
        "        seq_batch = seq_batch.to(device)\n",
        "        ctx_batch = ctx_batch.to(device)\n",
        "        tgt_batch = tgt_batch.to(device)\n",
        "        preds = model(seq_batch, ctx_batch).cpu().numpy()\n",
        "        mae += np.abs(preds - tgt_batch.numpy()).sum(axis=0)\n",
        "        count += preds.shape[0]\n",
        "mae = mae / count\n",
        "\n",
        "metrics = {\n",
        "    \"MAE_Stability\": float(mae[0]),\n",
        "    \"MAE_Affinity\": float(mae[1]),\n",
        "    \"MAE_Fragility\": float(mae[2]),\n",
        "    \"MAE_SecondaryStructure\": float(mae[3]),\n",
        "    \"MAE_Immune\": float(mae[4]),\n",
        "}\n",
        "print(\"Validation MAE per task:\", metrics)\n",
        "\n",
        "# Save a small sample of the dataset for display\n",
        "sample_df = df.sample(n=12, random_state=SEED).reset_index(drop=True)\n",
        "sample_path = OUTDIR / \"synthetic_dataset_sample.csv\"\n",
        "sample_df.to_csv(sample_path, index=False)\n",
        "\n",
        "# Display sample dataframe if UI tool available\n",
        "if display_dataframe_to_user is not None:\n",
        "    display_dataframe_to_user(\"Synthetic Telomere Dataset Sample\", sample_df)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_history[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(train_history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.tight_layout()\n",
        "plot_path = OUTDIR / \"training_loss.png\"\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "\n",
        "# Save full dataset\n",
        "# (already saved as synthetic_telomere_dataset.csv)\n",
        "print(\"\\nSaved files:\")\n",
        "print(\" - Dataset CSV:\", str(csv_path))\n",
        "print(\" - Model:\", str(model_path))\n",
        "print(\" - Training history:\", str(OUTDIR / 'training_history.npy'))\n",
        "print(\" - Training loss plot:\", str(plot_path))\n",
        "\n",
        "# Provide download links (UI will show files under /mnt/data)\n",
        "print(\"\\nDownload links (use in the chat UI):\")\n",
        "print(f\"[Download synthetic dataset] (sandbox:{csv_path})\")\n",
        "print(f\"[Download trained model] (sandbox:{model_path})\")\n",
        "print(f\"[Download training loss plot] (sandbox:{plot_path})\")\n",
        "\n",
        "# End of pipeline code.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgHQS1GK9Rca",
        "outputId": "b9532d9e-2989-4e84-a889-0ddb5e122800"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01  Train Loss: 0.00341  Val Loss: 0.00123\n",
            "Epoch 02  Train Loss: 0.00127  Val Loss: 0.00110\n",
            "Epoch 03  Train Loss: 0.00105  Val Loss: 0.00093\n",
            "Epoch 04  Train Loss: 0.00096  Val Loss: 0.00089\n",
            "Epoch 05  Train Loss: 0.00091  Val Loss: 0.00090\n",
            "Epoch 06  Train Loss: 0.00090  Val Loss: 0.00087\n",
            "Epoch 07  Train Loss: 0.00089  Val Loss: 0.00086\n",
            "Epoch 08  Train Loss: 0.00088  Val Loss: 0.00084\n",
            "Epoch 09  Train Loss: 0.00087  Val Loss: 0.00088\n",
            "Epoch 10  Train Loss: 0.00086  Val Loss: 0.00092\n",
            "Epoch 11  Train Loss: 0.00085  Val Loss: 0.00086\n",
            "Epoch 12  Train Loss: 0.00085  Val Loss: 0.00087\n",
            "Validation MAE per task: {'MAE_Stability': 0.01740162990987301, 'MAE_Affinity': 0.02459146432578564, 'MAE_Fragility': 0.02554728798568249, 'MAE_SecondaryStructure': 0.01979451548308134, 'MAE_Immune': 0.02517802432179451}\n",
            "\n",
            "Saved files:\n",
            " - Dataset CSV: /mnt/data/synthetic_telomere_dataset.csv\n",
            " - Model: /mnt/data/telomere_multitask_model.pth\n",
            " - Training history: /mnt/data/training_history.npy\n",
            " - Training loss plot: /mnt/data/training_loss.png\n",
            "\n",
            "Download links (use in the chat UI):\n",
            "[Download synthetic dataset] (sandbox:/mnt/data/synthetic_telomere_dataset.csv)\n",
            "[Download trained model] (sandbox:/mnt/data/telomere_multitask_model.pth)\n",
            "[Download training loss plot] (sandbox:/mnt/data/training_loss.png)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OycQO91g9U5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}