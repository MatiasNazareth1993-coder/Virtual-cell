{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnIY7XWSzdpe1PkYFNTszu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasNazareth1993-coder/Virtual-cell/blob/main/Virtual_cell_simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6injDC02hV3y",
        "outputId": "8ab02209-52d4-420d-cdae-a2fb3da7f54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=1.4372 | val_acc=0.4150 | tel_auc=0.4693 | lr=0.00003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | train_loss=1.1201 | val_acc=0.8900 | tel_auc=0.5489 | lr=0.00005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | train_loss=0.7321 | val_acc=0.8900 | tel_auc=0.7092 | lr=0.00007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | train_loss=0.6517 | val_acc=0.8900 | tel_auc=0.7534 | lr=0.00010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | train_loss=0.5254 | val_acc=0.9000 | tel_auc=0.8882 | lr=0.00013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | train_loss=0.3668 | val_acc=0.9175 | tel_auc=0.9464 | lr=0.00015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | train_loss=0.2681 | val_acc=0.9137 | tel_auc=0.9624 | lr=0.00018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | train_loss=0.2052 | val_acc=0.9250 | tel_auc=0.9678 | lr=0.00020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | train_loss=0.1518 | val_acc=0.9269 | tel_auc=0.9691 | lr=0.00022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | train_loss=0.1018 | val_acc=0.9275 | tel_auc=0.9675 | lr=0.00025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | train_loss=0.0648 | val_acc=0.9306 | tel_auc=0.9675 | lr=0.00028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | train_loss=0.0413 | val_acc=0.9263 | tel_auc=0.9695 | lr=0.00030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | train_loss=0.0249 | val_acc=0.9263 | tel_auc=0.9696 | lr=0.00033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | train_loss=0.0174 | val_acc=0.9231 | tel_auc=0.9687 | lr=0.00035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | train_loss=0.0146 | val_acc=0.9244 | tel_auc=0.9715 | lr=0.00038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | train_loss=0.0130 | val_acc=0.9287 | tel_auc=0.9679 | lr=0.00040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | train_loss=0.0177 | val_acc=0.9200 | tel_auc=0.9709 | lr=0.00043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | train_loss=0.0270 | val_acc=0.9269 | tel_auc=0.9731 | lr=0.00045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | train_loss=0.0207 | val_acc=0.9225 | tel_auc=0.9722 | lr=0.00047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | train_loss=0.0185 | val_acc=0.9169 | tel_auc=0.9740 | lr=0.00050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 | train_loss=0.0209 | val_acc=0.9269 | tel_auc=0.9719 | lr=0.00052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 | train_loss=0.0242 | val_acc=0.9294 | tel_auc=0.9688 | lr=0.00055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 | train_loss=0.0193 | val_acc=0.9287 | tel_auc=0.9724 | lr=0.00057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 | train_loss=0.0198 | val_acc=0.9300 | tel_auc=0.9782 | lr=0.00060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 | train_loss=0.0123 | val_acc=0.9319 | tel_auc=0.9740 | lr=0.00063\n",
            "Training finished. Best telomerase AUC: 0.97820967489316\n"
          ]
        }
      ],
      "source": [
        "# virtual_cell_model.py\n",
        "# Requirements: torch, sklearn (for metrics), numpy\n",
        "# pip install torch sklearn numpy\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -------------------------\n",
        "# Synthetic dataset helper\n",
        "# -------------------------\n",
        "class SyntheticCellDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Creates synthetic multimodal samples for demonstration.\n",
        "    Each sample:\n",
        "      - genomic vector (e.g., summary features) -> dim_g\n",
        "      - proteomic vector -> dim_p\n",
        "      - metabolomic vector -> dim_m\n",
        "      - env signals vector (ROS, cytokines, nutrients) -> dim_e\n",
        "    Targets:\n",
        "      - fate (0=healthy_division,1=senescence,2=apoptosis)\n",
        "      - telomerase_action (0 or 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples=5000, dims=(64,32,32,8), seed=42):\n",
        "        super().__init__()\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.n = n_samples\n",
        "        self.dim_g, self.dim_p, self.dim_m, self.dim_e = dims\n",
        "        self._make()\n",
        "\n",
        "    def _make(self):\n",
        "        self.genomic = self.rng.normal(size=(self.n, self.dim_g)).astype(np.float32)\n",
        "        self.proteomic = self.rng.normal(size=(self.n, self.dim_p)).astype(np.float32)\n",
        "        self.metabolomic = self.rng.normal(size=(self.n, self.dim_m)).astype(np.float32)\n",
        "        self.env = self.rng.normal(size=(self.n, self.dim_e)).astype(np.float32)\n",
        "\n",
        "        # Synthetic rule: shorter \"telomere proxy\" in genomic increases chance of senescence,\n",
        "        # high ROS in env increases senescence, but controlled telomerase_action can reduce senescence.\n",
        "        # We'll craft a continuous score and discretize.\n",
        "        telomere_proxy = self.genomic[:, :1].squeeze()  # simple scalar proxy\n",
        "        ros_signal = self.env[:, 0]  # assume env[:,0] is ROS\n",
        "        noise = 0.1 * self.rng.normal(size=self.n)\n",
        "\n",
        "        # telomerase_action target: 1 when telomere_proxy < threshold and ROS low enough (simulated)\n",
        "        telomerase_prob = (telomere_proxy < -0.2) & (ros_signal < 0.5)\n",
        "        telomerase_action = telomerase_prob.astype(np.int64)\n",
        "\n",
        "        # fate: 0 healthy, 1 senescence, 2 apoptosis\n",
        "        # If telomere very short and no telomerase -> senescence; if env huge ROS -> apoptosis\n",
        "        fate = np.zeros(self.n, dtype=np.int64)\n",
        "        fate[(telomere_proxy < -0.6) & (telomerase_action == 0)] = 1\n",
        "        fate[(ros_signal > 2.0)] = 2\n",
        "        # randomize a bit\n",
        "        flip = self.rng.rand(self.n)\n",
        "        fate[flip < 0.02] = self.rng.randint(0,3,size=(flip < 0.02).sum())\n",
        "\n",
        "        self.targets_fate = fate\n",
        "        self.targets_tel = telomerase_action\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'genomic': self.genomic[idx],\n",
        "            'proteomic': self.proteomic[idx],\n",
        "            'metabolomic': self.metabolomic[idx],\n",
        "            'env': self.env[idx],\n",
        "            'fate': self.targets_fate[idx],\n",
        "            'tel': self.targets_tel[idx]\n",
        "        }\n",
        "\n",
        "# -------------------------\n",
        "# Model definition\n",
        "# -------------------------\n",
        "class ModalityEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid_dim),\n",
        "            nn.LayerNorm(hid_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hid_dim, hid_dim),\n",
        "            nn.LayerNorm(hid_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class VirtualCellModel(nn.Module):\n",
        "    def __init__(self, dims=(64,32,32,8), hidden=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        dim_g, dim_p, dim_m, dim_e = dims\n",
        "        enc_h = hidden // 2\n",
        "        self.enc_g = ModalityEncoder(dim_g, enc_h)\n",
        "        self.enc_p = ModalityEncoder(dim_p, enc_h)\n",
        "        self.enc_m = ModalityEncoder(dim_m, enc_h)\n",
        "        self.enc_e = ModalityEncoder(dim_e, enc_h)\n",
        "\n",
        "        fused_dim = enc_h * 4\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(fused_dim, hidden),\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # outputs\n",
        "        self.fate_head = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden//2, 3)  # 3-way classification\n",
        "        )\n",
        "        self.telomerase_head = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden//2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, genomic, proteomic, metabolomic, env):\n",
        "        g = self.enc_g(genomic)\n",
        "        p = self.enc_p(proteomic)\n",
        "        m = self.enc_m(metabolomic)\n",
        "        e = self.enc_e(env)\n",
        "        fused = torch.cat([g,p,m,e], dim=-1)\n",
        "        h = self.fusion(fused)\n",
        "        fate_logits = self.fate_head(h)\n",
        "        tel = self.telomerase_head(h).squeeze(-1)\n",
        "        return fate_logits, tel\n",
        "\n",
        "# -------------------------\n",
        "# Training utilities\n",
        "# -------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device, scaler=None, clip_grad=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        genomic = batch['genomic'].to(device)\n",
        "        proteomic = batch['proteomic'].to(device)\n",
        "        metabolomic = batch['metabolomic'].to(device)\n",
        "        env = batch['env'].to(device)\n",
        "        fate = batch['fate'].to(device)\n",
        "        tel = batch['tel'].to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        fate_logits, tel_pred = model(genomic, proteomic, metabolomic, env)\n",
        "\n",
        "        loss_fate = F.cross_entropy(fate_logits, fate)\n",
        "        loss_tel = F.binary_cross_entropy(tel_pred, tel)\n",
        "        # Combined loss: weight telomerase head to encourage correct transient control\n",
        "        loss = loss_fate + 0.5 * loss_tel\n",
        "\n",
        "        loss.backward()\n",
        "        # gradient clipping - helpful when using higher LR\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * genomic.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_fates = []\n",
        "    all_fate_preds = []\n",
        "    all_tel = []\n",
        "    all_tel_preds = []\n",
        "    for batch in dataloader:\n",
        "        genomic = batch['genomic'].to(device)\n",
        "        proteomic = batch['proteomic'].to(device)\n",
        "        metabolomic = batch['metabolomic'].to(device)\n",
        "        env = batch['env'].to(device)\n",
        "        fate = batch['fate'].cpu().numpy()\n",
        "        tel = batch['tel'].cpu().numpy()\n",
        "\n",
        "        fate_logits, tel_pred = model(genomic, proteomic, metabolomic, env)\n",
        "        fate_probs = F.softmax(fate_logits, dim=-1).cpu().numpy()\n",
        "        fate_pred = np.argmax(fate_probs, axis=1)\n",
        "        tel_pred_np = tel_pred.cpu().numpy()\n",
        "\n",
        "        all_fates.append(fate)\n",
        "        all_fate_preds.append(fate_pred)\n",
        "        all_tel.append(tel)\n",
        "        all_tel_preds.append(tel_pred_np)\n",
        "\n",
        "    all_fates = np.concatenate(all_fates)\n",
        "    all_fate_preds = np.concatenate(all_fate_preds)\n",
        "    all_tel = np.concatenate(all_tel)\n",
        "    all_tel_preds = np.concatenate(all_tel_preds)\n",
        "\n",
        "    acc = accuracy_score(all_fates, all_fate_preds)\n",
        "    try:\n",
        "        auc = roc_auc_score(all_tel, all_tel_preds)\n",
        "    except Exception:\n",
        "        auc = float('nan')\n",
        "    return {'accuracy': acc, 'telomerase_auc': auc}\n",
        "\n",
        "# -------------------------\n",
        "# Main training script\n",
        "# -------------------------\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Data\n",
        "    ds = SyntheticCellDataset(n_samples=8000)\n",
        "    train_size = int(0.8 * len(ds))\n",
        "    val_size = len(ds) - train_size\n",
        "    train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=1, pin_memory=True)\n",
        "\n",
        "    # Model\n",
        "    model = VirtualCellModel(dims=(ds.dim_g, ds.dim_p, ds.dim_m, ds.dim_e), hidden=256, dropout=0.12)\n",
        "    model.to(device)\n",
        "\n",
        "    # ---- HIGHER LEARNING RATE SETTING ----\n",
        "    # You asked for \"higher learning rate\". Start with lr=5e-3 (higher than typical 1e-3) and\n",
        "    # consider experimenting with 1e-2, but use warmup and reduce-on-plateau scheduler for stability.\n",
        "    lr = 5e-3\n",
        "    weight_decay = 1e-4\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Warmup scheduler (linear warmup then cosine decay)\n",
        "    total_steps = 2000\n",
        "    warmup_steps = 200\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        # cosine decay after warmup\n",
        "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "    best_val = -1.0\n",
        "    epochs = 25\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device, clip_grad=1.0)\n",
        "        scheduler.step()\n",
        "        step += 1\n",
        "\n",
        "        metrics = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1:02d} | train_loss={train_loss:.4f} | val_acc={metrics['accuracy']:.4f} | tel_auc={metrics['telomerase_auc']:.4f} | lr={optimizer.param_groups[0]['lr']:.5f}\")\n",
        "\n",
        "        # simple checkpointing\n",
        "        if metrics['telomerase_auc'] > best_val:\n",
        "            best_val = metrics['telomerase_auc']\n",
        "            torch.save(model.state_dict(), 'best_virtual_cell_model.pt')\n",
        "\n",
        "    print(\"Training finished. Best telomerase AUC:\", best_val)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RL-based telomerase controller (PPO) with a simple cell simulator environment.\n",
        "# This code is intended to be run in a local Python environment with PyTorch installed.\n",
        "# It creates a lightweight gym-like environment that simulates cell states and the\n",
        "# outcome probabilities (healthy division / senescence / apoptosis) influenced by\n",
        "# telomerase actions. A PPO agent learns when to transiently activate telomerase\n",
        "# to maximize long-term tissue health while minimizing cancer risk.\n",
        "#\n",
        "# To run here: the python_user_visible tool will execute and show training logs.\n",
        "# Requirements: torch, numpy\n",
        "# pip install torch numpy\n",
        "#\n",
        "# Note: This is an educational simulation — not biological guidance.\n",
        "\n",
        "import math, random, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# Simple cell environment\n",
        "# -------------------------\n",
        "class SimpleCellEnv:\n",
        "    \"\"\"\n",
        "    Minimal episodic environment representing a single cell's lifecycle across T steps.\n",
        "    State: [telomere_proxy, ros, age, cumulative_tel_actions]\n",
        "    Action: 0 (no telomerase), 1 (activate telomerase transiently)\n",
        "    Dynamics: telomere_proxy tends to shorten (decrease) each step; action restores a\n",
        "    small amount. High ROS pushes toward apoptosis; very short telomeres -> senescence.\n",
        "    Repeated telomerase activation increases cumulative_cancer_risk.\n",
        "    Reward: +1 for healthy division outcome, -1 for senescence, -2 for apoptosis.\n",
        "    Also penalize cancer_risk at episode end.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_steps=10, seed=0):\n",
        "        self.max_steps = max_steps\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # telomere_proxy: higher is healthier. Start near zero with variance.\n",
        "        self.tel = float(self.rng.normal(loc=0.0, scale=0.3))\n",
        "        # ROS baseline\n",
        "        self.ros = float(self.rng.normal(loc=0.2, scale=0.2))\n",
        "        self.age = 0.0\n",
        "        self.cumulative_tel_actions = 0.0\n",
        "        self.step_count = 0\n",
        "        self.done = False\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # normalize to roughly [-3,3] ranges\n",
        "        return np.array([self.tel, self.ros, self.age / (self.max_steps + 1), self.cumulative_tel_actions / (self.max_steps + 1)], dtype=np.float32)\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
        "        \"\"\"\n",
        "        Perform action (0 or 1). Returns obs, reward (immediate 0 until terminal), done, info.\n",
        "        We'll only give outcome reward at terminal step (a simple episodic decision process).\n",
        "        \"\"\"\n",
        "        assert action in (0,1)\n",
        "        # Action effect: transiently increases telomere length\n",
        "        if action == 1:\n",
        "            # small restoration\n",
        "            self.tel += 0.5 * (1.0 - 0.1 * self.cumulative_tel_actions)  # diminishing returns\n",
        "            self.cumulative_tel_actions += 1.0\n",
        "        # Natural telomere shortening and ROS changes\n",
        "        # telomere shortens a bit each step, and ROS may fluctuate\n",
        "        self.tel -= 0.2 + 0.05 * self.rng.randn()\n",
        "        self.ros += 0.05 * self.rng.randn()\n",
        "\n",
        "        self.age += 1.0\n",
        "        self.step_count += 1\n",
        "\n",
        "        done = False\n",
        "        reward = 0.0\n",
        "        info = {}\n",
        "\n",
        "        if self.step_count >= self.max_steps:\n",
        "            # determine outcome probabilistically\n",
        "            # base logits depend on tel and ros\n",
        "            # score: healthier with larger tel, lower ros\n",
        "            tel_score = self.tel  # higher better\n",
        "            ros_score = -self.ros  # higher ros worse -> negative\n",
        "            # softmax over three outcomes\n",
        "            logits = np.array([1.2*tel_score + 0.2*ros_score,    # healthy\n",
        "                               -0.5*tel_score + 0.6*(-ros_score), # senescence influenced by short tel (low tel_score)\n",
        "                               -0.8*ros_score + 0.1*(1.0 - tel_score)])  # apoptosis influenced by high ros\n",
        "            # add stochasticity\n",
        "            logits += 0.3 * self.rng.randn(3)\n",
        "            probs = np.exp(logits - np.max(logits))\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            # sample outcome\n",
        "            outcome = self.rng.choice(3, p=probs)\n",
        "            if outcome == 0:\n",
        "                reward = 1.0  # healthy division\n",
        "            elif outcome == 1:\n",
        "                reward = -1.0  # senescence\n",
        "            else:\n",
        "                reward = -2.0  # apoptosis\n",
        "\n",
        "            # cancer risk penalty: if cumulative telomerase activations exceed threshold,\n",
        "            # increase long-term cancer risk; penalize proportionally.\n",
        "            cancer_risk = max(0.0, (self.cumulative_tel_actions - 1.5) * 0.2)  # small penalty per excess use\n",
        "            reward -= cancer_risk\n",
        "\n",
        "            done = True\n",
        "            info['outcome'] = outcome\n",
        "            info['probs'] = probs\n",
        "            info['cancer_risk'] = cancer_risk\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        self.done = done\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# -------------------------\n",
        "# PPO agent (policy + value)\n",
        "# -------------------------\n",
        "class MLPActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # logit for binary action\n",
        "        )\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.shared(x)\n",
        "        logit = self.policy(h).squeeze(-1)\n",
        "        value = self.value(h).squeeze(-1)\n",
        "        return logit, value\n",
        "\n",
        "# Helper: compute advantages (GAE)\n",
        "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    advantages = np.zeros_like(rewards)\n",
        "    lastgae = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        nonterminal = 1.0 - dones[t]\n",
        "        delta = rewards[t] + gamma * (values[t+1] if t+1 < len(values) else 0.0) * nonterminal - values[t]\n",
        "        advantages[t] = lastgae = delta + gamma * lam * nonterminal * lastgae\n",
        "    returns = advantages + values[:len(advantages)]\n",
        "    return advantages, returns\n",
        "\n",
        "# -------------------------\n",
        "# Training loop (vectorized episodes)\n",
        "# -------------------------\n",
        "def train_ppo(env_ctor, steps=2000, batch_size=32, epochs=20, clip_eps=0.2,\n",
        "              policy_lr=3e-4, value_lr=1e-3, gamma=0.99):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = env_ctor()\n",
        "    obs_dim = env._get_obs().shape[0]\n",
        "    ac = MLPActorCritic(obs_dim).to(device)\n",
        "    optimizer = optim.Adam(ac.parameters(), lr=policy_lr)\n",
        "    value_optimizer = None  # we'll use single optimizer for simplicity\n",
        "\n",
        "    # Storage\n",
        "    all_rewards = []\n",
        "    print(f\"Training PPO for {steps} episodes. Device: {device}\")\n",
        "\n",
        "    for ep in range(steps):\n",
        "        obs_buf = []\n",
        "        act_buf = []\n",
        "        logp_buf = []\n",
        "        rew_buf = []\n",
        "        val_buf = []\n",
        "        done_buf = []\n",
        "\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        traj_obs = []\n",
        "        traj_rewards = []\n",
        "        traj_values = []\n",
        "        traj_logps = []\n",
        "        traj_actions = []\n",
        "        traj_dones = []\n",
        "\n",
        "        # run one episode\n",
        "        while True:\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                logit, value = ac(obs_t)\n",
        "                prob = torch.sigmoid(logit).cpu().numpy()[0]\n",
        "            action = 1 if random.random() < prob else 0\n",
        "            # compute logp for action\n",
        "            logp = math.log(prob + 1e-8) if action == 1 else math.log(1.0 - prob + 1e-8)\n",
        "\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            traj_obs.append(obs.copy())\n",
        "            traj_rewards.append(reward)\n",
        "            traj_values.append(value.cpu().numpy()[0])\n",
        "            traj_logps.append(logp)\n",
        "            traj_actions.append(action)\n",
        "            traj_dones.append(float(done))\n",
        "\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # value for terminal next state assumed 0\n",
        "        values_np = np.array(traj_values + [0.0], dtype=np.float32)\n",
        "        advantages, returns = compute_gae(traj_rewards, values_np, traj_dones, gamma=gamma)\n",
        "\n",
        "        # convert to tensors\n",
        "        obs_tensor = torch.tensor(np.array(traj_obs), dtype=torch.float32).to(device)\n",
        "        actions_tensor = torch.tensor(np.array(traj_actions), dtype=torch.float32).to(device)\n",
        "        old_logps = torch.tensor(np.array(traj_logps), dtype=torch.float32).to(device)\n",
        "        advantages_tensor = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Normalize advantages\n",
        "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
        "\n",
        "        # PPO update (several epochs on the single-episode batch)\n",
        "        for _ in range(8):  # minibatch multiple epochs\n",
        "            logits, values_pred = ac(obs_tensor)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            # new log prob\n",
        "            new_logps = actions_tensor * torch.log(probs + 1e-8) + (1 - actions_tensor) * torch.log(1 - probs + 1e-8)\n",
        "            ratio = torch.exp(new_logps - old_logps)\n",
        "            surr1 = ratio * advantages_tensor\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages_tensor\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.mse_loss(values_pred, returns_tensor)\n",
        "            entropy = -(probs * torch.log(probs + 1e-8) + (1-probs) * torch.log(1-probs + 1e-8)).mean()\n",
        "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(ac.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "        all_rewards.append(ep_reward)\n",
        "\n",
        "        # Logging\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            last_mean = np.mean(all_rewards[-50:])\n",
        "            print(f\"Episode {ep+1}/{steps} | avg reward (last 50): {last_mean:.3f} | last ep reward: {ep_reward:.3f}\")\n",
        "    return ac, all_rewards\n",
        "\n",
        "# -------------------------\n",
        "# Run training (short) and show results\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    # Smaller training for demo purposes (you can increase steps to 2000+ for real training)\n",
        "    policy, rewards = train_ppo(lambda: SimpleCellEnv(max_steps=8, seed=int(time.time()%10000)), steps=400, policy_lr=2e-4)\n",
        "    end = time.time()\n",
        "    print(f\"Finished training in {end-start:.2f}s. Mean reward last 50: {np.mean(rewards[-50:]):.3f}\")\n",
        "\n",
        "    # Show some sample rollouts of the learned policy\n",
        "    env = SimpleCellEnv(max_steps=8, seed=1234)\n",
        "    obs = env.reset()\n",
        "    traj = []\n",
        "    for _ in range(20):\n",
        "        obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "        logit, _ = policy(obs_t)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "        action = 1 if random.random() < prob else 0\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "        traj.append((obs.copy(), action, reward, done, info))\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(\"Sample rollout (obs, action, reward, done, info):\")\n",
        "    for x in traj:\n",
        "        print(x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY8rdFnzhhmo",
        "outputId": "14034af0-a39d-4175-8d26-2aab82f1593f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training PPO for 400 episodes. Device: cpu\n",
            "Episode 50/400 | avg reward (last 50): -1.132 | last ep reward: -1.900\n",
            "Episode 100/400 | avg reward (last 50): -1.064 | last ep reward: -0.300\n",
            "Episode 150/400 | avg reward (last 50): -1.116 | last ep reward: -0.300\n",
            "Episode 200/400 | avg reward (last 50): -1.400 | last ep reward: -2.300\n",
            "Episode 250/400 | avg reward (last 50): -1.212 | last ep reward: -2.300\n",
            "Episode 300/400 | avg reward (last 50): -1.644 | last ep reward: -0.300\n",
            "Episode 350/400 | avg reward (last 50): -1.208 | last ep reward: -0.300\n",
            "Episode 400/400 | avg reward (last 50): -0.980 | last ep reward: -0.300\n",
            "Finished training in 11.63s. Mean reward last 50: -0.980\n",
            "Sample rollout (obs, action, reward, done, info):\n",
            "(array([0.4298121 , 0.13746962, 0.        , 0.        ], dtype=float32), 1, 0.0, False, {})\n",
            "(array([0.76584154, 0.18182777, 0.11111111, 0.11111111], dtype=float32), 0, 0.0, False, {})\n",
            "(array([0.5228621 , 0.15000159, 0.22222222, 0.11111111], dtype=float32), 1, 0.0, False, {})\n",
            "(array([0.77207726, 0.03786734, 0.33333334, 0.22222222], dtype=float32), 1, 0.0, False, {})\n",
            "(array([0.9145755 , 0.08746465, 0.44444445, 0.33333334], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 1.0169092 , -0.01359809,  0.5555556 ,  0.44444445], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 1.1336131 , -0.01349218,  0.6666667 ,  0.5555556 ], dtype=float32), 1, 0.0, False, {})\n",
            "(array([1.1633404e+00, 9.6242025e-04, 7.7777779e-01, 6.6666669e-01],\n",
            "      dtype=float32), 1, -0.10000000000000009, True, {'outcome': 0, 'probs': array([0.71222184, 0.09055033, 0.19722783]), 'cancer_risk': 1.1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retry execution with graceful fallback if PyTorch missing.\n",
        "# This cell checks for torch; if unavailable, it prints installation instructions and exits.\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import torch.nn.functional as F\n",
        "except Exception as e:\n",
        "    print(\"PyTorch import failed:\", e)\n",
        "    print(\"If you want to run the RL demo locally, install PyTorch first. Example:\")\n",
        "    print(\"  pip install torch --index-url https://download.pytorch.org/whl/cpu\")\n",
        "    raise SystemExit(\"Missing PyTorch — aborting execution here.\")\n",
        "\n",
        "import math, random, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# Simple cell environment (same as before)\n",
        "# -------------------------\n",
        "class SimpleCellEnv:\n",
        "    def __init__(self, max_steps=10, seed=0):\n",
        "        self.max_steps = max_steps\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tel = float(self.rng.normal(loc=0.0, scale=0.3))\n",
        "        self.ros = float(self.rng.normal(loc=0.2, scale=0.2))\n",
        "        self.age = 0.0\n",
        "        self.cumulative_tel_actions = 0.0\n",
        "        self.step_count = 0\n",
        "        self.done = False\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.array([self.tel, self.ros, self.age / (self.max_steps + 1), self.cumulative_tel_actions / (self.max_steps + 1)], dtype=np.float32)\n",
        "\n",
        "    def step(self, action: int):\n",
        "        assert action in (0,1)\n",
        "        if action == 1:\n",
        "            self.tel += 0.5 * (1.0 - 0.1 * self.cumulative_tel_actions)\n",
        "            self.cumulative_tel_actions += 1.0\n",
        "        self.tel -= 0.2 + 0.05 * self.rng.randn()\n",
        "        self.ros += 0.05 * self.rng.randn()\n",
        "        self.age += 1.0\n",
        "        self.step_count += 1\n",
        "        done = False\n",
        "        reward = 0.0\n",
        "        info = {}\n",
        "        if self.step_count >= self.max_steps:\n",
        "            tel_score = self.tel\n",
        "            ros_score = -self.ros\n",
        "            logits = np.array([1.2*tel_score + 0.2*ros_score,\n",
        "                               -0.5*tel_score + 0.6*(-ros_score),\n",
        "                               -0.8*ros_score + 0.1*(1.0 - tel_score)])\n",
        "            logits += 0.3 * self.rng.randn(3)\n",
        "            probs = np.exp(logits - np.max(logits))\n",
        "            probs = probs / probs.sum()\n",
        "            outcome = self.rng.choice(3, p=probs)\n",
        "            if outcome == 0:\n",
        "                reward = 1.0\n",
        "            elif outcome == 1:\n",
        "                reward = -1.0\n",
        "            else:\n",
        "                reward = -2.0\n",
        "            cancer_risk = max(0.0, (self.cumulative_tel_actions - 1.5) * 0.2)\n",
        "            reward -= cancer_risk\n",
        "            done = True\n",
        "            info['outcome'] = int(outcome)\n",
        "            info['probs'] = probs.tolist()\n",
        "            info['cancer_risk'] = float(cancer_risk)\n",
        "        obs = self._get_obs()\n",
        "        self.done = done\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# -------------------------\n",
        "# PPO agent (policy + value)\n",
        "# -------------------------\n",
        "class MLPActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy = nn.Sequential(nn.Linear(hidden, 64), nn.ReLU(), nn.Linear(64, 1))\n",
        "        self.value = nn.Sequential(nn.Linear(hidden, 64), nn.ReLU(), nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.shared(x)\n",
        "        logit = self.policy(h).squeeze(-1)\n",
        "        value = self.value(h).squeeze(-1)\n",
        "        return logit, value\n",
        "\n",
        "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    advantages = np.zeros_like(rewards, dtype=np.float32)\n",
        "    lastgae = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        nonterminal = 1.0 - dones[t]\n",
        "        next_value = values[t+1] if t+1 < len(values) else 0.0\n",
        "        delta = rewards[t] + gamma * next_value * nonterminal - values[t]\n",
        "        lastgae = delta + gamma * lam * nonterminal * lastgae\n",
        "        advantages[t] = lastgae\n",
        "    returns = advantages + values[:len(advantages)]\n",
        "    return advantages, returns\n",
        "\n",
        "def train_ppo(env_ctor, steps=200, clip_eps=0.2, policy_lr=2e-4, gamma=0.99):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = env_ctor()\n",
        "    obs_dim = env._get_obs().shape[0]\n",
        "    ac = MLPActorCritic(obs_dim).to(device)\n",
        "    optimizer = optim.Adam(ac.parameters(), lr=policy_lr)\n",
        "    all_rewards = []\n",
        "    start_time = time.time()\n",
        "    for ep in range(steps):\n",
        "        obs = env.reset()\n",
        "        traj_obs, traj_actions, traj_rewards, traj_values, traj_logps, traj_dones = [], [], [], [], [], []\n",
        "        ep_reward = 0.0\n",
        "        while True:\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                logit, value = ac(obs_t)\n",
        "                prob = torch.sigmoid(logit).cpu().numpy()[0]\n",
        "            action = 1 if random.random() < prob else 0\n",
        "            logp = math.log(prob + 1e-8) if action == 1 else math.log(1.0 - prob + 1e-8)\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            traj_obs.append(obs.copy())\n",
        "            traj_actions.append(action)\n",
        "            traj_rewards.append(reward)\n",
        "            traj_values.append(value.cpu().numpy()[0])\n",
        "            traj_logps.append(logp)\n",
        "            traj_dones.append(float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        values_np = np.array(traj_values + [0.0], dtype=np.float32)\n",
        "        advantages, returns = compute_gae(traj_rewards, values_np, traj_dones, gamma=gamma)\n",
        "        obs_tensor = torch.tensor(np.array(traj_obs), dtype=torch.float32).to(device)\n",
        "        actions_tensor = torch.tensor(np.array(traj_actions), dtype=torch.float32).to(device)\n",
        "        old_logps = torch.tensor(np.array(traj_logps), dtype=torch.float32).to(device)\n",
        "        advantages_tensor = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
        "        # PPO update\n",
        "        for _ in range(6):\n",
        "            logits, values_pred = ac(obs_tensor)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            new_logps = actions_tensor * torch.log(probs + 1e-8) + (1 - actions_tensor) * torch.log(1 - probs + 1e-8)\n",
        "            ratio = torch.exp(new_logps - old_logps)\n",
        "            surr1 = ratio * advantages_tensor\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages_tensor\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.mse_loss(values_pred, returns_tensor)\n",
        "            entropy = -(probs * torch.log(probs + 1e-8) + (1-probs) * torch.log(1-probs + 1e-8)).mean()\n",
        "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(ac.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "        all_rewards.append(ep_reward)\n",
        "        if (ep + 1) % 40 == 0:\n",
        "            avg = np.mean(all_rewards[-40:])\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"Episode {ep+1}/{steps} | avg reward (last 40): {avg:.3f} | elapsed: {elapsed:.1f}s\")\n",
        "    return ac, all_rewards\n",
        "\n",
        "# Run a short training session for demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    ac_model, rewards = train_ppo(lambda: SimpleCellEnv(max_steps=8, seed=42), steps=200, policy_lr=2e-4)\n",
        "    print(\"Training complete. Mean reward (last 50):\", np.mean(rewards[-50:]))\n",
        "\n",
        "    # Show a sample rollout\n",
        "    env = SimpleCellEnv(max_steps=8, seed=999)\n",
        "    obs = env.reset()\n",
        "    rollout = []\n",
        "    for _ in range(20):\n",
        "        obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            logit, _ = ac_model(obs_t)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "        action = 1 if random.random() < prob else 0\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "        rollout.append((obs.copy(), action, reward, done, info))\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            break\n",
        "    print(\"Sample rollout:\")\n",
        "    for step in rollout:\n",
        "        print(step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTQr0Frah8x4",
        "outputId": "33fe01d5-93a9-49d6-e960-a7bf89453155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 40/200 | avg reward (last 40): -1.353 | elapsed: 0.8s\n",
            "Episode 80/200 | avg reward (last 40): -1.220 | elapsed: 1.8s\n",
            "Episode 120/200 | avg reward (last 40): -1.080 | elapsed: 2.9s\n",
            "Episode 160/200 | avg reward (last 40): -1.035 | elapsed: 3.9s\n",
            "Episode 200/200 | avg reward (last 40): -1.090 | elapsed: 4.8s\n",
            "Training complete. Mean reward (last 50): -1.0519999999999998\n",
            "Sample rollout:\n",
            "(array([0.0944445 , 0.02831017, 0.        , 0.        ], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 0.40775123, -0.00413487,  0.11111111,  0.11111111], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 0.57943785, -0.10870337,  0.22222222,  0.22222222], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 0.7066214 , -0.06143871,  0.33333334,  0.33333334], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 0.8766315 , -0.04567734,  0.44444445,  0.44444445], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 1.0321345 , -0.07491842,  0.5555556 ,  0.5555556 ], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 1.091555  , -0.03426724,  0.6666667 ,  0.6666667 ], dtype=float32), 1, 0.0, False, {})\n",
            "(array([1.0996202 , 0.04577634, 0.7777778 , 0.7777778 ], dtype=float32), 1, -3.3, True, {'outcome': 2, 'probs': [0.7121335307716062, 0.10662926303363995, 0.18123720619475378], 'cancer_risk': 1.3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ppo_telomerase.py\n",
        "# Requirements: torch, numpy\n",
        "# pip install torch numpy\n",
        "import math, random, time\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# Simple cell environment\n",
        "# -------------------------\n",
        "class SimpleCellEnv:\n",
        "    \"\"\"\n",
        "    Minimal episodic environment representing a single cell's lifecycle across T steps.\n",
        "    State: [telomere_proxy, ros, age_norm, cumulative_tel_actions_norm]\n",
        "    Action: 0 (no telomerase), 1 (activate telomerase transiently)\n",
        "    Reward: terminal only: +1 healthy, -1 senescence, -2 apoptosis, minus cancer risk penalty.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_steps=10, seed=0):\n",
        "        self.max_steps = max_steps\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tel = float(self.rng.normal(loc=0.0, scale=0.3))\n",
        "        self.ros = float(self.rng.normal(loc=0.2, scale=0.2))\n",
        "        self.age = 0.0\n",
        "        self.cumulative_tel_actions = 0.0\n",
        "        self.step_count = 0\n",
        "        self.done = False\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.array([\n",
        "            self.tel,\n",
        "            self.ros,\n",
        "            self.age / (self.max_steps + 1),\n",
        "            self.cumulative_tel_actions / (self.max_steps + 1)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step(self, action: int):\n",
        "        assert action in (0,1)\n",
        "        if action == 1:\n",
        "            # transient restoration, diminishing returns\n",
        "            self.tel += 0.5 * (1.0 - 0.1 * self.cumulative_tel_actions)\n",
        "            self.cumulative_tel_actions += 1.0\n",
        "\n",
        "        # natural decline + noise\n",
        "        self.tel -= 0.2 + 0.05 * self.rng.randn()\n",
        "        self.ros += 0.05 * self.rng.randn()\n",
        "        self.age += 1.0\n",
        "        self.step_count += 1\n",
        "\n",
        "        reward = 0.0\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        if self.step_count >= self.max_steps:\n",
        "            # outcome logits depend on tel and ros (toy model)\n",
        "            tel_score = self.tel\n",
        "            ros_score = -self.ros\n",
        "            logits = np.array([\n",
        "                1.2*tel_score + 0.2*ros_score,                      # healthy\n",
        "                -0.5*tel_score + 0.6*(-ros_score),                  # senescence (short tel)\n",
        "                -0.8*ros_score + 0.1*(1.0 - tel_score)              # apoptosis (high ros)\n",
        "            ])\n",
        "            logits += 0.3 * self.rng.randn(3)  # stochasticity\n",
        "            probs = np.exp(logits - np.max(logits))\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            outcome = self.rng.choice(3, p=probs)\n",
        "            if outcome == 0:\n",
        "                reward = 1.0\n",
        "            elif outcome == 1:\n",
        "                reward = -1.0\n",
        "            else:\n",
        "                reward = -2.0\n",
        "\n",
        "            # cancer risk penalty for repeated telomerase activations\n",
        "            cancer_risk = max(0.0, (self.cumulative_tel_actions - 1.5) * 0.2)\n",
        "            reward -= cancer_risk\n",
        "\n",
        "            done = True\n",
        "            info['outcome'] = int(outcome)\n",
        "            info['probs'] = probs.tolist()\n",
        "            info['cancer_risk'] = float(cancer_risk)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        self.done = done\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# -------------------------\n",
        "# PPO actor-critic model\n",
        "# -------------------------\n",
        "class MLPActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # binary action logit\n",
        "        )\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.shared(x)\n",
        "        logit = self.policy(h).squeeze(-1)\n",
        "        value = self.value(h).squeeze(-1)\n",
        "        return logit, value\n",
        "\n",
        "# Generalized Advantage Estimation\n",
        "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    advantages = np.zeros_like(rewards, dtype=np.float32)\n",
        "    lastgae = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        nonterminal = 1.0 - dones[t]\n",
        "        next_value = values[t+1] if t+1 < len(values) else 0.0\n",
        "        delta = rewards[t] + gamma * next_value * nonterminal - values[t]\n",
        "        lastgae = delta + gamma * lam * nonterminal * lastgae\n",
        "        advantages[t] = lastgae\n",
        "    returns = advantages + values[:len(advantages)]\n",
        "    return advantages, returns\n",
        "\n",
        "# -------------------------\n",
        "# PPO training loop\n",
        "# -------------------------\n",
        "def train_ppo(env_ctor, steps=1000, clip_eps=0.2, policy_lr=2e-4, gamma=0.99):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = env_ctor()\n",
        "    obs_dim = env._get_obs().shape[0]\n",
        "    ac = MLPActorCritic(obs_dim).to(device)\n",
        "    optimizer = optim.Adam(ac.parameters(), lr=policy_lr)\n",
        "    all_rewards = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for ep in range(steps):\n",
        "        obs = env.reset()\n",
        "        traj_obs, traj_actions, traj_rewards, traj_values, traj_logps, traj_dones = [], [], [], [], [], []\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        # collect one episode (you can vectorize for efficiency)\n",
        "        while True:\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                logit, value = ac(obs_t)\n",
        "                prob = torch.sigmoid(logit).cpu().numpy()[0]\n",
        "            action = 1 if random.random() < prob else 0\n",
        "            logp = math.log(prob + 1e-8) if action == 1 else math.log(1.0 - prob + 1e-8)\n",
        "\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            traj_obs.append(obs.copy())\n",
        "            traj_actions.append(action)\n",
        "            traj_rewards.append(reward)\n",
        "            traj_values.append(value.cpu().numpy()[0])\n",
        "            traj_logps.append(logp)\n",
        "            traj_dones.append(float(done))\n",
        "\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # compute GAE\n",
        "        values_np = np.array(traj_values + [0.0], dtype=np.float32)\n",
        "        advantages, returns = compute_gae(traj_rewards, values_np, traj_dones, gamma=gamma)\n",
        "\n",
        "        # tensors\n",
        "        obs_tensor = torch.tensor(np.array(traj_obs), dtype=torch.float32).to(device)\n",
        "        actions_tensor = torch.tensor(np.array(traj_actions), dtype=torch.float32).to(device)\n",
        "        old_logps = torch.tensor(np.array(traj_logps), dtype=torch.float32).to(device)\n",
        "        advs = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
        "        rets = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
        "\n",
        "        # PPO update: multiple epochs over this episode's data\n",
        "        for _ in range(6):\n",
        "            logits, values_pred = ac(obs_tensor)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            new_logps = actions_tensor * torch.log(probs + 1e-8) + (1 - actions_tensor) * torch.log(1 - probs + 1e-8)\n",
        "            ratio = torch.exp(new_logps - old_logps)\n",
        "            surr1 = ratio * advs\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advs\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.mse_loss(values_pred, rets)\n",
        "            entropy = -(probs * torch.log(probs + 1e-8) + (1-probs) * torch.log(1-probs + 1e-8)).mean()\n",
        "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(ac.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "        all_rewards.append(ep_reward)\n",
        "        # logging\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            avg = np.mean(all_rewards[-50:])\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"Episode {ep+1}/{steps} | avg reward (last 50): {avg:.3f} | elapsed: {elapsed:.1f}s\")\n",
        "    return ac, all_rewards\n",
        "\n",
        "# -------------------------\n",
        "# Run training (adjust steps for longer training)\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    model, rewards = train_ppo(lambda: SimpleCellEnv(max_steps=8, seed=42), steps=400, policy_lr=2e-4)\n",
        "    print(\"Training complete. Mean reward (last 50):\", np.mean(rewards[-50:]))\n",
        "\n",
        "    # Show a sample rollout\n",
        "    env = SimpleCellEnv(max_steps=8, seed=1234)\n",
        "    obs = env.reset()\n",
        "    rollout = []\n",
        "    for _ in range(20):\n",
        "        obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            logit, _ = model(obs_t)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "        action = 1 if random.random() < prob else 0\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "        rollout.append((obs.copy(), action, reward, done, info))\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            break\n",
        "    print(\"Sample rollout:\")\n",
        "    for step in rollout:\n",
        "        print(step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBKpAzKsinyh",
        "outputId": "9949c79b-d3d5-41c9-b620-050ea464a74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50/400 | avg reward (last 50): -1.178 | elapsed: 1.0s\n",
            "Episode 100/400 | avg reward (last 50): -0.996 | elapsed: 1.9s\n",
            "Episode 150/400 | avg reward (last 50): -1.152 | elapsed: 3.0s\n",
            "Episode 200/400 | avg reward (last 50): -1.056 | elapsed: 4.0s\n",
            "Episode 250/400 | avg reward (last 50): -1.260 | elapsed: 5.1s\n",
            "Episode 300/400 | avg reward (last 50): -1.452 | elapsed: 6.1s\n",
            "Episode 350/400 | avg reward (last 50): -0.712 | elapsed: 7.2s\n",
            "Episode 400/400 | avg reward (last 50): -1.484 | elapsed: 8.3s\n",
            "Training complete. Mean reward (last 50): -1.4839999999999998\n",
            "Sample rollout:\n",
            "(array([0.4298121 , 0.13746962, 0.        , 0.        ], dtype=float32), 1, 0.0, False, {})\n",
            "(array([0.76584154, 0.18182777, 0.11111111, 0.11111111], dtype=float32), 1, 0.0, False, {})\n",
            "(array([0.9728621 , 0.15000159, 0.22222222, 0.22222222], dtype=float32), 1, 0.0, False, {})\n",
            "(array([1.1720773 , 0.03786734, 0.33333334, 0.33333334], dtype=float32), 1, 0.0, False, {})\n",
            "(array([1.2645755 , 0.08746465, 0.44444445, 0.44444445], dtype=float32), 0, 0.0, False, {})\n",
            "(array([ 1.0169092 , -0.01359809,  0.5555556 ,  0.44444445], dtype=float32), 1, 0.0, False, {})\n",
            "(array([ 1.1336131 , -0.01349218,  0.6666667 ,  0.5555556 ], dtype=float32), 1, 0.0, False, {})\n",
            "(array([1.1633404e+00, 9.6242025e-04, 7.7777779e-01, 6.6666669e-01],\n",
            "      dtype=float32), 1, -0.10000000000000009, True, {'outcome': 0, 'probs': [0.7122218423358028, 0.0905503324426957, 0.19722782522150145], 'cancer_risk': 1.1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, random, math\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# Multi-cell tissue environment\n",
        "# -------------------------\n",
        "class Cell:\n",
        "    def __init__(self, rng):\n",
        "        self.rng = rng\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tel = float(self.rng.normal(loc=0.0, scale=0.3))\n",
        "        self.ros = float(self.rng.normal(loc=0.2, scale=0.2))\n",
        "        self.age = 0.0\n",
        "        self.cumulative_tel = 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        # action: 0 or 1 for this cell in this timestep\n",
        "        if action == 1:\n",
        "            self.tel += 0.5 * (1.0 - 0.1 * self.cumulative_tel)\n",
        "            self.cumulative_tel += 1.0\n",
        "        self.tel -= 0.15 + 0.05 * self.rng.randn()\n",
        "        self.ros += 0.03 * self.rng.randn()\n",
        "        self.age += 1.0\n",
        "\n",
        "    def get_obs(self, max_steps):\n",
        "        return np.array([self.tel, self.ros, self.age / (max_steps + 1), self.cumulative_tel], dtype=np.float32)\n",
        "\n",
        "\n",
        "class TissueEnv:\n",
        "    \"\"\"\n",
        "    Tissue of M cells. Episode runs T steps. At the end, each cell produces an outcome\n",
        "    (healthy, senescent, apoptotic) probabilistically based on tel and ros.\n",
        "    Reward: sum over cells of (+1 healthy, -1 senescent, -2 apoptotic) minus cancer penalties\n",
        "    that scale with excess telomerase activations per cell.\n",
        "    \"\"\"\n",
        "    def __init__(self, M=10, T=6, budget=3, seed=0):\n",
        "        self.M = M\n",
        "        self.T = T\n",
        "        self.budget = budget\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.cells = [Cell(self.rng) for _ in range(M)]\n",
        "        self.step_count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.cells = [Cell(self.rng) for _ in range(self.M)]\n",
        "        self.step_count = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # Return M x obs_dim matrix and a global summary\n",
        "        per_cell = np.stack([c.get_obs(self.T) for c in self.cells], axis=0)  # (M,4)\n",
        "        # global summary: mean tel, mean ros, step fraction\n",
        "        mean_tel = per_cell[:,0].mean()\n",
        "        mean_ros = per_cell[:,1].mean()\n",
        "        step_frac = self.step_count / (self.T + 1)\n",
        "        global_feat = np.array([mean_tel, mean_ros, step_frac], dtype=np.float32)\n",
        "        return per_cell, global_feat\n",
        "\n",
        "    def step(self, actions: List[int]):\n",
        "        # actions: list/array of length M with 0/1, but we assume caller enforces budget.\n",
        "        assert len(actions) == self.M\n",
        "        for c, a in zip(self.cells, actions):\n",
        "            c.step(int(a))\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= self.T\n",
        "        reward = 0.0\n",
        "        info = {}\n",
        "        if done:\n",
        "            # compute cell outcomes\n",
        "            rewards = []\n",
        "            cancer_penalty = 0.0\n",
        "            for c in self.cells:\n",
        "                tel_score = c.tel\n",
        "                ros_score = -c.ros\n",
        "                logits = np.array([1.1*tel_score + 0.2*ros_score,\n",
        "                                   -0.6*tel_score + 0.5*(-ros_score),\n",
        "                                   -0.9*ros_score + 0.05*(1.0 - tel_score)])\n",
        "                logits += 0.25 * self.rng.randn(3)\n",
        "                probs = np.exp(logits - np.max(logits))\n",
        "                probs = probs / probs.sum()\n",
        "                outcome = self.rng.choice(3, p=probs)\n",
        "                if outcome == 0:\n",
        "                    r = 1.0\n",
        "                elif outcome == 1:\n",
        "                    r = -1.0\n",
        "                else:\n",
        "                    r = -2.0\n",
        "                rewards.append(r)\n",
        "                # cancer risk if cumulative tel activations exceed threshold (per-cell)\n",
        "                cancer_penalty += max(0.0, (c.cumulative_tel - 1.2) * 0.3)\n",
        "            reward = float(sum(rewards) - cancer_penalty)\n",
        "            info['per_cell_rewards'] = rewards\n",
        "            info['cancer_penalty'] = cancer_penalty\n",
        "        obs = self._get_obs()\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# -------------------------\n",
        "# Policy / value network for tissue\n",
        "# -------------------------\n",
        "class TissuePolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Produces per-cell logits (for Bernoulli) and a scalar value for the whole tissue.\n",
        "    Architecture: per-cell encoder (shared), then optional global fusion.\n",
        "    \"\"\"\n",
        "    def __init__(self, per_cell_dim=4, global_dim=3, hidden=128):\n",
        "        super().__init__()\n",
        "        self.cell_enc = nn.Sequential(\n",
        "            nn.Linear(per_cell_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # produce logits per cell from encoded cell features + global summary\n",
        "        self.global_enc = nn.Sequential(\n",
        "            nn.Linear(global_dim, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear(hidden//2 + hidden//2, hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden//2, 1)  # logit per cell\n",
        "        )\n",
        "        # value head takes pooled cell encodings + global to predict scalar\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear((hidden//2) + (hidden//2), hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, per_cell_obs, global_feat):\n",
        "        # per_cell_obs: (M, per_cell_dim), global_feat: (global_dim,)\n",
        "        M = per_cell_obs.shape[0]\n",
        "        cell_h = self.cell_enc(per_cell_obs)  # (M, h2)\n",
        "        global_h = self.global_enc(global_feat).unsqueeze(0).expand(M, -1)  # (M, h2)\n",
        "        combined = torch.cat([cell_h, global_h], dim=-1)  # (M, h2*2)\n",
        "        logits = self.policy_head(combined).squeeze(-1)  # (M,)\n",
        "        # value: average pooling of cell encodings concatenated with mean global features\n",
        "        pooled = cell_h.mean(dim=0)\n",
        "        value_input = torch.cat([pooled, self.global_enc(global_feat)], dim=-1)\n",
        "        value = self.value_head(value_input).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "# -------------------------\n",
        "# PPO utilities (GAE, etc.)\n",
        "# -------------------------\n",
        "def compute_gae_seq(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    # rewards: list len=T of scalar rewards; values: list len=T of scalar values (and last bootstrap if used)\n",
        "    adv = np.zeros(len(rewards), dtype=np.float32)\n",
        "    lastgaelam = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        nonterminal = 1.0 - dones[t]\n",
        "        nextval = values[t+1] if t+1 < len(values) else 0.0\n",
        "        delta = rewards[t] + gamma * nextval * nonterminal - values[t]\n",
        "        lastgaelam = delta + gamma * lam * nonterminal * lastgaelam\n",
        "        adv[t] = lastgaelam\n",
        "    returns = adv + values[:len(rewards)]\n",
        "    return adv, returns\n",
        "\n",
        "# -------------------------\n",
        "# Training loop for PPO on TissueEnv\n",
        "# -------------------------\n",
        "def train_tissue_ppo(env_ctor, episodes=500, budget=3, clip_eps=0.2, policy_lr=3e-4, gamma=0.99):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = env_ctor()\n",
        "    M = env.M\n",
        "    obs_dim = env._get_obs()[0].shape[1]\n",
        "    global_dim = env._get_obs()[1].shape[0]\n",
        "    policy = TissuePolicy(per_cell_dim=obs_dim, global_dim=global_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n",
        "    all_rewards = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        per_cell_obs, global_feat = env.reset()\n",
        "        traj_obs_cells = []\n",
        "        traj_globals = []\n",
        "        traj_actions = []\n",
        "        traj_logps = []\n",
        "        traj_values = []\n",
        "        traj_rewards = []\n",
        "        traj_dones = []\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        while True:\n",
        "            # prepare tensors\n",
        "            per_cell_tensor = torch.tensor(per_cell_obs, dtype=torch.float32).to(device)  # (M,4)\n",
        "            global_tensor = torch.tensor(global_feat, dtype=torch.float32).to(device)     # (g,)\n",
        "            logits, value = policy(per_cell_tensor, global_tensor)  # logits (M,), value scalar\n",
        "            probs = torch.sigmoid(logits).cpu().detach().numpy()\n",
        "            # sample bernoulli per cell\n",
        "            sampled = (np.random.rand(M) < probs).astype(np.int32)\n",
        "            # enforce budget: if exceed, pick top-k by probs\n",
        "            if sampled.sum() > budget:\n",
        "                # choose indices of top probabilities\n",
        "                topk_idx = np.argsort(-probs)[:budget]\n",
        "                actions = np.zeros(M, dtype=np.int32)\n",
        "                actions[topk_idx] = 1\n",
        "            else:\n",
        "                actions = sampled\n",
        "\n",
        "            # compute logps for chosen actions\n",
        "            eps = 1e-8\n",
        "            logps = actions * np.log(probs + eps) + (1 - actions) * np.log(1 - probs + eps)\n",
        "            logp_sum = float(logps.sum())  # sum logp across cells so policy gradient treats joint action\n",
        "            # step env\n",
        "            (next_per_cell_obs, next_global), reward, done, info = env.step(actions.tolist())\n",
        "            traj_obs_cells.append(per_cell_obs.copy())\n",
        "            traj_globals.append(global_feat.copy())\n",
        "            traj_actions.append(actions.copy())\n",
        "            traj_logps.append(logp_sum)\n",
        "            traj_values.append(float(value.cpu().detach().numpy()))\n",
        "            traj_rewards.append(reward)\n",
        "            traj_dones.append(float(done))\n",
        "            ep_reward += reward\n",
        "\n",
        "            per_cell_obs, global_feat = next_per_cell_obs, next_global\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # compute GAE for this episode (single scalar reward per timestep)\n",
        "        values_np = np.array(traj_values + [0.0], dtype=np.float32)\n",
        "        advs, returns = compute_gae_seq(traj_rewards, values_np, traj_dones, gamma=gamma)\n",
        "\n",
        "        # convert to tensors for update\n",
        "        obs_cells_tensor = torch.tensor(np.array(traj_obs_cells), dtype=torch.float32).to(device)  # (T, M, per_cell_dim)\n",
        "        globals_tensor = torch.tensor(np.array(traj_globals), dtype=torch.float32).to(device)      # (T, global_dim)\n",
        "        actions_tensor = torch.tensor(np.array(traj_actions), dtype=torch.float32).to(device)     # (T, M)\n",
        "        old_logps_tensor = torch.tensor(np.array(traj_logps), dtype=torch.float32).to(device)    # (T,)\n",
        "        advs_tensor = torch.tensor(advs, dtype=torch.float32).to(device)\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        advs_tensor = (advs_tensor - advs_tensor.mean()) / (advs_tensor.std() + 1e-8)\n",
        "\n",
        "        # PPO update: perform several epochs over this episode's trajectory\n",
        "        for _ in range(6):\n",
        "            # recompute logits and values for each timestep\n",
        "            new_logps = []\n",
        "            values_pred = []\n",
        "            for t in range(len(traj_obs_cells)):\n",
        "                per_cell_t = obs_cells_tensor[t]  # (M, per_cell_dim)\n",
        "                global_t = globals_tensor[t]\n",
        "                logits_t, val_t = policy(per_cell_t, global_t)\n",
        "                probs_t = torch.sigmoid(logits_t)\n",
        "                # compute joint log prob (sum over cells) of the actions taken at time t\n",
        "                actions_t = actions_tensor[t]\n",
        "                logp_cells = actions_t * torch.log(probs_t + 1e-8) + (1 - actions_t) * torch.log(1 - probs_t + 1e-8)\n",
        "                joint_logp = logp_cells.sum()\n",
        "                new_logps.append(joint_logp)\n",
        "                values_pred.append(val_t)\n",
        "            new_logps = torch.stack(new_logps)  # (T,)\n",
        "            values_pred = torch.stack(values_pred).squeeze(-1)  # (T,)\n",
        "\n",
        "            ratio = torch.exp(new_logps - old_logps_tensor)\n",
        "            surr1 = ratio * advs_tensor\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advs_tensor\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.mse_loss(values_pred, returns_tensor)\n",
        "            # entropy over per-cell Bernoullis (sum entropy across cells, averaged over timesteps)\n",
        "            entropies = []\n",
        "            for t in range(len(traj_obs_cells)):\n",
        "                per_cell_t = obs_cells_tensor[t]\n",
        "                logits_t, _ = policy(per_cell_t, globals_tensor[t])\n",
        "                probs_t = torch.sigmoid(logits_t)\n",
        "                ent = -(probs_t * torch.log(probs_t + 1e-8) + (1-probs_t) * torch.log(1-probs_t + 1e-8)).sum()\n",
        "                entropies.append(ent)\n",
        "            entropy_term = torch.stack(entropies).mean()\n",
        "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_term\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        all_rewards.append(ep_reward)\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            avg = np.mean(all_rewards[-50:])\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"Episode {ep+1}/{episodes} | avg tissue reward (last50): {avg:.3f} | elapsed: {elapsed:.1f}s\")\n",
        "\n",
        "    return policy, all_rewards\n",
        "\n",
        "# -------------------------\n",
        "# Demo run (short)\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    env_ctor = lambda: TissueEnv(M=10, T=6, budget=3, seed=42)\n",
        "    policy_model, rewards = train_tissue_ppo(env_ctor, episodes=300, budget=3, policy_lr=3e-4)\n",
        "    print(\"Training finished. Mean reward last 50:\", np.mean(rewards[-50:]))\n",
        "\n",
        "    # Show one sample rollout from trained policy\n",
        "    env = env_ctor()\n",
        "    per_cell_obs, global_feat = env.reset()\n",
        "    rollout = []\n",
        "    for t in range(env.T):\n",
        "        per_cell_tensor = torch.tensor(per_cell_obs, dtype=torch.float32)\n",
        "        global_tensor = torch.tensor(global_feat, dtype=torch.float32)\n",
        "        logits, val = policy_model(per_cell_tensor, global_tensor)\n",
        "        probs = torch.sigmoid(logits).detach().numpy()\n",
        "        sampled = (np.random.rand(env.M) < probs).astype(int)\n",
        "        if sampled.sum() > env.budget:\n",
        "            topk = np.argsort(-probs)[:env.budget]\n",
        "            actions = np.zeros(env.M, dtype=int); actions[topk] = 1\n",
        "        else:\n",
        "            actions = sampled\n",
        "        (per_cell_obs, global_feat), reward, done, info = env.step(actions.tolist())\n",
        "        rollout.append((actions.copy(), reward, info))\n",
        "        if done: break\n",
        "\n",
        "    print(\"Sample rollout (actions per timestep, reward, info):\")\n",
        "    for step in rollout:\n",
        "        print(step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slClcHeKjhXe",
        "outputId": "4546e1d0-4ccb-42ae-a2ac-c6d7416a46a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50/300 | avg tissue reward (last50): -11.025 | elapsed: 5.8s\n",
            "Episode 100/300 | avg tissue reward (last50): -10.964 | elapsed: 10.7s\n",
            "Episode 150/300 | avg tissue reward (last50): -11.815 | elapsed: 16.5s\n",
            "Episode 200/300 | avg tissue reward (last50): -12.072 | elapsed: 21.5s\n",
            "Episode 250/300 | avg tissue reward (last50): -12.682 | elapsed: 27.8s\n",
            "Episode 300/300 | avg tissue reward (last50): -11.736 | elapsed: 32.8s\n",
            "Training finished. Mean reward last 50: -11.736400000000001\n",
            "Sample rollout (actions per timestep, reward, info):\n",
            "(array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1]), 0.0, {})\n",
            "(array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1]), 0.0, {})\n",
            "(array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1]), 0.0, {})\n",
            "(array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1]), 0.0, {})\n",
            "(array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1]), 0.0, {})\n",
            "(array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1]), -13.32, {'per_cell_rewards': [-2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 1.0, -2.0, -1.0, -1.0], 'cancer_penalty': 4.32})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy\n",
        "pip install torch   # choose the correct CUDA/CPU wheel for your machine\n"
      ],
      "metadata": {
        "id": "U2aQria6kTC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-cell tissue environment with per-cell identity (stem vs differentiated)\n",
        "# and PPO training where the policy can learn to preferentially target stem cells.\n",
        "# Educational demo. Save as `tissue_with_id.py` and run locally.\n",
        "# Requirements: torch, numpy\n",
        "# pip install torch numpy\n",
        "\n",
        "import time, random, math\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# Cell and Tissue environment with identity\n",
        "# -------------------------\n",
        "class Cell:\n",
        "    def __init__(self, rng, is_stem: bool):\n",
        "        self.rng = rng\n",
        "        self.is_stem = bool(is_stem)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Stem cells start with slightly longer telomeres and lower ROS baseline\n",
        "        if self.is_stem:\n",
        "            self.tel = float(self.rng.normal(loc=0.2, scale=0.25))\n",
        "            self.ros = float(self.rng.normal(loc=0.15, scale=0.15))\n",
        "        else:\n",
        "            self.tel = float(self.rng.normal(loc=0.0, scale=0.3))\n",
        "            self.ros = float(self.rng.normal(loc=0.25, scale=0.2))\n",
        "        self.age = 0.0\n",
        "        self.cumulative_tel = 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        # action: 0 or 1 for this cell in this timestep\n",
        "        if action == 1:\n",
        "            # stem cells may respond more strongly to telomerase (slightly larger gain)\n",
        "            gain = 0.6 if self.is_stem else 0.45\n",
        "            self.tel += gain * (1.0 - 0.08 * self.cumulative_tel)\n",
        "            self.cumulative_tel += 1.0\n",
        "        # telomere shortens slower for stem cells\n",
        "        base_shorten = 0.12 if self.is_stem else 0.18\n",
        "        self.tel -= base_shorten + 0.04 * self.rng.randn()\n",
        "        # ROS dynamics: differentiated cells may accumulate ROS faster\n",
        "        ros_noise = 0.02 if self.is_stem else 0.04\n",
        "        self.ros += ros_noise * self.rng.randn()\n",
        "        self.age += 1.0\n",
        "\n",
        "    def get_obs(self, max_steps):\n",
        "        # include identity as a feature (1.0 for stem, 0.0 for differentiated)\n",
        "        return np.array([self.tel, self.ros, self.age / (max_steps + 1), self.cumulative_tel, float(self.is_stem)], dtype=np.float32)\n",
        "\n",
        "\n",
        "class TissueEnvWithID:\n",
        "    \"\"\"\n",
        "    Tissue environment with per-cell identity. M cells, episode length T, budget activations per step.\n",
        "    Identity: some fraction of cells are stem cells; policy receives per-cell identity as input.\n",
        "    \"\"\"\n",
        "    def __init__(self, M=12, T=6, budget=3, stem_fraction=0.3, seed=0):\n",
        "        self.M = M\n",
        "        self.T = T\n",
        "        self.budget = budget\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.stem_fraction = stem_fraction\n",
        "        self.cells = []\n",
        "        self.step_count = 0\n",
        "        self._make_cells()\n",
        "\n",
        "    def _make_cells(self):\n",
        "        self.cells = []\n",
        "        num_stem = max(1, int(round(self.M * self.stem_fraction)))\n",
        "        stem_indices = set(self.rng.choice(self.M, size=num_stem, replace=False))\n",
        "        for i in range(self.M):\n",
        "            is_stem = i in stem_indices\n",
        "            self.cells.append(Cell(self.rng, is_stem))\n",
        "\n",
        "    def reset(self):\n",
        "        self._make_cells()\n",
        "        for c in self.cells:\n",
        "            c.reset()\n",
        "        self.step_count = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        per_cell = np.stack([c.get_obs(self.T) for c in self.cells], axis=0)  # (M,5)\n",
        "        mean_tel = per_cell[:,0].mean()\n",
        "        mean_ros = per_cell[:,1].mean()\n",
        "        stem_frac = np.mean([1.0 if c.is_stem else 0.0 for c in self.cells])\n",
        "        step_frac = self.step_count / (self.T + 1)\n",
        "        global_feat = np.array([mean_tel, mean_ros, stem_frac, step_frac], dtype=np.float32)\n",
        "        return per_cell, global_feat\n",
        "\n",
        "    def step(self, actions: List[int]):\n",
        "        assert len(actions) == self.M\n",
        "        for c, a in zip(self.cells, actions):\n",
        "            c.step(int(a))\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= self.T\n",
        "        reward = 0.0\n",
        "        info = {}\n",
        "        if done:\n",
        "            rewards = []\n",
        "            cancer_penalty = 0.0\n",
        "            # make cancer risk heavier for stem cells (biologically plausible)\n",
        "            for c in self.cells:\n",
        "                tel_score = c.tel\n",
        "                ros_score = -c.ros\n",
        "                logits = np.array([1.2*tel_score + 0.15*ros_score,\n",
        "                                   -0.6*tel_score + 0.5*(-ros_score),\n",
        "                                   -0.9*ros_score + 0.05*(1.0 - tel_score)])\n",
        "                logits += 0.25 * self.rng.randn(3)\n",
        "                probs = np.exp(logits - np.max(logits))\n",
        "                probs = probs / probs.sum()\n",
        "                outcome = self.rng.choice(3, p=probs)\n",
        "                if outcome == 0:\n",
        "                    r = 1.0\n",
        "                elif outcome == 1:\n",
        "                    r = -1.0\n",
        "                else:\n",
        "                    r = -2.0\n",
        "                rewards.append(r)\n",
        "                # cancer risk weight higher for stem cells\n",
        "                risk_weight = 0.5 if c.is_stem else 0.2\n",
        "                cancer_penalty += risk_weight * max(0.0, (c.cumulative_tel - 1.2) * 0.3)\n",
        "            reward = float(sum(rewards) - cancer_penalty)\n",
        "            info['per_cell_rewards'] = rewards\n",
        "            info['cancer_penalty'] = cancer_penalty\n",
        "        obs = self._get_obs()\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# -------------------------\n",
        "# Policy / value network updated to accept per-cell identity\n",
        "# -------------------------\n",
        "class TissuePolicyWithID(nn.Module):\n",
        "    \"\"\"\n",
        "    Produces per-cell logits and a scalar tissue value.\n",
        "    Per-cell encoder now takes identity as an extra input (last feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, per_cell_dim=5, global_dim=4, hidden=128):\n",
        "        super().__init__()\n",
        "        self.cell_enc = nn.Sequential(\n",
        "            nn.Linear(per_cell_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.global_enc = nn.Sequential(\n",
        "            nn.Linear(global_dim, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear((hidden//2) + (hidden//2), hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden//2, 1)  # logit per cell\n",
        "        )\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear((hidden//2) + (hidden//2), hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, per_cell_obs, global_feat):\n",
        "        # per_cell_obs: (M, per_cell_dim), global_feat: (global_dim,)\n",
        "        M = per_cell_obs.shape[0]\n",
        "        cell_h = self.cell_enc(per_cell_obs)  # (M, h2)\n",
        "        global_h = self.global_enc(global_feat).unsqueeze(0).expand(M, -1)  # (M, h2)\n",
        "        combined = torch.cat([cell_h, global_h], dim=-1)  # (M, h2*2)\n",
        "        logits = self.policy_head(combined).squeeze(-1)  # (M,)\n",
        "        pooled = cell_h.mean(dim=0)\n",
        "        value_input = torch.cat([pooled, self.global_enc(global_feat)], dim=-1)\n",
        "        value = self.value_head(value_input).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "# -------------------------\n",
        "# PPO utilities and training (similar to earlier tissue PPO)\n",
        "# -------------------------\n",
        "def compute_gae_seq(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    adv = np.zeros(len(rewards), dtype=np.float32)\n",
        "    lastgaelam = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        nonterminal = 1.0 - dones[t]\n",
        "        nextval = values[t+1] if t+1 < len(values) else 0.0\n",
        "        delta = rewards[t] + gamma * nextval * nonterminal - values[t]\n",
        "        lastgaelam = delta + gamma * lam * nonterminal * lastgaelam\n",
        "        adv[t] = lastgaelam\n",
        "    returns = adv + values[:len(rewards)]\n",
        "    return adv, returns\n",
        "\n",
        "def train_tissue_ppo_with_id(env_ctor, episodes=400, budget=3, clip_eps=0.2, policy_lr=3e-4, gamma=0.99):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = env_ctor()\n",
        "    M = env.M\n",
        "    per_cell_dim = env._get_obs()[0].shape[1]  # 5 now (tel,ros,age,cum_tel,is_stem)\n",
        "    global_dim = env._get_obs()[1].shape[0]    # 4 (mean_tel,mean_ros,stem_frac,step_frac)\n",
        "    policy = TissuePolicyWithID(per_cell_dim=per_cell_dim, global_dim=global_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n",
        "    all_rewards = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        per_cell_obs, global_feat = env.reset()\n",
        "        traj_obs_cells, traj_globals, traj_actions, traj_logps, traj_values, traj_rewards, traj_dones = [], [], [], [], [], [], []\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        while True:\n",
        "            per_cell_tensor = torch.tensor(per_cell_obs, dtype=torch.float32).to(device)  # (M,5)\n",
        "            global_tensor = torch.tensor(global_feat, dtype=torch.float32).to(device)     # (g,)\n",
        "            logits, value = policy(per_cell_tensor, global_tensor)\n",
        "            probs = torch.sigmoid(logits).cpu().detach().numpy()\n",
        "            sampled = (np.random.rand(M) < probs).astype(np.int32)\n",
        "            # enforce budget top-k if exceed\n",
        "            if sampled.sum() > budget:\n",
        "                topk_idx = np.argsort(-probs)[:budget]\n",
        "                actions = np.zeros(M, dtype=np.int32)\n",
        "                actions[topk_idx] = 1\n",
        "            else:\n",
        "                actions = sampled\n",
        "            # joint logp\n",
        "            eps = 1e-8\n",
        "            logps = actions * np.log(probs + eps) + (1 - actions) * np.log(1 - probs + eps)\n",
        "            logp_sum = float(logps.sum())\n",
        "            (next_per_cell_obs, next_global), reward, done, info = env.step(actions.tolist())\n",
        "            traj_obs_cells.append(per_cell_obs.copy())\n",
        "            traj_globals.append(global_feat.copy())\n",
        "            traj_actions.append(actions.copy())\n",
        "            traj_logps.append(logp_sum)\n",
        "            traj_values.append(float(value.cpu().detach().numpy()))\n",
        "            traj_rewards.append(reward)\n",
        "            traj_dones.append(float(done))\n",
        "            ep_reward += reward\n",
        "            per_cell_obs, global_feat = next_per_cell_obs, next_global\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # GAE\n",
        "        values_np = np.array(traj_values + [0.0], dtype=np.float32)\n",
        "        advs, returns = compute_gae_seq(traj_rewards, values_np, traj_dones, gamma=gamma)\n",
        "\n",
        "        # tensors\n",
        "        obs_cells_tensor = torch.tensor(np.array(traj_obs_cells), dtype=torch.float32).to(device)\n",
        "        globals_tensor = torch.tensor(np.array(traj_globals), dtype=torch.float32).to(device)\n",
        "        actions_tensor = torch.tensor(np.array(traj_actions), dtype=torch.float32).to(device)\n",
        "        old_logps_tensor = torch.tensor(np.array(traj_logps), dtype=torch.float32).to(device)\n",
        "        advs_tensor = torch.tensor(advs, dtype=torch.float32).to(device)\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        advs_tensor = (advs_tensor - advs_tensor.mean()) / (advs_tensor.std() + 1e-8)\n",
        "\n",
        "        # PPO update\n",
        "        for _ in range(6):\n",
        "            new_logps = []\n",
        "            values_pred = []\n",
        "            entropies = []\n",
        "            for t in range(len(traj_obs_cells)):\n",
        "                per_cell_t = obs_cells_tensor[t]\n",
        "                global_t = globals_tensor[t]\n",
        "                logits_t, val_t = policy(per_cell_t, global_t)\n",
        "                probs_t = torch.sigmoid(logits_t)\n",
        "                actions_t = actions_tensor[t]\n",
        "                logp_cells = actions_t * torch.log(probs_t + 1e-8) + (1 - actions_t) * torch.log(1 - probs_t + 1e-8)\n",
        "                joint_logp = logp_cells.sum()\n",
        "                new_logps.append(joint_logp)\n",
        "                values_pred.append(val_t)\n",
        "                ent = -(probs_t * torch.log(probs_t + 1e-8) + (1-probs_t) * torch.log(1-probs_t + 1e-8)).sum()\n",
        "                entropies.append(ent)\n",
        "            new_logps = torch.stack(new_logps)\n",
        "            values_pred = torch.stack(values_pred).squeeze(-1)\n",
        "            ratio = torch.exp(new_logps - old_logps_tensor)\n",
        "            surr1 = ratio * advs_tensor\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advs_tensor\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.mse_loss(values_pred, returns_tensor)\n",
        "            entropy_term = torch.stack(entropies).mean()\n",
        "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_term\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        all_rewards.append(ep_reward)\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            avg = np.mean(all_rewards[-50:])\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"Episode {ep+1}/{episodes} | avg tissue reward (last50): {avg:.3f} | elapsed: {elapsed:.1f}s\")\n",
        "\n",
        "    return policy, all_rewards\n",
        "\n",
        "# -------------------------\n",
        "# Demo run (short)\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    env_ctor = lambda: TissueEnvWithID(M=12, T=6, budget=3, stem_fraction=0.25, seed=42)\n",
        "    policy_model, rewards = train_tissue_ppo_with_id(env_ctor, episodes=300, budget=3, policy_lr=3e-4)\n",
        "    print(\"Training finished. Mean reward last 50:\", np.mean(rewards[-50:]))\n",
        "\n",
        "    # Show one sample rollout from trained policy\n",
        "    env = env_ctor()\n",
        "    per_cell_obs, global_feat = env.reset()\n",
        "    rollout = []\n",
        "    for t in range(env.T):\n",
        "        per_cell_tensor = torch.tensor(per_cell_obs, dtype=torch.float32)\n",
        "        global_tensor = torch.tensor(global_feat, dtype=torch.float32)\n",
        "        logits, val = policy_model(per_cell_tensor, global_tensor)\n",
        "        probs = torch.sigmoid(logits).detach().numpy()\n",
        "        sampled = (np.random.rand(env.M) < probs).astype(int)\n",
        "        if sampled.sum() > env.budget:\n",
        "            topk = np.argsort(-probs)[:env.budget]\n",
        "            actions = np.zeros(env.M, dtype=int); actions[topk] = 1\n",
        "        else:\n",
        "            actions = sampled\n",
        "        (per_cell_obs, global_feat), reward, done, info = env.step(actions.tolist())\n",
        "        rollout.append((actions.copy(), reward, info))\n",
        "        if done: break\n",
        "\n",
        "    print(\"Sample rollout (actions per timestep, reward, info):\")\n",
        "    for step in rollout:\n",
        "        print(step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeGc2xZNkWxN",
        "outputId": "8cff2805-ca84-4ef6-f850-b84ff98065d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50/300 | avg tissue reward (last50): -10.479 | elapsed: 4.7s\n",
            "Episode 100/300 | avg tissue reward (last50): -10.239 | elapsed: 9.5s\n",
            "Episode 150/300 | avg tissue reward (last50): -11.616 | elapsed: 14.0s\n",
            "Episode 200/300 | avg tissue reward (last50): -12.250 | elapsed: 18.6s\n",
            "Episode 250/300 | avg tissue reward (last50): -12.084 | elapsed: 22.3s\n",
            "Episode 300/300 | avg tissue reward (last50): -10.533 | elapsed: 26.1s\n",
            "Training finished. Mean reward last 50: -10.5326\n",
            "Sample rollout (actions per timestep, reward, info):\n",
            "(array([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]), 0.0, {})\n",
            "(array([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]), 0.0, {})\n",
            "(array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]), 0.0, {})\n",
            "(array([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]), 0.0, {})\n",
            "(array([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]), 0.0, {})\n",
            "(array([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]), -10.336, {'per_cell_rewards': [-2.0, -1.0, 1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, -1.0], 'cancer_penalty': 0.33599999999999997})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python tissue_with_id.py\n"
      ],
      "metadata": {
        "id": "S5R8D03FkWsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy\n",
        "pip install torch\n"
      ],
      "metadata": {
        "id": "XA2QHcz1kWn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gumbel_topk_sample(logits, k, tau=0.5, hard=False):\n",
        "    \"\"\"\n",
        "    Differentiable top-k via Gumbel-Softmax.\n",
        "    Returns soft mask (shape [M]) with at most k active entries (softly).\n",
        "    If hard=True, returns a hard top-k mask (0/1).\n",
        "    \"\"\"\n",
        "    gumbel = -torch.log(-torch.log(torch.rand_like(logits) + 1e-9) + 1e-9)\n",
        "    y = (logits + gumbel) / tau\n",
        "    y_soft = F.softmax(y, dim=0)\n",
        "    if hard:\n",
        "        topk = torch.topk(y_soft, k)[1]\n",
        "        hard_mask = torch.zeros_like(y_soft)\n",
        "        hard_mask[topk] = 1.0\n",
        "        # straight-through: replace forward with hard, keep soft gradients\n",
        "        y_soft = (hard_mask - y_soft).detach() + y_soft\n",
        "    return y_soft\n"
      ],
      "metadata": {
        "id": "NSNcr4GblWs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tissue_gumbel_topk.py\n",
        "# Requirements: torch, numpy\n",
        "# pip install torch numpy\n",
        "\n",
        "import time, math, random\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# Gumbel-Top-k helper (differentiable)\n",
        "# -------------------------\n",
        "def sample_gumbel(shape, device='cpu', eps=1e-9):\n",
        "    u = torch.rand(shape, device=device)\n",
        "    return -torch.log(-torch.log(u + eps) + eps)\n",
        "\n",
        "def gumbel_topk_soft(logits: torch.Tensor, k: int, tau: float = 0.5, hard: bool = True):\n",
        "    \"\"\"\n",
        "    Differentiable Gumbel-Top-k.\n",
        "    - logits: (M,) tensor of per-item logits\n",
        "    - k: how many items to select\n",
        "    - tau: softmax temperature (higher -> softer)\n",
        "    - hard: if True, returns a hard top-k mask for forward pass but gradients flow through soft weights\n",
        "    Returns:\n",
        "      soft_mask: (M,) continuous values that sum to ~1 (soft selection over top-k)\n",
        "      hard_mask: (M,) 0/1 mask (detached when used in forward environment) - only if hard requested\n",
        "    Implementation notes:\n",
        "      - We add Gumbel noise to logits and softmax at temperature tau.\n",
        "      - Then approximate top-k by turning off low-weight entries (keep top-k by soft value).\n",
        "      - Straight-through estimator: forward uses hard_mask, backward uses soft_mask.\n",
        "    \"\"\"\n",
        "    device = logits.device\n",
        "    M = logits.shape[0]\n",
        "\n",
        "    # Gumbel noise\n",
        "    g = sample_gumbel((M,), device=device)\n",
        "    y = (logits + g) / (tau + 1e-12)\n",
        "    y_soft = F.softmax(y, dim=0)  # soft selection across all M\n",
        "\n",
        "    # keep only top-k entries of the soft vector (by magnitude) - produce hard mask\n",
        "    _, topk_idx = torch.topk(y_soft, k)\n",
        "    hard_mask = torch.zeros_like(y_soft)\n",
        "    hard_mask[topk_idx] = 1.0\n",
        "\n",
        "    if hard:\n",
        "        # Straight-through: use hard_mask in forward, but keep gradient of y_soft\n",
        "        out = (hard_mask - y_soft).detach() + y_soft\n",
        "        return out, hard_mask\n",
        "    else:\n",
        "        return y_soft, hard_mask\n",
        "\n",
        "# -------------------------\n",
        "# Environment with identity (stem vs differentiated)\n",
        "# -------------------------\n",
        "class Cell:\n",
        "    def __init__(self, rng, is_stem: bool):\n",
        "        self.rng = rng\n",
        "        self.is_stem = bool(is_stem)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self.is_stem:\n",
        "            self.tel = float(self.rng.normal(loc=0.2, scale=0.25))\n",
        "            self.ros = float(self.rng.normal(loc=0.15, scale=0.15))\n",
        "        else:\n",
        "            self.tel = float(self.rng.normal(loc=0.0, scale=0.3))\n",
        "            self.ros = float(self.rng.normal(loc=0.25, scale=0.2))\n",
        "        self.age = 0.0\n",
        "        self.cumulative_tel = 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        # action: 0 or 1\n",
        "        if action == 1:\n",
        "            gain = 0.6 if self.is_stem else 0.45\n",
        "            self.tel += gain * (1.0 - 0.08 * self.cumulative_tel)\n",
        "            self.cumulative_tel += 1.0\n",
        "        base_shorten = 0.12 if self.is_stem else 0.18\n",
        "        self.tel -= base_shorten + 0.04 * self.rng.randn()\n",
        "        ros_noise = 0.02 if self.is_stem else 0.04\n",
        "        self.ros += ros_noise * self.rng.randn()\n",
        "        self.age += 1.0\n",
        "\n",
        "    def get_obs(self, max_steps):\n",
        "        # tel, ros, age_norm, cumulative_tel, is_stem\n",
        "        return np.array([self.tel, self.ros, self.age / (max_steps + 1), self.cumulative_tel, float(self.is_stem)], dtype=np.float32)\n",
        "\n",
        "class TissueEnvWithID:\n",
        "    def __init__(self, M=12, T=6, budget=3, stem_fraction=0.25, seed=0):\n",
        "        self.M = M\n",
        "        self.T = T\n",
        "        self.budget = budget\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.stem_fraction = stem_fraction\n",
        "        self._make_cells()\n",
        "        self.step_count = 0\n",
        "\n",
        "    def _make_cells(self):\n",
        "        num_stem = max(1, int(round(self.M * self.stem_fraction)))\n",
        "        stem_idx = set(self.rng.choice(self.M, size=num_stem, replace=False))\n",
        "        self.cells = [Cell(self.rng, i in stem_idx) for i in range(self.M)]\n",
        "\n",
        "    def reset(self):\n",
        "        self._make_cells()\n",
        "        for c in self.cells:\n",
        "            c.reset()\n",
        "        self.step_count = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        per_cell = np.stack([c.get_obs(self.T) for c in self.cells], axis=0)  # (M,5)\n",
        "        mean_tel = per_cell[:,0].mean()\n",
        "        mean_ros = per_cell[:,1].mean()\n",
        "        stem_frac = np.mean([1.0 if c.is_stem else 0.0 for c in self.cells])\n",
        "        step_frac = self.step_count / (self.T + 1)\n",
        "        global_feat = np.array([mean_tel, mean_ros, stem_frac, step_frac], dtype=np.float32)\n",
        "        return per_cell, global_feat\n",
        "\n",
        "    def step(self, actions: List[int]):\n",
        "        assert len(actions) == self.M\n",
        "        for c, a in zip(self.cells, actions):\n",
        "            c.step(int(a))\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= self.T\n",
        "        reward = 0.0\n",
        "        info = {}\n",
        "        if done:\n",
        "            rewards = []\n",
        "            cancer_penalty = 0.0\n",
        "            for c in self.cells:\n",
        "                tel_score = c.tel\n",
        "                ros_score = -c.ros\n",
        "                logits = np.array([1.2*tel_score + 0.15*ros_score,\n",
        "                                   -0.6*tel_score + 0.5*(-ros_score),\n",
        "                                   -0.9*ros_score + 0.05*(1.0 - tel_score)])\n",
        "                logits += 0.25 * self.rng.randn(3)\n",
        "                probs = np.exp(logits - np.max(logits))\n",
        "                probs = probs / probs.sum()\n",
        "                outcome = self.rng.choice(3, p=probs)\n",
        "                if outcome == 0:\n",
        "                    r = 1.0\n",
        "                elif outcome == 1:\n",
        "                    r = -1.0\n",
        "                else:\n",
        "                    r = -2.0\n",
        "                rewards.append(r)\n",
        "                risk_weight = 0.5 if c.is_stem else 0.2\n",
        "                cancer_penalty += risk_weight * max(0.0, (c.cumulative_tel - 1.2) * 0.3)\n",
        "            reward = float(sum(rewards) - cancer_penalty)\n",
        "            info['per_cell_rewards'] = rewards\n",
        "            info['cancer_penalty'] = cancer_penalty\n",
        "        obs = self._get_obs()\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# -------------------------\n",
        "# Policy with per-cell identity\n",
        "# -------------------------\n",
        "class TissuePolicyWithID(nn.Module):\n",
        "    def __init__(self, per_cell_dim=5, global_dim=4, hidden=128):\n",
        "        super().__init__()\n",
        "        self.cell_enc = nn.Sequential(\n",
        "            nn.Linear(per_cell_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.global_enc = nn.Sequential(\n",
        "            nn.Linear(global_dim, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear((hidden//2) + (hidden//2), hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden//2, 1)  # logit per cell\n",
        "        )\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear((hidden//2) + (hidden//2), hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, per_cell_obs, global_feat):\n",
        "        M = per_cell_obs.shape[0]\n",
        "        cell_h = self.cell_enc(per_cell_obs)  # (M, h2)\n",
        "        global_h = self.global_enc(global_feat).unsqueeze(0).expand(M, -1)\n",
        "        combined = torch.cat([cell_h, global_h], dim=-1)\n",
        "        logits = self.policy_head(combined).squeeze(-1)\n",
        "        pooled = cell_h.mean(dim=0)\n",
        "        value_input = torch.cat([pooled, self.global_enc(global_feat)], dim=-1)\n",
        "        value = self.value_head(value_input).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "# -------------------------\n",
        "# PPO utilities\n",
        "# -------------------------\n",
        "def compute_gae_seq(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    adv = np.zeros(len(rewards), dtype=np.float32)\n",
        "    lastgaelam = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        nonterminal = 1.0 - dones[t]\n",
        "        nextval = values[t+1] if t+1 < len(values) else 0.0\n",
        "        delta = rewards[t] + gamma * nextval * nonterminal - values[t]\n",
        "        lastgaelam = delta + gamma * lam * nonterminal * lastgaelam\n",
        "        adv[t] = lastgaelam\n",
        "    returns = adv + values[:len(rewards)]\n",
        "    return adv, returns\n",
        "\n",
        "# -------------------------\n",
        "# Training loop with Gumbel-top-k\n",
        "# -------------------------\n",
        "def train_tissue_ppo_gumbel(env_ctor, episodes=400, budget=3, clip_eps=0.2,\n",
        "                            policy_lr=3e-4, gamma=0.99, tau=0.6, device=None):\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    env = env_ctor()\n",
        "    M = env.M\n",
        "    per_cell_dim = env._get_obs()[0].shape[1]\n",
        "    global_dim = env._get_obs()[1].shape[0]\n",
        "    policy = TissuePolicyWithID(per_cell_dim=per_cell_dim, global_dim=global_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n",
        "    all_rewards = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        per_cell_obs, global_feat = env.reset()\n",
        "        traj_obs_cells, traj_globals, traj_actions, traj_logps, traj_values, traj_rewards, traj_dones = [], [], [], [], [], [], []\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        while True:\n",
        "            per_cell_tensor = torch.tensor(per_cell_obs, dtype=torch.float32).to(device)\n",
        "            global_tensor = torch.tensor(global_feat, dtype=torch.float32).to(device)\n",
        "            logits, value = policy(per_cell_tensor, global_tensor)  # logits (M,), value scalar\n",
        "\n",
        "            # Gumbel-Top-k soft sample (straight-through)\n",
        "            soft_mask, hard_mask = gumbel_topk_soft(logits, k=budget, tau=tau, hard=True)\n",
        "            # soft_mask: used for logprob & gradients; hard_mask: used for environment forward\n",
        "            # compute joint \"log-prob\" surrogate from soft mask\n",
        "            eps = 1e-9\n",
        "            joint_logp = torch.log(soft_mask + eps).sum()  # treat soft_mask as pseudo-prob\n",
        "            joint_logp_val = float(joint_logp.detach().cpu().numpy())\n",
        "\n",
        "            # actions for environment: hard mask (0/1) cast to ints\n",
        "            actions = hard_mask.detach().cpu().numpy().astype(int).tolist()\n",
        "\n",
        "            (next_per_cell_obs, next_global), reward, done, info = env.step(actions)\n",
        "\n",
        "            traj_obs_cells.append(per_cell_obs.copy())\n",
        "            traj_globals.append(global_feat.copy())\n",
        "            traj_actions.append(actions)             # store discrete actions for info (not used in logp)\n",
        "            traj_logps.append(joint_logp_val)       # old logprobs: scalar per timestep\n",
        "            traj_values.append(float(value.cpu().detach().numpy()))\n",
        "            traj_rewards.append(reward)\n",
        "            traj_dones.append(float(done))\n",
        "            ep_reward += reward\n",
        "\n",
        "            per_cell_obs, global_feat = next_per_cell_obs, next_global\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # compute advantages/returns\n",
        "        values_np = np.array(traj_values + [0.0], dtype=np.float32)\n",
        "        advs, returns = compute_gae_seq(traj_rewards, values_np, traj_dones, gamma=gamma)\n",
        "\n",
        "        # pack tensors\n",
        "        obs_cells_tensor = torch.tensor(np.array(traj_obs_cells), dtype=torch.float32).to(device)\n",
        "        globals_tensor = torch.tensor(np.array(traj_globals), dtype=torch.float32).to(device)\n",
        "        old_logps_tensor = torch.tensor(np.array(traj_logps), dtype=torch.float32).to(device)\n",
        "        advs_tensor = torch.tensor(advs, dtype=torch.float32).to(device)\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        advs_tensor = (advs_tensor - advs_tensor.mean()) / (advs_tensor.std() + 1e-8)\n",
        "\n",
        "        # PPO update\n",
        "        for _ in range(6):\n",
        "            new_logps = []\n",
        "            values_pred = []\n",
        "            entropies = []\n",
        "            for t in range(len(traj_obs_cells)):\n",
        "                per_cell_t = obs_cells_tensor[t]\n",
        "                global_t = globals_tensor[t]\n",
        "                logits_t, val_t = policy(per_cell_t, global_t)\n",
        "                # recompute soft_mask (no straight-through here) to compute new_logp\n",
        "                soft_mask_t, hard_mask_t = gumbel_topk_soft(logits_t, k=budget, tau=tau, hard=False)\n",
        "                # use soft_mask_t for joint logp\n",
        "                logp_t = torch.log(soft_mask_t + 1e-9).sum()\n",
        "                new_logps.append(logp_t)\n",
        "                values_pred.append(val_t)\n",
        "                # entropy on soft_mask to encourage exploration (sum across cells)\n",
        "                ent = -(soft_mask_t * torch.log(soft_mask_t + 1e-9)).sum()\n",
        "                entropies.append(ent)\n",
        "            new_logps = torch.stack(new_logps)\n",
        "            values_pred = torch.stack(values_pred).squeeze(-1)\n",
        "            ratio = torch.exp(new_logps - old_logps_tensor)\n",
        "            surr1 = ratio * advs_tensor\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advs_tensor\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.mse_loss(values_pred, returns_tensor)\n",
        "            entropy_term = torch.stack(entropies).mean()\n",
        "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_term\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        all_rewards.append(ep_reward)\n",
        "        # Optional: anneal temperature over time to make selection crisper\n",
        "        # tau = max(0.1, tau * 0.995)\n",
        "\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            avg = np.mean(all_rewards[-50:])\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"Episode {ep+1}/{episodes} | avg tissue reward (last50): {avg:.3f} | elapsed: {elapsed:.1f}s\")\n",
        "\n",
        "    return policy, all_rewards\n",
        "\n",
        "# -------------------------\n",
        "# Demo run\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    env_ctor = lambda: TissueEnvWithID(M=12, T=6, budget=3, stem_fraction=0.25, seed=42)\n",
        "    policy_model, rewards = train_tissue_ppo_gumbel(env_ctor, episodes=300, budget=3, policy_lr=3e-4, tau=0.8)\n",
        "    print(\"Training finished. Mean reward last 50:\", np.mean(rewards[-50:]))\n",
        "\n",
        "    # sample rollout from learned policy (deterministic top-k via soft mask -> topk)\n",
        "    env = env_ctor()\n",
        "    per_cell_obs, global_feat = env.reset()\n",
        "    rollout = []\n",
        "    for t in range(env.T):\n",
        "        per_cell_tensor = torch.tensor(per_cell_obs, dtype=torch.float32)\n",
        "        global_tensor = torch.tensor(global_feat, dtype=torch.float32)\n",
        "        logits, val = policy_model(per_cell_tensor, global_tensor)\n",
        "        # deterministic top-k: take topk indices of logits (no noise)\n",
        "        _, topk_idx = torch.topk(logits, env.budget)\n",
        "        actions = np.zeros(env.M, dtype=int)\n",
        "        actions[topk_idx.cpu().numpy()] = 1\n",
        "        (per_cell_obs, global_feat), reward, done, info = env.step(actions.tolist())\n",
        "        rollout.append((actions.copy(), reward, info))\n",
        "        if done: break\n",
        "\n",
        "    print(\"Sample rollout (actions per timestep, reward, info):\")\n",
        "    for step in rollout:\n",
        "        print(step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmTKj5WGlWoW",
        "outputId": "2d05703c-a7cd-4e2e-c2ff-7593e916ad31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50/300 | avg tissue reward (last50): -10.973 | elapsed: 5.9s\n",
            "Episode 100/300 | avg tissue reward (last50): -11.190 | elapsed: 12.2s\n",
            "Episode 150/300 | avg tissue reward (last50): -10.649 | elapsed: 16.0s\n",
            "Episode 200/300 | avg tissue reward (last50): -11.590 | elapsed: 20.1s\n",
            "Episode 250/300 | avg tissue reward (last50): -10.846 | elapsed: 23.7s\n",
            "Episode 300/300 | avg tissue reward (last50): -10.004 | elapsed: 27.2s\n",
            "Training finished. Mean reward last 50: -10.004159999999999\n",
            "Sample rollout (actions per timestep, reward, info):\n",
            "(array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]), 0.0, {})\n",
            "(array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]), 0.0, {})\n",
            "(array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]), 0.0, {})\n",
            "(array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]), 0.0, {})\n",
            "(array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]), 0.0, {})\n",
            "(array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]), -6.864, {'per_cell_rewards': [-2.0, -1.0, 1.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, 1.0, -2.0, -2.0], 'cancer_penalty': 0.8639999999999999})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch numpy\n"
      ],
      "metadata": {
        "id": "Ci222Nxambab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python tissue_with_predictor_gumbel_topk.py\n"
      ],
      "metadata": {
        "id": "LYfa_XC3mbYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tissue_with_predictor_gumbel_topk.py\n",
        "# Full pipeline: train/load VirtualCellModel predictor -> TissueEnv that uses predictor ->\n",
        "# PPO agent with Gumbel-top-k differentiable selection -> training loop.\n",
        "#\n",
        "# Requirements: torch, numpy\n",
        "# pip install torch numpy\n",
        "\n",
        "import time, math, random\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# -------------------------\n",
        "# Synthetic dataset to train predictor\n",
        "# -------------------------\n",
        "class SyntheticCellDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Produces per-cell samples with features:\n",
        "      [tel, ros, age_norm, cumulative_tel, is_stem]\n",
        "    Labels: 0=healthy,1=senescent,2=apoptotic\n",
        "    \"\"\"\n",
        "    def __init__(self, n=10000, seed=42):\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.n = n\n",
        "        self._make()\n",
        "\n",
        "    def _make(self):\n",
        "        tel = self.rng.normal(loc=0.0, scale=0.4, size=(self.n, 1)).astype(np.float32)\n",
        "        ros = self.rng.normal(loc=0.2, scale=0.25, size=(self.n,1)).astype(np.float32)\n",
        "        age = self.rng.randint(0, 8, size=(self.n,1)).astype(np.float32)\n",
        "        cum_tel = self.rng.randint(0,4,size=(self.n,1)).astype(np.float32)\n",
        "        is_stem = (self.rng.rand(self.n,1) < 0.25).astype(np.float32)\n",
        "\n",
        "        X = np.concatenate([tel, ros, age/10.0, cum_tel, is_stem], axis=1)  # age normalized\n",
        "        # Define rule to create labels (toy biologically-inspired rule)\n",
        "        labels = np.zeros(self.n, dtype=np.int64)\n",
        "        # apoptosis if ROS very high\n",
        "        labels[ros.squeeze() > 0.9] = 2\n",
        "        # senescence if tel very short and no telomerase\n",
        "        labels[(tel.squeeze() < -0.6) & (cum_tel.squeeze() < 1.0)] = 1\n",
        "        # otherwise healthy\n",
        "        # add noise to labels\n",
        "        flip = self.rng.rand(self.n)\n",
        "        noisy = (flip < 0.03)\n",
        "        labels[noisy] = self.rng.randint(0,3, noisy.sum())\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], int(self.y[idx])\n",
        "\n",
        "# -------------------------\n",
        "# VirtualCellModel predictor (supervised)\n",
        "# -------------------------\n",
        "class VirtualCellModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Small MLP that maps per-cell features to logits for 3 outcomes.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=5, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.LayerNorm(hidden//2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden//2, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, in_dim)\n",
        "        logits = self.net(x)\n",
        "        return logits  # logits for 3 classes\n",
        "\n",
        "def train_predictor(save_path='predictor.pt', epochs=8, batch_size=256, lr=1e-3, device=None):\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    ds = SyntheticCellDataset(n=12000)\n",
        "    train_size = int(len(ds)*0.8)\n",
        "    val_size = len(ds) - train_size\n",
        "    train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "    model = VirtualCellModel(in_dim=5, hidden=128).to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    best_acc = 0.0\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for X,y in train_loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(X)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "        # val\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for X,y in val_loader:\n",
        "                X = X.to(device); y = y.to(device)\n",
        "                logits = model(X)\n",
        "                pred = logits.argmax(dim=1)\n",
        "                correct += (pred==y).sum().item()\n",
        "                total += X.size(0)\n",
        "        acc = correct/total\n",
        "        print(f\"[Predictor] Epoch {ep+1}/{epochs} loss={total_loss/len(train_loader.dataset):.4f} val_acc={acc:.4f}\")\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "    print(\"[Predictor] Best val acc:\", best_acc)\n",
        "    return model, best_acc\n",
        "\n",
        "# -------------------------\n",
        "# Gumbel-Top-k helper (differentiable)\n",
        "# -------------------------\n",
        "def sample_gumbel(shape, device='cpu', eps=1e-9):\n",
        "    u = torch.rand(shape, device=device)\n",
        "    return -torch.log(-torch.log(u + eps) + eps)\n",
        "\n",
        "def gumbel_topk_soft(logits: torch.Tensor, k: int, tau: float = 0.5, hard: bool = True):\n",
        "    device = logits.device\n",
        "    M = logits.shape[0]\n",
        "    g = sample_gumbel((M,), device=device)\n",
        "    y = (logits + g) / (tau + 1e-12)\n",
        "    y_soft = F.softmax(y, dim=0)\n",
        "    _, topk_idx = torch.topk(y_soft, k)\n",
        "    hard_mask = torch.zeros_like(y_soft)\n",
        "    hard_mask[topk_idx] = 1.0\n",
        "    if hard:\n",
        "        out = (hard_mask - y_soft).detach() + y_soft\n",
        "        return out, hard_mask\n",
        "    else:\n",
        "        return y_soft, hard_mask\n",
        "\n",
        "# -------------------------\n",
        "# Environment uses predictor for outcome probabilities\n",
        "# -------------------------\n",
        "class Cell:\n",
        "    def __init__(self, rng, is_stem: bool):\n",
        "        self.rng = rng\n",
        "        self.is_stem = bool(is_stem)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self.is_stem:\n",
        "            self.tel = float(self.rng.normal(loc=0.2, scale=0.25))\n",
        "            self.ros = float(self.rng.normal(loc=0.15, scale=0.15))\n",
        "        else:\n",
        "            self.tel = float(self.rng.normal(loc=0.0, scale=0.3))\n",
        "            self.ros = float(self.rng.normal(loc=0.25, scale=0.2))\n",
        "        self.age = 0.0\n",
        "        self.cumulative_tel = 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 1:\n",
        "            gain = 0.6 if self.is_stem else 0.45\n",
        "            self.tel += gain * (1.0 - 0.08 * self.cumulative_tel)\n",
        "            self.cumulative_tel += 1.0\n",
        "        base_shorten = 0.12 if self.is_stem else 0.18\n",
        "        self.tel -= base_shorten + 0.04 * self.rng.randn()\n",
        "        ros_noise = 0.02 if self.is_stem else 0.04\n",
        "        self.ros += ros_noise * self.rng.randn()\n",
        "        self.age += 1.0\n",
        "\n",
        "    def get_obs(self, max_steps):\n",
        "        return np.array([self.tel, self.ros, self.age / (max_steps + 1), self.cumulative_tel, float(self.is_stem)], dtype=np.float32)\n",
        "\n",
        "class TissueEnvWithPredictor:\n",
        "    \"\"\"\n",
        "    Tissue environment that uses a trained predictor to compute per-cell outcome probabilities.\n",
        "    Predictor should output logits for 3 classes; we softmax and sample per-cell outcome.\n",
        "    \"\"\"\n",
        "    def __init__(self, predictor: VirtualCellModel, device=None, M=12, T=6, budget=3, stem_fraction=0.25, seed=0):\n",
        "        self.predictor = predictor\n",
        "        self.device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "        self.predictor.to(self.device)\n",
        "        self.predictor.eval()\n",
        "        self.M = M\n",
        "        self.T = T\n",
        "        self.budget = budget\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.stem_fraction = stem_fraction\n",
        "        self._make_cells()\n",
        "        self.step_count = 0\n",
        "\n",
        "    def _make_cells(self):\n",
        "        num_stem = max(1, int(round(self.M * self.stem_fraction)))\n",
        "        stem_idx = set(self.rng.choice(self.M, size=num_stem, replace=False))\n",
        "        self.cells = [Cell(self.rng, i in stem_idx) for i in range(self.M)]\n",
        "\n",
        "    def reset(self):\n",
        "        self._make_cells()\n",
        "        for c in self.cells:\n",
        "            c.reset()\n",
        "        self.step_count = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        per_cell = np.stack([c.get_obs(self.T) for c in self.cells], axis=0)  # (M,5)\n",
        "        mean_tel = per_cell[:,0].mean()\n",
        "        mean_ros = per_cell[:,1].mean()\n",
        "        stem_frac = np.mean([1.0 if c.is_stem else 0.0 for c in self.cells])\n",
        "        step_frac = self.step_count / (self.T + 1)\n",
        "        global_feat = np.array([mean_tel, mean_ros, stem_frac, step_frac], dtype=np.float32)\n",
        "        return per_cell, global_feat\n",
        "\n",
        "    def step(self, actions: List[int]):\n",
        "        assert len(actions) == self.M\n",
        "        for c, a in zip(self.cells, actions):\n",
        "            c.step(int(a))\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= self.T\n",
        "        reward = 0.0\n",
        "        info = {}\n",
        "        if done:\n",
        "            rewards = []\n",
        "            cancer_penalty = 0.0\n",
        "            # build batch of per-cell features and get predictor probabilities\n",
        "            X = np.stack([c.get_obs(self.T) for c in self.cells], axis=0).astype(np.float32)  # (M,5)\n",
        "            with torch.no_grad():\n",
        "                X_t = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "                logits = self.predictor(X_t)  # (M,3)\n",
        "                probs = F.softmax(logits, dim=-1).cpu().numpy()  # (M,3)\n",
        "            # sample outcome per cell using predictor probabilities\n",
        "            for i, c in enumerate(self.cells):\n",
        "                p = probs[i]\n",
        "                # ensure numeric stability\n",
        "                p = p / p.sum()\n",
        "                outcome = self.rng.choice(3, p=p)\n",
        "                if outcome == 0:\n",
        "                    r = 1.0\n",
        "                elif outcome == 1:\n",
        "                    r = -1.0\n",
        "                else:\n",
        "                    r = -2.0\n",
        "                rewards.append(r)\n",
        "                risk_weight = 0.5 if c.is_stem else 0.2\n",
        "                cancer_penalty += risk_weight * max(0.0, (c.cumulative_tel - 1.2) * 0.3)\n",
        "            reward = float(sum(rewards) - cancer_penalty)\n",
        "            info['per_cell_rewards'] = rewards\n",
        "            info['cancer_penalty'] = cancer_penalty\n",
        "        obs = self._get_obs()\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# -------------------------\n",
        "# Policy (same as before) - per-cell identity supported\n",
        "# -------------------------\n",
        "class TissuePolicyWithID(nn.Module):\n",
        "    def __init__(self, per_cell_dim=5, global_dim=4, hidden=128):\n",
        "        super().__init__()\n",
        "        self.cell_enc = nn.Sequential(\n",
        "            nn.Linear(per_cell_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.global_enc = nn.Sequential(\n",
        "            nn.Linear(global_dim, hidden//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear((hidden//2) + (hidden//2), hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden//2, 1)  # logit per cell\n",
        "        )\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear((hidden//2) + (hidden//2), hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, per_cell_obs, global_feat):\n",
        "        M = per_cell_obs.shape[0]\n",
        "        cell_h = self.cell_enc(per_cell_obs)  # (M, h2)\n",
        "        global_h = self.global_enc(global_feat).unsqueeze(0).expand(M, -1)\n",
        "        combined = torch.cat([cell_h, global_h], dim=-1)\n",
        "        logits = self.policy_head(combined).squeeze(-1)\n",
        "        pooled = cell_h.mean(dim=0)\n",
        "        value_input = torch.cat([pooled, self.global_enc(global_feat)], dim=-1)\n",
        "        value = self.value_head(value_input).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "# -------------------------\n",
        "# PPO train loop integrating predictor and Gumbel-top-k\n",
        "# -------------------------\n",
        "def compute_gae_seq(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    adv = np.zeros(len(rewards), dtype=np.float32)\n",
        "    lastgaelam = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        nonterminal = 1.0 - dones[t]\n",
        "        nextval = values[t+1] if t+1 < len(values) else 0.0\n",
        "        delta = rewards[t] + gamma * nextval * nonterminal - values[t]\n",
        "        lastgaelam = delta + gamma * lam * nonterminal * lastgaelam\n",
        "        adv[t] = lastgaelam\n",
        "    returns = adv + values[:len(rewards)]\n",
        "    return adv, returns\n",
        "\n",
        "def train_tissue_ppo_with_predictor(env_ctor, episodes=400, budget=3, clip_eps=0.2,\n",
        "                                    policy_lr=3e-4, gamma=0.99, tau=0.6, device=None):\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    env = env_ctor()\n",
        "    M = env.M\n",
        "    per_cell_dim = env._get_obs()[0].shape[1]\n",
        "    global_dim = env._get_obs()[1].shape[0]\n",
        "    policy = TissuePolicyWithID(per_cell_dim=per_cell_dim, global_dim=global_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n",
        "    all_rewards = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        per_cell_obs, global_feat = env.reset()\n",
        "        traj_obs_cells, traj_globals, traj_actions, traj_logps, traj_values, traj_rewards, traj_dones = [], [], [], [], [], [], []\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        while True:\n",
        "            per_cell_tensor = torch.tensor(per_cell_obs, dtype=torch.float32).to(device)\n",
        "            global_tensor = torch.tensor(global_feat, dtype=torch.float32).to(device)\n",
        "            logits, value = policy(per_cell_tensor, global_tensor)  # logits (M,), value scalar\n",
        "\n",
        "            # Gumbel-Top-k (straight-through)\n",
        "            soft_mask, hard_mask = gumbel_topk_soft(logits, k=budget, tau=tau, hard=True)\n",
        "            joint_logp = torch.log(soft_mask + 1e-9).sum()\n",
        "            joint_logp_val = float(joint_logp.detach().cpu().numpy())\n",
        "\n",
        "            actions = hard_mask.detach().cpu().numpy().astype(int).tolist()\n",
        "            (next_per_cell_obs, next_global), reward, done, info = env.step(actions)\n",
        "\n",
        "            traj_obs_cells.append(per_cell_obs.copy())\n",
        "            traj_globals.append(global_feat.copy())\n",
        "            traj_actions.append(actions)\n",
        "            traj_logps.append(joint_logp_val)\n",
        "            traj_values.append(float(value.cpu().detach().numpy()))\n",
        "            traj_rewards.append(reward)\n",
        "            traj_dones.append(float(done))\n",
        "            ep_reward += reward\n",
        "\n",
        "            per_cell_obs, global_feat = next_per_cell_obs, next_global\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        values_np = np.array(traj_values + [0.0], dtype=np.float32)\n",
        "        advs, returns = compute_gae_seq(traj_rewards, values_np, traj_dones, gamma=gamma)\n",
        "\n",
        "        obs_cells_tensor = torch.tensor(np.array(traj_obs_cells), dtype=torch.float32).to(device)\n",
        "        globals_tensor = torch.tensor(np.array(traj_globals), dtype=torch.float32).to(device)\n",
        "        old_logps_tensor = torch.tensor(np.array(traj_logps), dtype=torch.float32).to(device)\n",
        "        advs_tensor = torch.tensor(advs, dtype=torch.float32).to(device)\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        advs_tensor = (advs_tensor - advs_tensor.mean()) / (advs_tensor.std() + 1e-8)\n",
        "\n",
        "        for _ in range(6):\n",
        "            new_logps = []\n",
        "            values_pred = []\n",
        "            entropies = []\n",
        "            for t in range(len(traj_obs_cells)):\n",
        "                per_cell_t = obs_cells_tensor[t]\n",
        "                global_t = globals_tensor[t]\n",
        "                logits_t, val_t = policy(per_cell_t, global_t)\n",
        "                soft_mask_t, _ = gumbel_topk_soft(logits_t, k=budget, tau=tau, hard=False)\n",
        "                logp_t = torch.log(soft_mask_t + 1e-9).sum()\n",
        "                new_logps.append(logp_t)\n",
        "                values_pred.append(val_t)\n",
        "                ent = -(soft_mask_t * torch.log(soft_mask_t + 1e-9)).sum()\n",
        "                entropies.append(ent)\n",
        "            new_logps = torch.stack(new_logps)\n",
        "            values_pred = torch.stack(values_pred).squeeze(-1)\n",
        "            ratio = torch.exp(new_logps - old_logps_tensor)\n",
        "            surr1 = ratio * advs_tensor\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advs_tensor\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.mse_loss(values_pred, returns_tensor)\n",
        "            entropy_term = torch.stack(entropies).mean()\n",
        "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_term\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        all_rewards.append(ep_reward)\n",
        "        # optional: anneal temperature\n",
        "        # tau = max(0.1, tau * 0.995)\n",
        "\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            avg = np.mean(all_rewards[-50:])\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"[PPO] Episode {ep+1}/{episodes} | avg reward (last50): {avg:.3f} | elapsed: {elapsed:.1f}s\")\n",
        "\n",
        "    return policy, all_rewards\n",
        "\n",
        "# -------------------------\n",
        "# Main: train predictor (or load), then train PPO using predictor-driven env\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    PREDICTOR_PATH = \"predictor.pt\"\n",
        "\n",
        "    # 1) Train predictor if no saved weights\n",
        "    if not os.path.exists(PREDICTOR_PATH):\n",
        "        print(\"[Main] Training predictor from synthetic data...\")\n",
        "        predictor, acc = train_predictor(save_path=PREDICTOR_PATH, epochs=10, batch_size=256, lr=1e-3, device=DEVICE)\n",
        "    else:\n",
        "        print(f\"[Main] Loading predictor weights from {PREDICTOR_PATH}\")\n",
        "        predictor = VirtualCellModel(in_dim=5, hidden=128).to(DEVICE)\n",
        "        predictor.load_state_dict(torch.load(PREDICTOR_PATH, map_location=DEVICE))\n",
        "        predictor.eval()\n",
        "\n",
        "    # 2) Create env that uses predictor\n",
        "    env_ctor = lambda: TissueEnvWithPredictor(predictor=predictor, device=DEVICE, M=12, T=6, budget=3, stem_fraction=0.25, seed=42)\n",
        "\n",
        "    # 3) Train PPO agent with Gumbel-top-k selection\n",
        "    print(\"[Main] Training PPO agent using predictor-driven environment...\")\n",
        "    policy_model, rewards = train_tissue_ppo_with_predictor(env_ctor, episodes=300, budget=3, policy_lr=3e-4, tau=0.8, device=DEVICE)\n",
        "    print(\"[Main] PPO training finished. Mean reward (last 50):\", np.mean(rewards[-50:]))\n",
        "\n",
        "    # Save policy if desired\n",
        "    torch.save(policy_model.state_dict(), \"policy_model.pt\")\n",
        "    print(\"[Main] Saved policy_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSsiJyoymdZ0",
        "outputId": "416d9b17-ee76-4a2a-fc89-4c01c2f429c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Main] Training predictor from synthetic data...\n",
            "[Predictor] Epoch 1/10 loss=0.3015 val_acc=0.9646\n",
            "[Predictor] Epoch 2/10 loss=0.1493 val_acc=0.9675\n",
            "[Predictor] Epoch 3/10 loss=0.1359 val_acc=0.9712\n",
            "[Predictor] Epoch 4/10 loss=0.1250 val_acc=0.9746\n",
            "[Predictor] Epoch 5/10 loss=0.1202 val_acc=0.9779\n",
            "[Predictor] Epoch 6/10 loss=0.1179 val_acc=0.9729\n",
            "[Predictor] Epoch 7/10 loss=0.1170 val_acc=0.9746\n",
            "[Predictor] Epoch 8/10 loss=0.1165 val_acc=0.9742\n",
            "[Predictor] Epoch 9/10 loss=0.1169 val_acc=0.9750\n",
            "[Predictor] Epoch 10/10 loss=0.1153 val_acc=0.9750\n",
            "[Predictor] Best val acc: 0.9779166666666667\n",
            "[Main] Training PPO agent using predictor-driven environment...\n",
            "[PPO] Episode 50/300 | avg reward (last50): -3.357 | elapsed: 3.5s\n",
            "[PPO] Episode 100/300 | avg reward (last50): -3.584 | elapsed: 7.7s\n",
            "[PPO] Episode 150/300 | avg reward (last50): -3.758 | elapsed: 11.3s\n",
            "[PPO] Episode 200/300 | avg reward (last50): -3.421 | elapsed: 14.8s\n",
            "[PPO] Episode 250/300 | avg reward (last50): -3.730 | elapsed: 18.9s\n",
            "[PPO] Episode 300/300 | avg reward (last50): -3.904 | elapsed: 22.5s\n",
            "[Main] PPO training finished. Mean reward (last 50): -3.90368\n",
            "[Main] Saved policy_model.pt\n"
          ]
        }
      ]
    }
  ]
}